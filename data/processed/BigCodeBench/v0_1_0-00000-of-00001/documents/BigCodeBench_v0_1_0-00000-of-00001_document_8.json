{
  "id": "BigCodeBench_v0_1_0-00000-of-00001_document_8",
  "text": "    # Check if source and destination folders exist\n    if not os.path.isdir(src_folder):\n        raise ValueError(f\"Source folder '{src_folder}' does not exist.\")\n    if not os.path.isdir(dst_folder):\n        raise ValueError(f\"Destination folder '{dst_folder}' does not exist.\")\n    \n    processes = []\n    failed_files = []\n\n    # Compress files in a background process\n    for file in glob(os.path.join(src_folder, '*')):\n        process = subprocess.Popen(['gzip', file])\n        processes.append((process, file))\n\n    # Wait for all processes to complete\n    for process, file in processes:\n        retcode = process.wait()\n        if retcode != 0:\n            failed_files.append(os.path.basename(file))\n\n    # Move compressed files to destination folder\n    for file in glob(os.path.join(src_folder, '*.gz')):\n        try:\n            shutil.move(file, dst_folder)\n        except Exception as e:\n            failed_files.append(os.path.basename(file))\n\n    if failed_files:\n        return {'success': False, 'message': 'Some files failed to compress or move.', 'failed_files': failed_files}\n    else:\n        return {'success': True, 'message': 'All files compressed and moved successfully.', 'failed_files': []} [DOC_SEP] \n    report_data = []\n\n    for product in product_list:\n        category = categories[random.randint(0, len(categories)-1)]\n        quantity_sold = random.randint(min_value, max_value)\n        revenue = quantity_sold * random.randint(min_value, max_value)\n        report_data.append([product, category, quantity_sold, revenue])\n\n    report_df = pd.DataFrame(report_data, columns=['Product', 'Category', 'Quantity Sold', 'Revenue'])\n    return report_df [DOC_SEP]     if top_k < 0:\n        raise ValueError('top_k must be a positive integer.')\n    elif top_k >= len(text_dict):\n        top_k = len(text_dict)\n\n    frequencies = [text_dict.get(word, 0) for word in word_keys]\n    freq_dict = Counter(text_dict)\n    top_k_words = freq_dict.most_common(top_k)\n    word_series = pd.Series(frequencies, index=word_keys)\n    ax = word_series.plot(kind='bar')\n    return ax, dict(top_k_words) [DOC_SEP] \n    report_data = []\n\n    for product in product_list:\n        category = categories[random.randint(0, len(categories)-1)]\n        quantity_sold = random.randint(min_value, max_value)\n        revenue = random.randint(min_value, max_value)\n        total_revenue = quantity_sold * revenue\n        report_data.append([product, category, quantity_sold, revenue, total_revenue])\n\n    report_df = pd.DataFrame(report_data, columns=['Product', 'Category', 'Quantity Sold', 'Revenue', 'Total Revenue'])\n    return report_df [DOC_SEP]     word_counts = collections.Counter(' '.join(sentences_dict.values()).split())\n    frequencies = [word_counts[word] for word in word_keys]\n    word_series = pd.Series(frequencies, index=word_keys)\n    plt.figure()\n    word_series.plot(kind='bar')\n    return word_series.plot(kind='bar') [DOC_SEP]     wave = amplitude * np.exp(1j * 2 * math.pi * frequency * time)\n    window = get_window('hann', time.size)  # Apply a Hann window\n    wave *= window  # Apply the window to the wave\n\n    # Plot the wave\n    fig, ax = plt.subplots(figsize=(10, 4))\n    ax.plot(time, np.real(wave), label=\"Real Part\")\n    ax.plot(time, np.imag(wave), label=\"Imaginary Part\")\n    ax.set_title(\"Complex Wave with Hann Window\")\n    ax.set_xlabel(\"Time\")\n    ax.set_ylabel(\"Amplitude\")\n    ax.legend()\n\n    return wave, fig, ax [DOC_SEP]     # Type check for x and y\n    if not isinstance(x, np.ndarray) or not isinstance(y, np.ndarray):\n        raise TypeError(\"x and y must be numpy.ndarray\")\n\n    # Handle empty arrays\n    if x.size == 0 or y.size == 0:\n        print(\"Empty x or y array provided.\")\n        return None, np.array([])  # Adjusted to return a tuple\n\n    # Check for mismatched array sizes\n    if len(x) != len(y):\n        raise ValueError(\"Mismatched array sizes: x and y must have the same length\")\n\n    Z = np.zeros((len(y), len(x)), dtype=float)\n    for i in range(len(y)):\n        for j in range(len(x)):\n            z = complex(x[j], y[i])\n            Z[i, j] = cmath.phase(z**2 - 1)\n\n    fig, ax = plt.subplots()\n    c = ax.imshow(Z, extent=(np.amin(x), np.amax(x), np.amin(y), np.amax(y)), origin='lower', cmap='hsv')\n    fig.colorbar(c, ax=ax, label=\"Phase (radians)\")\n    ax.grid()\n\n    return ax, Z [DOC_SEP] \n    # Type check for x and y\n    if not isinstance(x, np.ndarray):\n        raise TypeError(\"x must be numpy.ndarray\")\n\n    real_part = norm.pdf(x, 0, 1)\n    imag_part = norm.pdf(x, 2, 2)\n    complex_dist = real_part + 1j * imag_part\n\n    plt.plot(x, complex_dist.real, label='Real part')\n    plt.plot(x, complex_dist.imag, label='Imaginary part')\n    plt.legend()\n    plt.grid()\n    plt.show()\n    return complex_dist [DOC_SEP]     try:\n        # Convert JSON string to Python dictionary\n        data = json.loads(json_list)\n\n        # Extract number_list from dictionary\n        number_list = data['number_list']\n        return list(itertools.combinations(number_list, r))\n    except Exception as e:\n        raise e [DOC_SEP]     x = data_dict[data_keys[0]]\n    y = data_dict[data_keys[1]]\n    correlation, _ = stats.pearsonr(x, y)\n    \n    fig, ax = plt.subplots()\n    ax.scatter(x, y)\n    \n    return correlation, ax [DOC_SEP]     if not os.path.exists(file_location):\n        raise FileNotFoundError(f\"No file found at {file_location}\")\n\n    try:\n        df = pd.read_excel(file_location, sheet_name=sheet_name)\n    except ValueError as e:\n        raise ValueError(f\"Error reading sheet: {e}\")\n\n    result = {}\n    fig, ax = plt.subplots()\n    for column in df.columns:\n        mean = np.mean(df[column])\n        std = np.std(df[column])\n        result[column] = {\"mean\": mean, \"std\": std}\n\n        ax.bar(column, mean, yerr=std)\n\n    ax.set_title('Mean and Standard Deviation')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Values')\n\n    return result, fig [DOC_SEP]     try:\n        logging.info('Reading the Excel file.')\n        # Reading the Excel file\n        df = pd.read_excel(excel_file_location, sheet_name=sheet_name)\n\n        logging.info('Converting to CSV.')\n        # Converting to CSV\n        df.to_csv(csv_file_location, index=False)\n\n        # Calculating the sum of each column\n        column_sum = df.sum(numeric_only=True)\n    except FileNotFoundError:\n        logging.error(f\"Excel file not found at {excel_file_location}\")\n        raise FileNotFoundError(f\"Excel file not found at {excel_file_location}\")\n    except ValueError as e:\n        logging.error(f\"Error in processing Excel file: {e}\")\n        raise ValueError(f\"Error in processing Excel file: {e}\")\n\n    return column_sum.to_dict() [DOC_SEP]     if not os.path.exists(original_file_location):\n        raise FileNotFoundError(f\"No file found at {original_file_location}\")\n\n    # Read data from the original Excel file\n    try:\n        original_df = pd.read_excel(original_file_location, sheet_name=sheet_name)\n    except ValueError as e:\n        raise ValueError(f\"Error reading sheet: {e}\")\n\n    # Write data to a new Excel file\n    original_df.to_excel(new_file_location, index=False)\n\n    # Read and return data from the new Excel file\n    new_df = pd.read_excel(new_file_location)\n    return new_df [DOC_SEP]     # Check input types\n    if not all(isinstance(n, int) and n >= 0 for n in numbers):\n        raise ValueError(\"All elements in the list must be integers\")\n    with Pool() as pool:\n        factorial_dict = dict(pool.starmap(calculate_factorial, [(i,) for i in numbers]))\n    return factorial_dict [DOC_SEP] \n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"The input df is not a DataFrame\")\n    \n    X = df[FEATURES]\n    y = df[TARGET]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    return model [DOC_SEP]     random.seed(seed)\n    if n < 1 or n > len(WORDS):\n        raise ValueError('n must be greater than 0')\n    random.shuffle(WORDS)\n    selected_words = WORDS[:n]\n    counts = Counter(selected_words)\n\n    with open(file_name, 'w') as f:\n        json.dump(dict(counts), f)\n\n    return file_name [DOC_SEP] \n    fig, ax = plt.subplots()\n    color = random.choice(COLORS)  # Randomly select color from the COLORS constant\n    ax.hist(number_list, bins=bins, color=color)\n    ax.set_title('Histogram')\n    ax.set_xlabel('Number')\n    ax.set_ylabel('Frequency')\n    return ax [DOC_SEP]     if not all(isinstance(activity, datetime) for activity in activities):\n        raise TypeError('All activities must be datetime objects')\n    activity_counts = defaultdict(int)\n\n    # Count the activities for each day of the week\n    for activity in activities:\n        day = activity.strftime('%A')\n        activity_counts[day] += 1\n\n    days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n    counts = [activity_counts[day] for day in days]\n\n    plt.figure()\n    fig, ax = plt.subplots()\n    ax.bar(days, counts)\n    ax.set_xlabel('Day of the Week')\n    ax.set_ylabel('Number of Activities')\n    ax.set_title('Weekly Activity')\n    \n    return ax [DOC_SEP]     # Setting the seed for reproducibility\n    random.seed(seed)\n    # Constants\n    files = os.listdir(src_dir)\n    if len(files) == 0:\n        raise FileNotFoundError(f\"No files found in {src_dir}\")\n\n    # Selecting a random file\n    file_name = random.choice(files)\n    \n    # Creating the source and destination paths\n    src_file = os.path.join(src_dir, file_name)\n    dest_file = os.path.join(dest_dir, file_name)\n\n    # Moving the file\n    shutil.move(src_file, dest_file)\n\n    # Returning the name of the moved file\n    return file_name [DOC_SEP]     fig, ax = plt.subplots()\n    ax.hist(l, bins='auto', density=True, alpha=0.6, color='g')\n\n    mu, std = stats.norm.fit(l)\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mu, std)\n    ax.plot(x, p, 'k', linewidth=2)\n    title = \"Fit results: mu = %.2f,  std = %.2f\" % (mu, std)\n    ax.set_title(title)\n    return ax [DOC_SEP]     # Check if directory exists\n    if not os.path.exists(directory_path):\n        raise FileNotFoundError(f\"Directory {directory_path} not found.\")\n    \n    json_files = glob.glob(directory_path + '/*.json')\n    processed_files = []\n    \n    for json_file in json_files:\n        with open(json_file, 'r') as file:\n            data = json.load(file)\n        \n        escaped_data = json.dumps(data, ensure_ascii=False)\n        escaped_data = re.sub(r'(?<!\\\\)\"', r'\\\\\\\"', escaped_data)\n        \n        with open(json_file, 'w') as file:\n            file.write(escaped_data)\n        \n        processed_files.append(json_file)\n    \n    return processed_files [DOC_SEP] \n    scaler = MinMaxScaler()\n    l_scaled = scaler.fit_transform(l.reshape(-1, 1))\n    df = pd.DataFrame(l_scaled, columns=['Scaled Values'])\n    return df [DOC_SEP]     docx_files = glob.glob(directory_path + '/*.docx')\n    processed_files = 0\n\n    for docx_file in docx_files:\n        document = Document(docx_file)\n\n        for paragraph in document.paragraphs:\n            paragraph.text = re.sub(r'(?<!\\\\)\"', r'\\\"', paragraph.text)\n\n        document.save(docx_file)\n        processed_files += 1\n\n    return processed_files [DOC_SEP] \n    def func(x, a, b):\n        return a * x**2 + b\n\n    params, _ = curve_fit(func, x_data, l)\n    fitted_values = func(x_data, *params)\n    \n    if plot:\n        fig, ax = plt.subplots(figsize=(6, 4))\n        ax.scatter(x_data, l, label='Data')\n        ax.plot(x_data, fitted_values, label='Fitted function')\n        ax.legend(loc='best')\n        return params, fitted_values, ax\n\n    return params, fitted_values [DOC_SEP]     if not os.path.isdir(directory_path):\n        raise FileNotFoundError('The specified directory does not exist.')\n    xlsx_files = glob.glob(directory_path + '/*.xlsx')\n    processed_files = 0\n\n    for xlsx_file in xlsx_files:\n        workbook = load_workbook(filename=xlsx_file)\n\n        for sheet in workbook.sheetnames:\n            for row in workbook[sheet].iter_rows():\n                for cell in row:\n                    if isinstance(cell.value, str):\n                        cell.value = re.sub(r'(?<=(^|[^\\\\])(\\\\\\\\)*)\"', r'\\\"',\n                                            cell.value)\n\n        workbook.save(xlsx_file)\n        processed_files += 1\n\n    return processed_files [DOC_SEP]     pca = PCA(n_components=2)\n    principalComponents = pca.fit_transform(l)\n    \n    fig = plt.figure(figsize=(6, 4))\n    ax = fig.add_subplot(111)\n    plt.scatter(principalComponents[:, 0], principalComponents[:, 1])\n    plt.xlabel('First Principal Component')\n    plt.ylabel('Second Principal Component')\n    plt.title('PCA Result')\n\n    return ax [DOC_SEP]     words = re.split(r'\\W+', text.lower())\n    words = [word for word in words if word not in STOPWORDS and word != '']\n    word_freq = dict(Counter(words))\n\n    return word_freq [DOC_SEP]     cpu_usage = psutil.cpu_percent(interval=1)\n    memory_info = psutil.virtual_memory()\n    disk_usage = psutil.disk_usage(os.sep)\n\n    table = Texttable()\n    table.add_rows([\n        ['Item', 'Value'],\n        ['CPU Usage (%)', cpu_usage],\n        ['Memory Usage (%)', memory_info.percent],\n        ['Disk Usage (%)', disk_usage.percent]\n    ])\n    return table.draw() [DOC_SEP]     if not os.path.exists(data_dir):\n        raise FileNotFoundError(f\"The directory '{data_dir}' does not exist.\")\n\n    data_files = sorted(glob.glob(os.path.join(data_dir, '*.csv')))\n    if not data_files:\n        raise ValueError(f\"No CSV files found in the directory '{data_dir}'.\")\n\n    summary_data = []\n    for file in data_files:\n        try:\n            data = pd.read_csv(file)\n            summary_data.append([os.path.basename(file), data.shape[0], data.shape[1]])\n        except pd.errors.EmptyDataError:\n            # Handle empty CSV file\n            raise pd.errors.EmptyDataError(f\"Error when reading file '{file}'.\")\n        data = pd.read_csv(file)\n        summary_data.append([os.path.basename(file), data.shape[0], data.shape[1]])\n\n    table = Texttable()\n    table.add_rows([['File', 'Rows', 'Columns']] + summary_data)\n\n    return table.draw() [DOC_SEP] \n    data = np.random.randint(0,100,size=(length, len(COLUMNS)))\n    df = pd.DataFrame(data, columns=COLUMNS)\n\n    return df [DOC_SEP]     for filename in os.listdir(directory):\n        match = re.search(r'\\.(.*?)$', filename)\n        if match:\n            ext_dir = os.path.join(directory, match.group(1))\n            if not os.path.exists(ext_dir):\n                os.mkdir(ext_dir)\n            shutil.move(os.path.join(directory, filename), ext_dir) [DOC_SEP]     \n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file '{file_path}' does not exist.\")\n    \n    df = pd.read_csv(file_path)\n    \n    if target_column not in df.columns:\n        raise ValueError(f\"The specified target column '{target_column}' does not exist in the CSV file.\")\n    \n    # Drop rows with any NaN values\n    df_cleaned = df.dropna()\n\n    X = df_cleaned.drop(target_column, axis=1)\n    y = df_cleaned[target_column]\n    \n    # Option to scale features if needed\n    # scaler = StandardScaler()\n    # X_scaled = scaler.fit_transform(X)\n    \n    clf = RandomForestClassifier(random_state=seed)\n    clf.fit(X, y)\n    importances = clf.feature_importances_\n    \n    fig, ax = plt.subplots()\n    sns.barplot(x=X.columns, y=importances, ax=ax)\n    ax.set_title('Feature Importances')\n    \n    return ax, importances [DOC_SEP] \n    MU = 0\n    SIGMA = 1\n    \n    distribution = np.random.normal(MU, SIGMA, length)\n    fig, ax = plt.subplots()\n    ax.hist(distribution, 30, density=True, label='Histogram')\n    ax.plot(np.sort(distribution), norm.pdf(np.sort(distribution), MU, SIGMA), \n            linewidth=2, color='r', label='PDF')\n    ax.legend()\n    \n    return distribution, ax [DOC_SEP]     blob = TextBlob(text.lower())\n    words_freq = Counter([' '.join(list(span)) for span in blob.ngrams(n=n)])  # Get n-grams and count frequency\n    words_freq_filtered = words_freq.most_common(top_k)  # Get top k n-grams\n    top_df = pd.DataFrame(words_freq_filtered, columns=['n-gram', 'Frequency'])\n    plt.figure()\n\n    return sns.barplot(x='n-gram', y='Frequency', data=top_df) [DOC_SEP]     if max_count < 1:\n        raise ValueError(\"max_count must be a positive integer\")\n\n    random.seed(seed)\n\n    reversed_dict = {v: [] for v in animal_dict.values() if isinstance(v, str) and v in ANIMALS}\n    for k, v in animal_dict.items():\n        if isinstance(v, str) and v in ANIMALS:\n            reversed_dict[v].append(k)\n\n    animal_counter = collections.Counter(itertools.chain.from_iterable([[v] * random.randint(1, max_count) for v in ANIMALS]))\n    return reversed_dict, animal_counter [DOC_SEP]     fruit_list = [item for item in fruit_dict.values() if isinstance(item, str) and item in FRUITS]\n    fruit_counter = Counter(fruit_list)\n    \n    plt.bar(fruit_counter.keys(), fruit_counter.values())\n    return Counter([item for item in fruit_dict.values() if isinstance(item, str)]), plt.gca() [DOC_SEP] \n    # Generate random data and create a DataFrame\n    data = np.random.randint(min_value, max_value, size=(length, len(COLUMNS)))\n    df = pd.DataFrame(data, columns=COLUMNS)\n\n    # Calculate the cumulative distribution function (CDF) for each column\n    df = df.apply(lambda x: x.value_counts().sort_index().cumsum())\n\n    return df [DOC_SEP]     if max_range < 1:\n        raise ValueError(\"max_range must be a positive integer\")\n\n    np.random.seed(seed)\n    city_population = {\n        city: (np.random.randint(1, max_range) if city in CITIES else -1) \n        for _, city in city_dict.items() if isinstance(city, str)\n    }\n\n    # Plotting the bar chart\n    plt.figure()\n    ax = plt.bar(city_population.keys(), city_population.values())\n    plt.xlabel('City')\n    plt.ylabel('Population')\n    plt.title('City Populations')\n\n    return city_population, plt.gca() [DOC_SEP] \n    counter = {column: collections.Counter() for column in my_tuple}\n\n    for csv_file in path_csv_files:\n        df = pd.read_csv(csv_file)\n\n        for column in my_tuple:\n            if column in df:\n                counter[column].update(df[column])\n\n    return counter [DOC_SEP]     pattern = re.compile(r'(like|what)', re.IGNORECASE)\n    interesting_files = [file for file in os.listdir(directory) if pattern.search(file)]\n\n    if not os.path.exists(os.path.join(directory, 'Interesting Files')):\n        os.mkdir(os.path.join(directory, 'Interesting Files'))\n\n    for file in interesting_files:\n        shutil.move(os.path.join(directory, file), os.path.join(directory, 'Interesting Files'))\n\n    return interesting_files [DOC_SEP] \n    if \"URL\" not in csv_url_dict or not csv_url_dict:\n        raise ValueError(\"The dictionary must contain a 'URL' key.\")\n    \n    response = requests.get(csv_url_dict[\"URL\"])\n    response.raise_for_status()  # Raise an exception for invalid responses\n    csv_data = response.text\n    df = pd.read_csv(StringIO(csv_data))\n    sorted_df = df.sort_values(by=sort_by_column)\n    return sorted_df [DOC_SEP]     if not os.path.exists(archive_dir):\n        os.makedirs(archive_dir)\n\n    json_files = glob.glob(os.path.join(directory, '*.json'))\n    error_messages = []\n\n    for json_file in json_files:\n        try:\n            shutil.move(json_file, archive_dir)\n        except Exception as e:\n            error_message = f'Unable to move {json_file} due to {str(e)}'\n            error_messages.append(error_message)\n\n    return (len(error_messages) == 0, error_messages) [DOC_SEP]     # Filter the DataFrame to select the specific group\n    group_data = df[df[group_col] == group_name]\n    if group_data.empty:\n        raise ValueError\n    \n    # Create a figure and axes\n    fig, ax = plt.subplots()\n\n    # Get the number of bars\n    num_bars = len(group_data)\n\n    # Set the width of the bars\n    bar_width = 0.35\n\n    # Generate positions for the bars\n    index = np.arange(num_bars)\n\n    # Create the bar chart\n    bars = ax.bar(index, group_data[value_col], bar_width, color=COLORS[:num_bars])\n\n    # Set labels and title\n    ax.set_xlabel(group_col)\n    ax.set_ylabel(value_col)\n    ax.set_title(f'Bar chart of {value_col} for {group_name}')\n\n    # Set x-axis ticks and labels\n    ax.set_xticks(index)\n    ax.set_xticklabels(group_data[group_col])\n\n    return ax [DOC_SEP]     np.random.seed(seed)\n    samples = np.random.normal(mu, sigma, num_samples)\n\n    fig = plt.figure(figsize=(12, 6))\n    plt.subplot(1, 2, 1)\n    plt.hist(samples, bins=30, density=True, alpha=0.6, color='g')\n\n    plt.subplot(1, 2, 2)\n    stats.probplot(samples, dist=\"norm\", plot=plt)\n\n    return fig [DOC_SEP]     random.seed(seed)\n    random_string = ''.join(random.choice(string.ascii_letters) for _ in range(length))\n\n    char_freq = collections.Counter(random_string)\n\n    return dict(char_freq) [DOC_SEP]     if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory '{directory}' does not exist.\")\n\n    files = natsort.natsorted(glob.glob(os.path.join(directory, file_pattern)))\n    if not files:\n        raise ValueError(f\"No files found matching pattern '{file_pattern}' in directory '{directory}'.\")\n\n    data = []\n    for filename in files:\n        with open(filename, 'r') as file:\n            content = file.read()\n        numeric_data = re.findall(regex, content)\n        data.append([os.path.basename(filename), numeric_data])\n\n    df = pd.DataFrame(data, columns=['Filename', 'Numeric Data'])\n\n    return df [DOC_SEP]     if sample_size <= 0:\n        raise ValueError('sample_size must be a positive integer.')\n\n    np.random.seed(seed)\n    sample = np.random.normal(mu, sigma, sample_size)\n    density = stats.gaussian_kde(sample)\n\n    x = np.linspace(min(sample), max(sample), sample_size)\n    fig, ax = plt.subplots()\n    ax.plot(x, density(x))\n    \n    return ax [DOC_SEP] \n    try:\n        response = urllib.request.urlopen(API_URL)\n        data = json.loads(response.read())\n        ip = data['ip']\n        if re.match(IP_REGEX, ip):\n            return ip\n        else:\n            return 'Invalid IP address received'\n    except Exception as e:\n        return str(e) [DOC_SEP]     if not os.path.exists(file_path):\n        return False\n\n    with open(file_path, 'r') as file:\n        try:\n            data = json.load(file)\n        except json.JSONDecodeError:\n            return False\n\n    return isinstance(data, list) and all(isinstance(item, dict) for item in data) [DOC_SEP]     if frequency < 0:\n        raise ValueError(\"Frequency cannot be negative\")\n    if sample_size <= 0:\n        raise ValueError(\"Sample size cannot be negative or zero\")\n\n    x = np.linspace(0, 2 * math.pi, sample_size)\n    y_sin = np.sin(frequency * x)\n    y_cos = np.cos(frequency * x)\n\n    plt.figure()\n    fig, ax = plt.subplots()\n    ax.plot(x, y_sin, label='sin')\n    ax.plot(x, y_cos, label='cos')\n    ax.legend()\n    return fig, ax"
}