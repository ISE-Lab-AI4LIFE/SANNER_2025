{
  "id": "v0_1_0-00000-of-00001_document_18",
  "text": "    if seed is not None:\n        random.seed(seed)\n\n    report_data = []\n\n    for student in students:\n        grades = [random.randint(0, 100) for _ in subjects]\n        avg_grade = statistics.mean(grades)\n        report_data.append((student,) + tuple(grades) + (avg_grade,))\n\n    report_df = pd.DataFrame(report_data, columns=['Student'] + subjects + ['Average Grade'])\n\n    return report_df [DOC_SEP]     lines = input_string.split('\\\\n')\n    wrapped_lines = [textwrap.fill(line, width, break_long_words=False) for line in lines]\n    # Join wrapped lines into a single string\n    wrapped_string = '\\\\n'.join(wrapped_lines)\n    \n    # Additional processing using regular expressions (re)\n    # For example, let's replace all whole-word instances of 'is' with 'was'\n    wrapped_string = re.sub(r'\\bis\\b', 'was', wrapped_string)\n    \n    return wrapped_string [DOC_SEP]     # Handling negative input\n    if max_length < 1:\n        raise ValueError(\"max_length must be larger than or equal to 1.\")\n\n    # Constants within the function for better encapsulation\n    LETTERS = string.ascii_lowercase\n\n    # Setting the seed for the random number generator for reproducibility\n    if seed is not None:\n        random.seed(seed)\n\n    all_combinations = []\n\n    for i in range(n_samples):\n        random_length = random.randint(1, max_length)\n        combination = ''.join(random.choices(LETTERS, k=random_length))\n        all_combinations.append(combination)\n\n\n    # Simplifying the reduction using native functionality\n    return all_combinations [DOC_SEP]     summary = {}\n    for filename in os.listdir(directory_path):\n        if any(char in INVALID_CHARACTERS for char in filename):\n            if not os.path.exists(os.path.join(directory_path, 'Invalid')):\n                os.mkdir(os.path.join(directory_path, 'Invalid'))\n            shutil.move(os.path.join(directory_path, filename), os.path.join(directory_path, 'Invalid'))\n            summary['Invalid'] = summary.get('Invalid', 0) + 1\n        else:\n            extension = os.path.splitext(filename)[-1].strip('.')\n            if not os.path.exists(os.path.join(directory_path, extension)):\n                os.mkdir(os.path.join(directory_path, extension))\n            shutil.move(os.path.join(directory_path, filename), os.path.join(directory_path, extension))\n            summary[extension] = summary.get(extension, 0) + 1\n    return summary [DOC_SEP] \n    if not isinstance(numbers, list):\n        raise TypeError(\"numbers should be a list of integers.\")\n    \n    if not all(isinstance(number, int) for number in numbers):\n        raise TypeError(\"numbers should be a list of integers.\")\n    \n    if not all(number >= 0 for number in numbers):\n        raise ValueError(\"each number in numbers should be non negative.\")\n\n    if len(numbers) == 0:\n        return [], []\n\n    all_permutations = list(permutations(numbers))\n    sums = [reduce(lambda a, b: a + b, [math.factorial(n) for n in permutation]) for permutation in all_permutations]\n    return sums, all_permutations [DOC_SEP]     strings = [''.join(random.choice(VALID_CHARACTERS) for _ in range(string_length)) for _ in range(n_strings)]\n    character_counts = collections.Counter(''.join(strings))\n    return dict(character_counts) [DOC_SEP]     if seed is not None:\n        np.random.seed(seed)\n\n    if high <= low:\n        raise ValueError(\"The 'high' parameter must be greater than 'low'.\")\n\n    matrix = np.random.randint(low, high, shape)\n    values = matrix.flatten()\n\n    all_pairs = list(combinations(values, 2))\n\n    sum_of_products = reduce(lambda a, b: a + b, [np.prod(pair) for pair in all_pairs])\n\n    return sum_of_products, matrix [DOC_SEP]     \n    warnings.simplefilter('always')\n    transferred_files = []  # Ensure this is reset each time the function is called\n\n    for ext in EXTENSIONS:\n        for src_file in glob.glob(os.path.join(SOURCE_DIR, '*' + ext)):\n            try:\n                shutil.move(src_file, DEST_DIR)\n                transferred_files.append(os.path.basename(src_file))\n            except Exception as e:\n                warnings.warn(f\"Unable to move file {src_file}: {str(e)}\")\n\n    time.sleep(1)  # To ensure all warnings are processed\n    return transferred_files [DOC_SEP]     LETTERS = string.ascii_lowercase\n    if seed is not None:\n        random.seed(seed)\n    letters = [random.choice(LETTERS) for _ in range(n)]\n    letter_counts = Counter(letters)\n    return letter_counts [DOC_SEP]     warnings.simplefilter('always')\n    iris = datasets.load_iris()\n    # Set random_state to any fixed number to ensure consistency in data splitting\n    X_train, X_test, y_train, y_test = model_selection.train_test_split(\n        iris.data, iris.target, test_size=0.33, random_state=42)\n    \n    # Initialize the classifier with a fixed random_state\n    clf = svm.SVC(random_state=42)\n    clf.fit(X_train, y_train)\n    predictions = clf.predict(X_test)\n    accuracy = metrics.accuracy_score(y_test, predictions)\n\n    warning_msg = None\n    if accuracy < 0.9:\n        warning_msg = \"The accuracy of the SVM classification is below 0.9.\"\n        warnings.warn(warning_msg)\n\n    return accuracy, warning_msg [DOC_SEP]     if seed is not None:\n        random.seed(seed)\n    rand_str = ''.join(random.choice(string.ascii_letters + string.digits) for _ in range(n))\n    matches = re.findall(pattern, rand_str)\n    return matches [DOC_SEP]     seed(42)  # Set the seed for reproducibility\n    baskets = []\n    for list_ in list_of_lists:\n        basket = Counter()\n        for _ in list_:\n            basket[choice(POSSIBLE_ITEMS)] += 1\n        baskets.append(basket)\n\n    return baskets [DOC_SEP]     LETTERS = string.ascii_lowercase\n    random.seed(seed)\n    letter_dict = defaultdict(list)\n    for _ in range(n):\n        letter = random.choice(LETTERS)\n        letter_dict[letter].append(letter)\n    return letter_dict [DOC_SEP]     sums = []\n    for list_ in list_of_lists:\n        sum_ = sum(math.pow(x, 2) for x in POSSIBLE_NUMBERS[:len(list_)])\n        sums.append(sum_)\n\n    return sums [DOC_SEP] \n    if len(fruit_data) == 0:\n        return pd.DataFrame()\n\n    # Unpacking the fruit names and counts separately\n    fruits, counts = zip(*fruit_data)\n    fruits = unique_values = list(set(fruits))\n    # Calculating total counts\n    total_counts = {fruit: np.sum([count for fruit_, count in fruit_data if fruit_ == fruit])\n                  for fruit in fruits}\n    # Calculating average counts\n    avg_counts = {fruit: np.mean([count for fruit_, count in fruit_data if fruit_ == fruit])\n                  for fruit in fruits}\n\n    # Creating a DataFrame to hold the report\n    report_df = pd.DataFrame(list(zip(total_counts.values(), avg_counts.values())),\n                             index=fruits,\n                             columns=['Total Count', 'Average Count'])\n\n    return report_df [DOC_SEP]     # Extracting items, counts, and weights from the input data\n    items, counts, weights = zip(*data)\n    \n    # Normalizing the counts and weights\n    counts_normalized = zscore(counts)\n    scaler = MinMaxScaler()\n    weights_normalized = scaler.fit_transform(np.array(weights).reshape(-1, 1)).flatten()\n\n    # Creating a DataFrame with the normalized data\n    report_df = pd.DataFrame({\n        'Item': items,\n        'Normalized Count': counts_normalized,\n        'Normalized Weight': weights_normalized\n    })\n\n    return report_df [DOC_SEP]     items, x_values, y_values = zip(*data)\n    coordinates = np.array(list(zip(x_values, y_values)))\n\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state).fit(coordinates)\n    labels = kmeans.labels_\n\n    return labels [DOC_SEP]     # Constants\n    PUNCTUATION = string.punctuation\n\n    cleaned_texts = []\n\n    # Remove punctuation from each text string\n    for text in [text1, text2]:\n        cleaned_text = re.sub('['+re.escape(PUNCTUATION)+']', '', text)\n        cleaned_texts.append(cleaned_text)\n\n    return tuple(cleaned_texts) [DOC_SEP] \n    # Setting the seed for the random number generator\n    if rng_seed is not None:\n        seed(rng_seed)\n\n    color_cycle = cycle(colors)\n    color_pattern = []\n\n    for _ in range(n_colors):\n        color = next(color_cycle) if _ % 2 == 0 else choice(colors)\n        color_pattern.append(color)\n\n    return color_pattern [DOC_SEP] \n    if len(students) == 0:\n        raise ValueError(\"The students list should contain at least one student.\")\n\n    seed(rng_seed)\n\n    student_cycle = cycle(students)\n    grade_data = []\n\n    for _ in range(n_grades):\n        student = next(student_cycle)\n        grade = randint(min(grade_range), max(grade_range))\n        grade_data.append([student, grade])\n\n    grade_df = pd.DataFrame(grade_data, columns=['Student', 'Grade'])\n\n    return grade_df [DOC_SEP] \n    # Unzip the data, filling missing values with NaN so they don't affect the mean calculation\n    unzipped_data = list(itertools.zip_longest(*data_list, fillvalue=np.nan))\n\n    # Calculate the mean of numerical values, skipping the first column assuming it's non-numerical\n    # Filter out non-numeric values from the column before calculating the mean\n    mean_values = []\n    for column in unzipped_data[:]:\n        numeric_values = [val for val in column if isinstance(val, (int, float))]\n        if numeric_values:\n            mean_values.append(np.nanmean(numeric_values))\n        else:\n            mean_values.append(np.nan)\n\n    # Create a DataFrame with the results\n    df = pd.DataFrame(mean_values, columns=['Mean Value'], \n                      index=['Position {}'.format(i) for i in range(len(mean_values))])\n\n    return df [DOC_SEP]     # Unzipping the data to separate the elements of the tuples\n    unzipped_data = list(itertools.zip_longest(*data_list, fillvalue=np.nan))\n    mean_values = []\n    # Calculating the mean values excluding the first position (non-numerical)\n    for column in unzipped_data[1:]:\n        numeric_values = [val for val in column if isinstance(val, (int, float))]\n        if numeric_values:\n            mean_values.append(np.nanmean(numeric_values))\n        else:\n            mean_values.append(np.nan)\n\n    # Writing the mean values to the specified file\n    with open(file_name, 'w') as f:\n        for i, mean_value in enumerate(mean_values, start=1):\n            f.write('Position {}: {}\\n'.format(i, mean_value))\n    \n    # Returning the list of mean values for testing purposes\n    return mean_values [DOC_SEP]     # Unzip the data while handling uneven tuple lengths by filling missing values with NaN\n    unzipped_data = list(itertools.zip_longest(*data_list, fillvalue=np.nan))\n\n    # Calculate the mean of numeric values, ignoring non-numeric ones\n    mean_values = [np.nanmean([val for val in column if isinstance(val, (int, float))]) for column in unzipped_data]\n\n    return mean_values [DOC_SEP]     if file_path is None:\n        raise ValueError(\"The file path is invalid.\")\n\n    with open(file_path, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(headers)\n        for row in data:\n            if len(row) < len(headers):\n                row += (None,) * (len(headers) - len(row))\n            writer.writerow(row)\n    return os.path.abspath(file_path) [DOC_SEP]     distances = []\n    for point1, point2 in zip_longest(points, points[1:]):\n        if point2 is not None:\n            distances.append(distance.euclidean(point1, point2))\n            \n    return distances [DOC_SEP]     if seed is not None:\n        random.seed(seed)\n\n    df = pd.DataFrame(data, columns=columns)\n\n    if fill_missing:\n        for col in df.columns:\n            if df[col].dtype in ['float64', 'int64']:\n                df[col] = df[col].apply(lambda x: random.randint(*num_range) if pd.isnull(x) else x)\n\n    return df [DOC_SEP]     # Add the key 'a' with value 1\n    data_dict.update({'a': 1})\n\n    # Count the frequency of the values\n    counter = collections.Counter(data_dict.values())\n\n    # Sort the dictionary by the frequency\n    sorted_dict = sorted(counter.items(), key=operator.itemgetter(1), reverse=True)\n\n    # Backup files\n    backup_status = False\n    if os.path.isdir(source_directory):\n        shutil.copytree(source_directory, backup_directory, dirs_exist_ok=True)\n        backup_status = True\n\n    return data_dict, sorted_dict, backup_status [DOC_SEP]     if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"data should be a DataFrame.\")\n\n    if not data.apply(lambda s: pd.to_numeric(s, errors='coerce').notnull().all()).all():\n        raise ValueError(\"DataFrame should only contain numeric values.\")\n    \n    if n_components > len(data.columns):\n        raise ValueError(\"n_components should not be greater than the number of columns in data.\")\n    \n    scaler = StandardScaler()\n    data_scaled = scaler.fit_transform(data)\n    pca = PCA(n_components=n_components)\n    data_reduced = pca.fit_transform(data_scaled)\n    return pd.DataFrame(data_reduced) [DOC_SEP]     data = pd.DataFrame(data)\n    if data.empty or target not in data.columns:\n        raise ValueError(\"Data must not be empty and target column must exist in the DataFrame.\")\n\n    # Splitting the data into training and test sets\n    X_train, X_test, y_train, y_test = train_test_split(\n        data.drop(columns=[target]), data[target], test_size=test_size, random_state=random_state\n    )\n\n    # Training the model\n    model = RandomForestRegressor(random_state=random_state)\n    model.fit(X_train, y_train)\n\n    # Making predictions and returning the MSE\n    predictions = model.predict(X_test)\n    mse = mean_squared_error(y_test, predictions)\n    return mse, model, data [DOC_SEP]     # Check if DataFrame is empty\n    if data.empty:\n        raise ValueError(\"The input DataFrame is empty.\")\n\n    # Check if specified columns exist\n    if col1 not in data or col2 not in data:\n        raise ValueError(f\"One or both of the columns '{col1}' and '{col2}' do not exist in the DataFrame.\")\n\n    # Check for non-categorical data (numerical values)\n    if np.issubdtype(data[col1].dtype, np.number) or np.issubdtype(data[col2].dtype, np.number):\n        raise TypeError(\"One or both of the columns contain non-categorical data. The chi-square test requires categorical data.\")\n\n    # Check for single category (no variability)\n    if len(data[col1].unique()) < 2 or len(data[col2].unique()) < 2:\n        raise ValueError(\"One or both of the columns do not have multiple categories. The chi-square test requires variability in data.\")\n\n    # Check for small counts in numerous categories\n    contingency_table = pd.crosstab(data[col1], data[col2])\n    if (contingency_table < 5).any().any():\n        raise ValueError(\"Some categories have less than 5 observations. This violates the assumptions of the chi-square test.\")\n\n    # Perform the chi-square test\n    chi2, p, dof, expected = chi2_contingency(contingency_table)\n    return p [DOC_SEP]     if not data.apply(lambda s: pd.to_numeric(s, errors='coerce').notnull().all()).all():\n        raise ValueError(\"DataFrame should only contain numeric values.\")\n\n    kmeans = KMeans(n_clusters=n_clusters, random_state=seed, n_init=10)\n    kmeans.fit(data)\n\n    return kmeans.labels_, kmeans [DOC_SEP]     df = pd.read_csv(csv_file)\n    matches = df[df[column_name].str.contains(pattern, na=False)]\n\n    if sample_size is not None:\n        random.seed(seed)  # Set the seed for reproducibility\n        sample_size = min(sample_size, len(matches))  # Ensure sample size is not greater than the number of matches\n        sampled_indices = random.sample(range(len(matches)), sample_size)  # Randomly select indices\n        matches = matches.iloc[sampled_indices]  # Select rows corresponding to sampled indices\n\n    return matches [DOC_SEP] \n    if not os.path.isfile(db_file):\n        raise ValueError('db_file does not exist.')\n\n    conn = sqlite3.connect(db_file)\n    df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn)\n\n    if df[column_name].dtype == 'object':  # Check if the column data type is a string\n        matches = df[df[column_name].str.contains(pattern)]\n    else:\n        matches = pd.DataFrame(columns=df.columns)  # Return an empty DataFrame\n\n    return matches [DOC_SEP]     # Filter rows based on column_b and column_c\n    filtered_df = df[(df[column_b] > 50) & (df[column_c] == 900)]\n\n    if filtered_df[column_a].nunique() <= 1:\n        return True\n\n    # If dataframe is empty after filtering, return False\n    if filtered_df.empty:\n        return True\n\n    # Perform Augmented Dickey-Fuller test\n    adf_result = adfuller(filtered_df[column_a])\n    p_value = adf_result[1]\n    return p_value <= 0.05 [DOC_SEP]     if len(columns) != 3:\n        raise ValueError(\"Exactly three columns should be specified.\")\n    \n    for column in columns:\n        if column not in df.columns:\n            raise ValueError('The specified columns should exist in the DataFrame.')\n    \n    col_categorical, col_numerical, col_filter = columns\n\n    # Filtering the data based on the specified conditions\n    selected = df[(df[col_numerical] > larger) & (df[col_filter] == equal)][[col_categorical, col_numerical]]\n\n    # Creating a contingency table for the chi-square test\n    contingency_table = pd.crosstab(selected[col_categorical], selected[col_numerical])\n    \n    # Check if the contingency table is empty (no data meeting the criteria)\n    if contingency_table.size == 0:\n        raise ValueError(\"Insufficient data - no matching data for the applied conditions.\")\n    \n    # Performing the chi-square test\n    _, p_value, _, _ = chi2_contingency(contingency_table)\n    \n    return p_value [DOC_SEP]     # Validating the input dataframe\n    if df.empty or not all(col in df for col in [col_a, col_b, col_c]):\n        return None  # Invalid input scenario\n    \n    try:\n        # Ensuring the columns contain numeric data\n        df[[col_a, col_b, col_c]] = df[[col_a, col_b, col_c]].apply(pd.to_numeric, errors='raise')\n    except ValueError:\n        return None  # Non-numeric data encountered\n\n    # Filtering the data based on the conditions\n    selected = df[(df[col_b] > 50) & (df[col_c] == 900)][[col_a, col_b]]\n\n    if selected.empty:\n        return None\n    \n    # Preparing the data for linear regression\n    X_train, X_test, y_train, _ = train_test_split(selected[col_a].values.reshape(-1, 1),\n                                                   selected[col_b].values,\n                                                   test_size=0.2,\n                                                   random_state=seed)\n\n    # Applying linear regression\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    predictions = model.predict(X_test)\n\n    return predictions, model [DOC_SEP] \n    if not all(key in data for key in ['Name', 'Age', 'Score']):\n        raise ValueError(\"The dictionary must have the keys 'Name', 'Age', 'Score'\")\n\n    # Creating a dataframe and sorting it\n    df = pd.DataFrame(data).sort_values(['Name', 'Age'])\n\n    # Calculating average scores\n    avg_scores = df.groupby('Name')['Score'].mean()\n\n    # Getting the most common age\n    age_counts = Counter(df['Age'])\n    most_common_age = age_counts.most_common(1)[0][0] if age_counts else None\n\n    return df, avg_scores, most_common_age [DOC_SEP]     np.random.seed(seed)\n    int_list = [list(map(int, x)) for x in T1]\n    flattened_list = list(itertools.chain(*int_list))\n    total_cols = sum(flattened_list)\n\n    data = np.random.randint(0, 100, size=(row_num, total_cols))\n    df = pd.DataFrame(data, columns=[f'Col_{i+1}' for i in range(total_cols)])\n\n    return df [DOC_SEP]     merged_df = pd.DataFrame()\n\n    for file in csv_files:\n        file_path = os.path.join(data_dir, file)\n        df = pd.read_csv(file_path)\n        merged_df = pd.concat([merged_df, df], ignore_index=True)\n\n    return merged_df [DOC_SEP]     file_path = os.path.join(data_dir, csv_file)\n    try:\n        df = pd.read_csv(file_path)\n    except pd.errors.EmptyDataError:\n        return pd.DataFrame()\n\n    for column in df.columns:\n        if np.issubdtype(df[column].dtype, np.number):  # checking for numeric columns\n            df[column].fillna(df[column].mean(), inplace=True)\n\n    return df [DOC_SEP] \n    random.seed(seed)\n\n    file = csv_files[random.randint(0, len(csv_files) - 1)]\n    file_path = os.path.join(data_dir, file)\n\n    try:\n        df = pd.read_csv(file_path)\n    except pd.errors.EmptyDataError:\n        return file, pd.DataFrame()\n\n    selected_rows = df.sample(n=random.randint(1, len(df)), random_state=seed)\n\n    return file, selected_rows [DOC_SEP]     df = pd.read_csv(csv_file_path)\n    X = df.drop(columns=[attribute])\n    y = df[attribute]\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=test_size, random_state=random_state\n    )\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    predictions = model.predict(X_test)\n    return model, predictions [DOC_SEP]     if not strings:\n        return Counter()\n\n    pattern = '}'\n    random_choices = random.choices(strings, k=10)\n    pattern_counts = Counter([string.count(pattern) for string in random_choices])\n\n    return pattern_counts [DOC_SEP]     \n    error_times = []\n    total_time = 0\n\n    for log in logs:\n        if \"ERROR\" in log:\n            time_match = re.search(r'(\\d{2}):(\\d{2}):\\d{2}', log)\n            if time_match:\n                hour, minute = map(int, time_match.groups())\n                error_times.append(time(hour, minute))\n                total_time += hour * 60 + minute\n\n    if error_times:\n        avg_hour = (total_time // len(error_times)) // 60\n        avg_minute = (total_time // len(error_times)) % 60\n        avg_time = time(avg_hour, avg_minute)\n    else:\n        avg_time = time(0, 0)\n\n    return error_times, avg_time [DOC_SEP]     array = np.random.randint(1, 100, size=ARRAY_SIZE)\n    mean = np.mean(array)\n    std = np.std(array)\n\n    fig, ax = plt.subplots()\n    ax.hist(array, bins='auto')\n    ax.set_title('Histogram of Random Integers')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    ax.axvline(mean, color='red', linestyle='dashed', linewidth=1)\n    ax.axvline(mean + std, color='purple', linestyle='dashed', linewidth=1)\n    ax.axvline(mean - std, color='purple', linestyle='dashed', linewidth=1)\n    ax.legend([\"Mean\", \"Standard Deviation\"])\n    plt.show()\n    \n    return array, mean, std, ax [DOC_SEP]     array = np.random.randint(1, 500, size=ARRAY_SIZE)\n    mean = np.mean(array)\n    std = np.std(array)\n\n    fig, ax = plt.subplots()\n    ax.hist(array, bins='auto')\n    ax.set_title('Histogram of Random Values')\n    ax.set_xlabel('Val')\n    ax.set_ylabel('Freq')\n    return array, mean, std, ax [DOC_SEP]     random.seed(seed)\n    strings = [''.join(random.choices(['a', 'b', 'c', 'd', 'e'], k=length)) for _ in range(count)]\n    letter_frequency = Counter(itertools.chain(*strings))\n    \n    return letter_frequency [DOC_SEP]     if seed is not None:\n        random.seed(seed)\n        \n    outcomes = [random.choice(NUMBERS) for _ in range(rolls)]\n    frequencies = np.bincount(outcomes, minlength=7)[1:]  # Excluding 0 as dice starts from 1\n\n    # Creating histogram\n    fig, ax = plt.subplots()\n    ax.hist(outcomes, bins=np.arange(1, 7+1.5)-0.5, edgecolor='black')\n    ax.set_title('Histogram of Dice Rolls')\n    ax.set_xlabel('Dice Value')\n    ax.set_ylabel('Frequency')\n\n    return frequencies, ax [DOC_SEP]     random.seed(seed)\n\n    pairs = [tuple(random.choices(LETTERS, k=2)) for _ in range(count)]\n    pair_frequency = Counter(pairs)\n\n    return pair_frequency [DOC_SEP]     if length < 0:\n        raise ValueError(\"length must be a non-negative integer\")\n    random.seed(seed)\n    steps = [1 if random.random() > 0.5 else -1 for _ in range(length)]\n    walk = np.cumsum([0] + steps)  # Starts at 0\n    return walk"
}