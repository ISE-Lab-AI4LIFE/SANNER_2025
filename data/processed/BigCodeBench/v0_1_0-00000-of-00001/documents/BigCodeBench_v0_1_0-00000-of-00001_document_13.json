{
  "id": "BigCodeBench_v0_1_0-00000-of-00001_document_13",
  "text": "    df = pd.DataFrame(df)\n    regex = '^' + letter\n    filtered_df = df[df['Word'].str.contains(regex, regex=True)]\n    word_lengths = filtered_df['Word'].str.len()\n    statistics = {'mean': np.mean(word_lengths), 'median': np.median(word_lengths), 'mode': word_lengths.mode().values[0]}\n\n    return statistics [DOC_SEP]     start_time = time.time()\n    # Validate if 'Word' column exists in df\n    if 'Word' not in df.columns:\n        raise ValueError(\"The DataFrame should contain a 'Word' column.\")\n\n    # Handle empty DataFrame\n    if df.empty:\n        print(\"The DataFrame is empty.\")\n        return None\n\n    regex = f'^{letter}'\n    filtered_df = df[df['Word'].str.match(regex)]\n    if filtered_df.empty:\n        print(f\"No words start with the letter '{letter}'.\")\n        return None\n\n    word_lengths = filtered_df['Word'].str.len()\n    ax = sns.boxplot(x=word_lengths)\n    ax.set_title(f\"Word Lengths Distribution for Words Starting with '{letter}'\")\n    end_time = time.time()  # End timing\n    cost = f\"Operation completed in {end_time - start_time} seconds.\"\n    return ax [DOC_SEP]     if not os.path.exists(output_dir):\n        os.mkdir(output_dir)\n    matrix = pd.DataFrame(np.random.choice(LETTERS, (10, 10)))\n    matrix.to_csv(file_path, sep='\\t', header=False, index=False)\n\n    return None [DOC_SEP]     combined_matrix = np.concatenate((matrix1, matrix2), axis=1)\n    df = pd.DataFrame(combined_matrix)\n    return df.to_string(index=False, header=False) [DOC_SEP]     # Configure logging\n    logging.basicConfig(level=logging.INFO)\n\n    # Try to compile the C++ file\n    try:\n        subprocess.check_call(['g++', filepath, '-o', filepath.split('.')[0]])\n        logging.info('Successfully compiled %s', filepath)\n    except subprocess.CalledProcessError as e:\n        logging.error('Failed to compile %s: %s', filepath, e)\n\n    except FileNotFoundError as e:\n        logging.error('Compiler not found or file does not exist: %s', e) [DOC_SEP]     df = pd.DataFrame(matrix)\n\n    fig, ax = plt.subplots()\n    ax.imshow(df, cmap='hot', interpolation='nearest')\n\n    return ax [DOC_SEP]     df = pd.DataFrame(matrix)\n    normalized_df = df.apply(stats.zscore)\n    # Handle NaN values by replacing them with 0.0\n    normalized_df = normalized_df.fillna(0.0)\n    return normalized_df [DOC_SEP] \n    # Ensure tuple elements match DataFrame columns for removal\n    df = df[~df.apply(tuple, axis=1).isin(tuples)]\n\n    # Generate random plots\n    plots = []\n    for _ in range(n_plots):\n        selected_columns = sample(COLUMNS, 2)\n        ax = df.plot(x=selected_columns[0], y=selected_columns[1], kind='scatter')\n        plots.append(ax)\n\n    plt.show()\n\n    return df, plots [DOC_SEP]     if not df.empty:\n        df = df[~df.apply(tuple, axis=1).isin(tuples)]\n\n    plots = []\n    if n_plots > 0 and not df.empty:\n        available_columns = df.columns.tolist()\n        for _ in range(min(n_plots, len(available_columns) // 2)):  # Ensure we have enough columns\n            # Randomly select two columns for pairplot\n            selected_columns = sample(available_columns, 2)\n            plot = sns.pairplot(df, vars=selected_columns)\n            plots.append(plot)\n\n    return df, plots [DOC_SEP]     COLUMNS = ['A', 'B', 'C', 'D', 'E']\n    df = df.set_index(list('ABCDE')).drop(tuples, errors='ignore').reset_index()\n    plots = []\n    possible_combinations = list(combinations(COLUMNS, 2))\n    for _ in range(min(n_plots, len(possible_combinations))):\n        selected_columns = sample(possible_combinations, 1)[0]\n        possible_combinations.remove(selected_columns)\n        ax = df.plot.scatter(x=selected_columns[0], y=selected_columns[1])\n        plots.append((selected_columns, ax))\n    return df, plots [DOC_SEP]     \n    # Drop rows based on tuples\n    df = df.set_index(list('ABCDE')).drop(tuples, errors='ignore').reset_index()\n    \n    plots = []\n    # Generate plots only if DataFrame is not empty\n    if not df.empty:\n        for _ in range(n_plots):\n            selected_columns = sample(COLUMNS, 2)\n            plot = sns.jointplot(data=df, x=selected_columns[0], y=selected_columns[1])\n            plots.append(plot)\n    \n    return df, plots [DOC_SEP]     mask = df.apply(tuple, axis=1).isin(tuples)\n    df = df[~mask]\n\n    plot_details = []\n    for _ in range(min(n_plots, len(df))):\n        selected_columns = sample(COLUMNS, 2)\n        df.plot(x=selected_columns[0], y=selected_columns[1], kind='line')\n        plot_details.append((selected_columns[0], selected_columns[1]))\n\n    plt.show()\n\n    return df, plot_details [DOC_SEP]     report_data = []\n    for team in teams:\n        team_goals = goals.get(team, 0)\n        team_penalties = penalties.get(team, 0)\n        penalties_cost = team_penalties * choice(penalties_costs)\n        performance_score = np.max([0, team_goals - team_penalties])\n        report_data.append({\n            'Team': team,\n            'Goals': team_goals,\n            'Penalties': team_penalties,\n            'Penalties Cost': penalties_cost,\n            'Performance Score': performance_score\n        })\n\n    report_df = pd.DataFrame(report_data)\n    return report_df [DOC_SEP] \n    scores_data = []\n\n    for team in TEAMS:\n        team_goals = goals.get(team, 0)\n        team_penalties = penalties.get(team, 0)\n        score = team_goals - team_penalties\n        scores_data.append([team, score])\n\n    scores_df = pd.DataFrame(scores_data, columns=['Team', 'Score'])\n    scores_df['Score'] = scores_df['Score'].clip(*GOALS_RANGE)\n\n    #Plotting (commented out for testing)\n    plt.figure(figsize=(10, 6))\n    plt.bar(scores_df['Team'], scores_df['Score'], color='skyblue')\n    plt.xlabel('Team')\n    plt.ylabel('Score')\n    plt.title('Team Scores Distribution')\n    plt.ylim(GOALS_RANGE[0] - 1, GOALS_RANGE[1] + 1)\n    plt.grid(axis='y', linestyle='--')\n    plt.show()\n\n    return scores_df [DOC_SEP]     # Constants\n    TEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\n\n    data = []\n    for team in TEAMS:\n        team_goals = goals.get(team, 0)\n        team_penalties = penalties.get(team, 0)\n        data.append([team, team_goals, team_penalties])\n\n    df = pd.DataFrame(data, columns=['Team', 'Goals', 'Penalties'])\n\n    plot = sns.pairplot(df, hue='Team')\n\n    return df, plot [DOC_SEP]     # Constants\n    TEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\n    PENALTY_COST = 1000  # in dollars\n\n    if rng_seed is not None:\n        seed(rng_seed)  # Set seed for reproducibility\n\n    match_results = []\n    for team in TEAMS:\n        team_goals = randint(0, abs(goals))\n        team_penalties = randint(0, abs(penalties))\n        penalty_cost = PENALTY_COST * team_penalties\n        result_string = f\"({team_goals} goals, ${penalty_cost})\"\n        match_results.append([team, result_string])\n\n    results_df = pd.DataFrame(match_results, columns=['Team', 'Match Result'])\n\n    return results_df [DOC_SEP]     if rng_seed is not None:\n        seed(rng_seed)\n\n    # Ensure goals and penalties are treated as positive\n    goals = abs(goals)\n    penalties = abs(penalties)\n\n    match_results = []\n    for team in teams:\n        team_goals = randint(0, goals)\n        team_penalties = randint(0, penalties)\n        team_penalty_cost = penalty_cost * team_penalties\n        match_results.append([team, team_goals, team_penalty_cost])\n\n    results_df = pd.DataFrame(match_results, columns=['Team', 'Goals', 'Penalty Cost'])\n    ax = results_df.plot(kind='bar', x='Team', y=['Goals', 'Penalty Cost'], stacked=True)\n    plt.ylabel('Results')\n\n    return results_df, ax [DOC_SEP]     if rng_seed is not None:\n        seed(rng_seed)\n\n    match_results = []\n\n    for team in teams:\n        team_goals = randint(0, goals)\n        team_penalties = randint(0, penalties)\n        penalty_cost = PENALTY_COST * team_penalties\n        result_string = f\"({team_goals} goals, ${penalty_cost})\"\n        match_results.append([team, result_string])\n\n    results_df = pd.DataFrame(match_results, columns=['Team', 'Match Result'])\n\n    if not results_df.empty:\n    # Extract goals and penalty cost from the result string\n        results_df['Goals'] = results_df['Match Result'].apply(lambda x: int(re.search(r'\\((\\d+) goals', x).group(1)))\n        results_df['Penalty Cost'] = results_df['Match Result'].apply(lambda x: int(re.search(r'\\$(\\d+)', x).group(1)))\n\n        # Visualization - this part will not be tested directly in unit tests\n        ax = results_df.set_index('Team')[['Goals', 'Penalty Cost']].plot(kind='bar', stacked=True)\n        plt.ylabel('Counts')\n        plt.title('Football Match Results Analysis')\n        plt.tight_layout()\n        plt.show()\n\n    return results_df [DOC_SEP]     match_results = []\n\n    for team in TEAMS:\n        team_goals = randint(0, goals)\n        team_penalties = randint(0, penalties)\n        penalty_cost = PENALTY_COST * team_penalties\n        match_results.append([team, team_goals, penalty_cost])\n\n    results_df = pd.DataFrame(match_results, columns=['Team', 'Goals', 'Penalty Cost'])\n\n    plot1 = sns.barplot(x='Team', y='Goals', data=results_df, palette='viridis')\n    plt.close()  # Close the plot to prevent it from displaying here\n    plot2 = sns.barplot(x='Team', y='Penalty Cost', data=results_df, palette='viridis')\n    plt.close()  # Close the plot to prevent it from displaying here\n\n    return results_df, [plot1, plot2] [DOC_SEP]     if rng_seed is not None:\n        seed(rng_seed)\n\n    # Generate match results\n    match_results = []\n    for team in TEAMS:\n        team_goals = randint(0, goals)\n        team_penalties = randint(0, penalties)\n        penalty_cost = PENALTY_COST * team_penalties\n        match_results.append([team, team_goals, penalty_cost])\n\n    # Create DataFrame\n    results_df = pd.DataFrame(match_results, columns=['Team', 'Goals', 'Penalty Cost'])\n\n    # Train Linear Regression Model\n    X = results_df[['Goals']]\n    y = results_df['Penalty Cost']\n    model = LinearRegression().fit(X, y)\n\n    return results_df, model [DOC_SEP]     rows, columns = L[0][0] * L[0][1], L[1][0] * L[1][1]\n    random_array = np.random.randint(RANGE[0], RANGE[1], size=(rows, columns))\n    df = pd.DataFrame(random_array)\n    \n    return df [DOC_SEP]     data = list(chain(*L))\n    data = np.array(data).reshape(-1, 1)\n\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(data)\n\n    fig, ax = plt.subplots()\n    ax.plot(standardized_data)\n    plt.close(fig)\n    return ax [DOC_SEP]     data = list(chain(*L))\n    mu, std = norm.fit(data)\n\n    fig, ax = plt.subplots()\n    ax.hist(data, bins=30, density=True, alpha=0.6, color='g')\n\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, std)\n    ax.plot(x, p, 'k', linewidth=2)\n    title = \"Fit results: mu = %.2f,  std = %.2f\" % (mu, std)\n    ax.set_title(title)\n\n    return ax [DOC_SEP]     # Constants\n    N_CLUSTERS = 3\n\n    data = list(chain(*L))\n    data = np.array(data).reshape(-1, 1)\n\n    kmeans = KMeans(n_clusters=N_CLUSTERS).fit(data)\n\n    fig, ax = plt.subplots()\n    ax.scatter(data, [0]*len(data), c=kmeans.labels_.astype(float))\n    \n    return ax [DOC_SEP]     data = np.array(L)\n\n    pca = PCA(n_components=N_COMPONENTS)\n    pca_result = pca.fit_transform(data)\n\n    fig, ax = plt.subplots()\n    ax.scatter(pca_result[:,0], pca_result[:,1])\n\n    return pca_result, ax [DOC_SEP]     population_data = []\n\n    for city in cities_list:\n        population = math.ceil(randint(1000000, 20000000) / 1000.0) * 1000\n        population_data.append([city, population])\n\n    population_df = pd.DataFrame(population_data, columns=['City', 'Population'])\n\n    return population_df [DOC_SEP]     from_tz = pytz.timezone(from_tz)\n    to_tz = pytz.timezone(choice(TIMEZONES))\n    given_date = parse(date_str).replace(tzinfo=from_tz)\n    converted_date = given_date.astimezone(to_tz)\n\n    return converted_date.strftime('%Y-%m-%d %H:%M:%S'), to_tz.zone [DOC_SEP]     sales_data = []\n\n    for product in products_list:\n        sales = [randint(100, 500) for _ in range(12)]\n        avg_sales = mean(sales)\n        sales.append(avg_sales)\n        sales_data.append([product] + sales)\n\n    sales_df = pd.DataFrame(sales_data, columns=['Product'] + [f'Month {i+1}' for i in range(12)] + ['Average Sales'])\n\n    return sales_df [DOC_SEP]     x = [i/100 for i in range(1000)]\n    frequency = randint(1, 5)\n    amplitude = randint(1, 5)\n    phase_shift = randint(0, 360)\n\n    y = [amplitude * math.sin(2 * math.pi * frequency * (xi + phase_shift)) for xi in x]\n\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    ax.set_title('Random Sine Wave')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Amplitude')\n    ax.grid(True)\n    \n    return ax  # Return the axis object for testing [DOC_SEP]     start_time = time.time()\n\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    filepath = os.path.join(output_dir, filename)\n    with open(filepath, 'w', newline='') as f:\n        for i, df in enumerate(dataset):\n            if i > 0:\n                # Write the separator with a newline at the end only\n                f.write('------\\n')\n            # Avoid writing the index and ensure no extra newline is added at the end of the DataFrame\n            df.to_csv(f, index=False, header=True, mode='a')\n            if i < len(dataset) - 1:\n                # Add a newline after the DataFrame content, except after the last DataFrame\n                f.write('\\n')\n\n    end_time = time.time()  # End timing\n    cost = f\"Operation completed in {end_time - start_time} seconds.\" [DOC_SEP]     if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    file_path = os.path.join(output_dir, filename)\n    df_clean = df.where(pd.notnull(df), None)\n    with open(file_path, 'w') as f:\n        df_clean.to_json(f, orient='records')\n    return file_path [DOC_SEP]     # Ensure the data directory exists\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    file_path = os.path.join(output_dir, filename)\n    df.to_csv(file_path, index=False, quoting=csv.QUOTE_NONNUMERIC)\n    return os.path.abspath(file_path) [DOC_SEP]     start_time = time.time()\n    # Ensure the data directory exists\n    if not os.path.exists(OUTPUT_DIR):\n        os.makedirs(OUTPUT_DIR)\n\n    file_path = os.path.join(OUTPUT_DIR, filename)\n\n    # Save DataFrame as JSON Lines\n    with open(file_path, 'w') as file:\n        for record in df.to_dict(orient='records'):\n            json.dump(record, file)\n            file.write('\\n')\n    end_time = time.time()  # End timing\n    cost = f\"Operation completed in {end_time - start_time} seconds.\"\n    return os.path.abspath(file_path) [DOC_SEP]     # Remove duplicate words\n    stop_words = set(stopwords.words('english'))\n    text = ' '.join(sorted(set(text.split()), key=text.index))\n    # Tokenize and remove stopwords\n    words = [word for word in re.findall(r'\\b\\w+\\b', text.lower()) if word not in stop_words]\n    \n    # Create frequency distribution\n    freq_dist = {}\n    for word in words:\n        freq_dist[word] = freq_dist.get(word, 0) + 1\n    \n    return freq_dist [DOC_SEP]     # Flattening the list with multiple repetitions\n    flattened_list = np.array(list(itertools.chain(*[input_list for _ in range(repetitions)])))\n    \n    # Calculating the mode\n    mode = stats.mode(flattened_list)\n    \n    return mode [DOC_SEP]     # Pre-processing the text\n    # Remove duplicate consecutive words\n    text = re.sub(r'\\b(\\w+)( \\1\\b)+', r'\\1', text)\n    stop_words = set(stopwords.words('english'))\n    # Remove stopwords\n    words_filtered = ' '.join([word for word in text.lower().split() if word not in stop_words])\n\n    # If words_filtered is empty after removing stopwords, return an empty DataFrame\n    if not words_filtered.strip():\n        empty_df = pd.DataFrame()\n        fig, ax = plt.subplots()\n        return empty_df, ax\n\n    # Generating co-occurrence matrix and plotting as before\n    vectorizer = CountVectorizer(ngram_range=(n, n))\n    X = vectorizer.fit_transform([words_filtered])  # Ensure input is treated as a single document\n    matrix = (X.T * X).todense()\n    np.fill_diagonal(matrix, 0)\n    feature_names = vectorizer.get_feature_names_out() if hasattr(vectorizer,\n                                                                  'get_feature_names_out') else vectorizer.get_feature_names()\n    matrix_df = pd.DataFrame(matrix, index=feature_names, columns=feature_names)\n\n    fig, ax = plt.subplots()\n    cax = ax.matshow(matrix_df, cmap='hot')\n    fig.colorbar(cax)\n    ax.set_xticks(np.arange(len(matrix_df.columns)))\n    ax.set_yticks(np.arange(len(matrix_df.index)))\n    ax.set_xticklabels(matrix_df.columns, rotation=90)\n    ax.set_yticklabels(matrix_df.index)\n\n    return matrix_df, ax [DOC_SEP]     plt.close('all')  # Clear previous plots\n    \n    # Create an empty DataFrame and Axes object for negative or zero rows\n    if rows <= 0:\n        empty_ax = plt.gca()\n        empty_ax.set_title('Non-Zero Value Counts')\n        return pd.DataFrame(columns=COLUMNS), empty_ax\n    \n    # Generate random data and create DataFrame\n    data = np.random.randint(10, size=(rows, len(COLUMNS)))\n    df = pd.DataFrame(data, columns=COLUMNS)\n    \n    # Count non-zero values in each column\n    counts = df.astype(bool).sum(axis=0)\n    \n    # Create bar plot for non-zero counts\n    ax = counts.plot(kind='bar')\n    ax.set_title('Non-Zero Value Counts')\n    \n    return df, ax [DOC_SEP]     # Generate sample students and grades\n\n    # Constants\n    STUDENTS = ['Student' + str(i) for i in range(1, 101)]\n    COURSES = ['Course' + str(i) for i in range(1, 6)]\n\n    students_sample = sample(STUDENTS, num_students)\n    grades = np.random.randint(40, 101, size=(num_students, len(COURSES)))\n\n    # Create DataFrame\n    df = pd.DataFrame(grades, index=students_sample, columns=COURSES)\n\n    # Create plot\n    fig, ax = plt.subplots()\n    df.mean().plot(kind='bar', ax=ax, position=1, width=0.4, color='b', label='Average Grade')\n    df[df >= 60].count().plot(kind='bar', ax=ax, position=0, width=0.4, color='g', label='Passing Grade Counts')\n    ax.set_title('Course-wise Average and Passing Grade Counts')\n    ax.legend()\n\n    return df, ax [DOC_SEP]     scores = np.random.randint(0, 101, size=(num_teams, num_games))\n    teams = ['Team' + str(i) for i in range(1, num_teams + 1)]\n    games = ['Game' + str(i) for i in range(1, num_games + 1)]\n    df = pd.DataFrame(scores, index=teams, columns=games)\n    return df [DOC_SEP]     FEATURES = ['Feature' + str(i) for i in range(1, num_features + 1)]\n    SAMPLES = ['Sample' + str(i) for i in range(1, num_samples + 1)]\n    \n    data = np.random.rand(len(SAMPLES), len(FEATURES))\n    df = pd.DataFrame(data, index=SAMPLES, columns=FEATURES)\n    \n    corr_matrix = df.corr()\n    ax = sns.heatmap(corr_matrix, annot=True)\n    \n    return df, ax [DOC_SEP]     sales = np.random.randint(100, 1001, size=(len(MONTHS), len(PRODUCTS)))\n    df = pd.DataFrame(sales, index=MONTHS, columns=PRODUCTS)\n\n    # Visualizations\n    total_sales = df.sum()\n    plt.figure(figsize=(10, 5))\n    total_sales.plot(kind='line', title='Total Sales per Product')\n    plt.ylabel('Total Sales')\n    plt.show()\n\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(df, annot=True, fmt=\"d\", cmap='viridis')\n    plt.title('Monthly Sales per Product')\n    plt.show()\n\n    return df [DOC_SEP]     matched_paths = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if re.match(pattern, file):\n                matched_paths.append(os.path.join(root, file))\n\n    df = pd.DataFrame(matched_paths, columns=['File Path'])\n    df.to_csv(output_csv, index=False)\n\n    return df [DOC_SEP]     hashes = {}\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if re.search(pattern, file):\n                path = os.path.join(root, file)\n                with open(path, 'rb') as f:\n                    data = f.read()\n                    hash_digest = hashlib.sha256(data).digest()\n                    hashes[path] = binascii.hexlify(hash_digest).decode()\n    return hashes [DOC_SEP]     for col in dataframe.columns:\n        dataframe[col] = dataframe[col].apply(lambda x: float(re.search(data_pattern, x).group(0)[1:-1])\n                                              if pd.notnull(x) and re.search(data_pattern, x) else np.nan)\n    return dataframe [DOC_SEP]     # Ensure the file exists\n    directory = os.path.dirname(filename)\n    os.makedirs(directory, exist_ok=True)\n    if not os.path.exists(filename):\n        open(filename, 'a').close()\n\n    # Encrypt the data using simple XOR operation with password hash as key\n    key = hashlib.sha256(password.encode()).digest()\n    encrypted_bytes = [byte ^ key[i % len(key)] for i, byte in enumerate(data.encode())]\n    encrypted = base64.b64encode(bytes(encrypted_bytes)).decode()\n\n    # Write to the file\n    with open(filename, 'w') as f:\n        f.write(encrypted)\n\n    return encrypted [DOC_SEP]     if not os.path.exists(filename):\n        raise FileNotFoundError(f\"No such file: '{filename}'\")\n\n    if os.stat(filename).st_size == 0:\n        # File is empty, return an empty DataFrame with no columns.\n        return pd.DataFrame()\n\n    df = pd.read_csv(filename)\n\n    # Erase the original file's content using a context manager to handle the file properly\n    with open(filename, 'w') as file:\n        file.truncate()\n\n    return df [DOC_SEP] \n    if not os.path.isfile(csv_path):\n        raise FileNotFoundError(f\"{csv_path} does not exist\")\n\n    df = pd.read_csv(csv_path)\n    df[date_column] = df[date_column].apply(lambda x: parse(x))\n\n    return df[date_column].dt.year.hist() [DOC_SEP]     # Get timezone objects for the source and destination timezones\n    from_tz_obj = pytz.timezone(from_tz)\n    to_tz_obj = pytz.timezone(to_tz)\n\n    # Parse the given date string and localize it to the source timezone\n    given_date_naive = parse(date_str)\n    given_date = from_tz_obj.localize(given_date_naive)\n\n    # Convert the given date to the destination timezone\n    given_date_in_to_tz = given_date.astimezone(to_tz_obj)\n\n    # Get the current time in the destination timezone\n    current_date_in_to_tz = datetime.now(pytz.utc).astimezone(to_tz_obj)\n\n    # Calculate the time difference in seconds\n    time_difference = current_date_in_to_tz - given_date_in_to_tz\n\n    return int(time_difference.total_seconds()) [DOC_SEP]     given_date = parse(date_str)\n    next_day = given_date\n\n    while True:\n        next_day = next_day + timedelta(days=1)\n\n        # Monday to Friday are business days\n        if 0 <= next_day.weekday() < 5:\n            break\n\n    return next_day [DOC_SEP]     DAYS_OF_WEEK = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n    weekdays = [parse(date_str).weekday() for date_str in dates_str_list]\n    weekday_counts = np.bincount(weekdays, minlength=7)\n    \n    distribution = pd.Series(weekday_counts, index=DAYS_OF_WEEK)\n\n    return distribution"
}