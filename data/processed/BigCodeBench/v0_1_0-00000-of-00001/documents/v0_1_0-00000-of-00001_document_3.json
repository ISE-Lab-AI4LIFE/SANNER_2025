{
  "id": "v0_1_0-00000-of-00001_document_3",
  "text": "    try:\n        plt.rc('font', family='Arial')\n\n        random.seed(seed)\n        dates = pd.date_range(end=datetime.now(), periods=30)\n        values = [random.randint(0, 100) for _ in range(30)]\n        \n        fig, ax = plt.subplots()\n        ax.plot(dates, values, label='Value over Time')\n        ax.set_xlabel('Date')\n        ax.set_ylabel('Value')\n        ax.set_title('Random Time Series Data')\n        ax.legend()\n\n        return ax\n    except Exception as e:\n        raise ValueError(f\"Error generating the plot: {e}\") [DOC_SEP]     try:\n        # Set font to Arial\n        font = {'sans-serif': 'Arial', 'family': 'sans-serif'}\n        plt.rc('font', **font)\n\n        # boston = load_boston()\n        # boston_df = pd.DataFrame(data=boston.data, columns=boston.feature_names)\n        # corr = boston_df.corr()\n\n        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n        target = raw_df.values[1::2, 2]\n\n        # Step 1: Convert data and target into DataFrame\n        columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n        boston_df = pd.DataFrame(data=data, columns=columns)\n\n        # Step 2: Compute correlation matrix\n        corr = boston_df.corr()\n\n\n        sns.set_theme(style=\"white\")  # Optional: for better aesthetics\n        plt.figure(figsize=(10, 8))  # Optional: adjust the size of the heatmap\n        ax = sns.heatmap(corr, annot=True)  # 'annot=True' to display correlation values\n        # if file_path:\n        #     plt.savefig(file_path)\n\n        return ax\n\n    except Exception as e:\n        raise ValueError(f\"An error occurred: {e}\") [DOC_SEP]     font = {'family': 'Arial'}\n    plt.rc('font', **font)  # Set the global font to Arial.\n    DIABETES = load_diabetes()\n    diabetes_df = pd.DataFrame(data=DIABETES.data, columns=DIABETES.feature_names)\n    pair_plot = sns.pairplot(diabetes_df)\n    return pair_plot.fig, diabetes_df [DOC_SEP]     try:\n        if temperatures.empty or not isinstance(temperatures, pd.DataFrame):\n            raise ValueError(\"Input temperatures must be a non-empty pandas DataFrame.\")\n\n        # Setting the font to Arial\n        font = {'sans-serif': 'Arial', 'family': 'sans-serif'}\n        plt.rc('font', **font)\n        \n        fig, ax = plt.subplots(figsize=(10, 6))\n        ax.plot(temperatures.index, temperatures['temperature'])\n        ax.set_xlabel('Date')\n        ax.set_ylabel('Temperature (Â°C)')\n        ax.set_title('Daily Temperatures in New York')\n\n        return ax\n\n    except Exception as e:\n        raise ValueError(f\"An error occurred: {e}\") [DOC_SEP] \n    if not isinstance(df, pd.DataFrame) or not all(col in df.columns for col in ['group', 'date', 'value']):\n        raise ValueError(\"Invalid 'df': must be a DataFrame with 'group', 'date', and 'value' columns.\")\n\n    color_cycle = cycle('bgrcmk')\n    fig, ax = plt.subplots(figsize=(10, 6))\n\n    for group in groups:\n        group_df = df[df['group'] == group].copy()\n        group_df['date'] = group_df['date'].apply(lambda x: x.toordinal())\n        ax.scatter(group_df['date'], group_df['value'], color=next(color_cycle))\n\n    ax.set_xlabel('Date (ordinal)')\n    ax.set_ylabel('Value')\n    ax.set_title('Scatterplot of Values for Each Group Over Time')\n\n    return ax [DOC_SEP]     if df.empty or not all(col in df.columns for col in ['group', 'date', 'value']):\n        raise ValueError(\"DataFrame must be non-empty and contain 'group', 'date', and 'value' columns.\")\n    \n    if not pd.api.types.is_datetime64_any_dtype(df['date']):\n        raise ValueError(\"'date' column must be in datetime format.\")\n\n    try:\n        df['date'] = df['date'].apply(lambda x: x.toordinal())\n        df_numeric = df.drop(columns=['group'])\n        correlation_matrix = df_numeric.corr()\n\n        heatmap_fig = plt.figure(figsize=(8, 6))\n        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n        plt.title('Correlation Matrix')\n\n        pairplot_grid = sns.pairplot(df)\n\n        return heatmap_fig, pairplot_grid\n\n    except Exception as e:\n        raise ValueError(f\"An error occurred: {e}\") [DOC_SEP] \n    if not isinstance(df, pd.DataFrame) or not all(col in df.columns for col in ['group', 'date', 'value']):\n        raise ValueError(\"Invalid 'df': must be a DataFrame with 'group', 'date', and 'value' columns.\")\n\n    df['date'] = df['date'].apply(lambda x: x.toordinal())\n    X = df[['date']]\n    y = df['value']\n\n    model = LinearRegression()\n    model.fit(X, y)\n    y_pred = model.predict(X)\n\n    fig, ax = plt.subplots()\n    ax.scatter(X, y, color='red')\n    ax.plot(X, y_pred, color='blue')\n    ax.set_title('Value vs Date (Linear Regression Prediction)')\n    ax.set_xlabel('Date (ordinal)')\n    ax.set_ylabel('Value')\n\n    return model, y_pred, ax [DOC_SEP]     if df.empty or not all(col in df.columns for col in ['group', 'date', 'value']):\n        raise ValueError(\"DataFrame must be non-empty and contain 'group', 'date', and 'value' columns.\")\n\n    if not pd.api.types.is_datetime64_any_dtype(df['date']):\n        raise ValueError(\"'date' column must be in datetime format.\")\n\n    df['date'] = df['date'].apply(lambda x: x.toordinal())\n    X = df[['date', 'value']]\n\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)\n    kmeans.fit(X)\n    y_kmeans = kmeans.predict(X)\n\n    fig, ax = plt.subplots()\n    ax.scatter(X['date'], X['value'], c=y_kmeans, cmap='viridis')\n    ax.set_title('KMeans Clustering of Value vs Date')\n    ax.set_xlabel('Date (ordinal)')\n    ax.set_ylabel('Value')\n\n    return ax [DOC_SEP]     # Validation\n    required_columns = ['group', 'date', 'value']\n    if not isinstance(df, pd.DataFrame) or not all(col in df.columns for col in required_columns):\n        raise ValueError(\"Invalid 'df': must be a DataFrame with 'group', 'date', and 'value' columns.\")\n    if decomposition_model not in ['additive', 'multiplicative']:\n        raise ValueError(\"Invalid 'decomposition_model': must be 'additive' or 'multiplicative'.\")\n    if not isinstance(freq, str):\n        raise ValueError(\"Invalid 'freq': must be a string representing frequency.\")\n\n    # Setting up DataFrame\n    df = df.set_index('date')\n    df = df.asfreq(freq, method='pad')\n    df['value'] = pd.to_numeric(df['value'], errors='coerce')\n\n    # Handling missing or non-numeric values in 'value' column\n    if df['value'].isnull().any():\n        raise ValueError(\"Non-numeric or missing values found in 'value' column.\")\n\n    # Decomposition\n    result = seasonal_decompose(df['value'], model=decomposition_model)\n\n    ax = df.plot(y='value')\n    plt.ylabel('Value')\n    plt.title('Time Series Decomposition')\n\n    return (result, ax) [DOC_SEP]     if not isinstance(df, pd.DataFrame) or not all(col in df.columns for col in ['Item', 'Location']):\n        raise ValueError(\"Invalid 'df': must be a DataFrame with 'Item' and 'Location' columns.\")\n\n    items = items or ['apple', 'banana', 'grape', 'orange', 'pineapple']\n    locations = locations or ['store1', 'store2', 'store3', 'store4', 'store5']\n\n    item_count_df = df.groupby(['Location', 'Item']).size().unstack().fillna(0)\n    ax = item_count_df.plot(kind='bar', stacked=True)\n    ax.set_title('Item Distribution by Location')\n    ax.set_ylabel('Count')\n    plt.show()\n    return ax [DOC_SEP]     if not isinstance(df, pd.DataFrame) or not all(col in df.columns for col in ['Date', 'Sales']):\n        raise ValueError(\"Invalid 'df': must be a DataFrame with 'Date' and 'Sales' columns.\")\n\n    df['Date'] = pd.to_datetime(df['Date'])\n    df = df.set_index('Date')\n    resampled_df = df.resample('D').sum()\n\n    if resampled_df.empty or resampled_df['Sales'].sum() == 0:\n        raise ValueError(\"No data available to plot after resampling.\")\n\n    ax = resampled_df.plot(y='Sales')\n    ax.set_title('Daily Turnover')\n    ax.set_ylabel('Sales')\n    plt.show()\n    return ax [DOC_SEP]     if not isinstance(df, pd.DataFrame) or not all(col in df.columns for col in ['Date', 'Time', 'Temperature']):\n        raise ValueError(\"Invalid 'df': must be a DataFrame with 'Date', 'Time', and 'Temperature' columns.\")\n\n    df['Date'] = pd.to_datetime(df['Date'])\n    df['Month'] = df['Date'].dt.month\n    df['Day'] = df['Date'].dt.day\n\n    df_pivot = df.pivot(index=\"Month\", columns=\"Day\", values=\"Temperature\")\n    ax = sns.heatmap(df_pivot)\n    ax.set_title('Temperature Heatmap')\n    return ax [DOC_SEP]     if not isinstance(df, pd.DataFrame) or 'Status' not in df.columns:\n        raise ValueError(\"Input must be a pandas DataFrame with a 'Status' column.\")\n\n    status_counts = df['Status'].value_counts()\n    fig, ax = plt.subplots()\n    ax.pie(status_counts, labels=status_counts.index, autopct='%1.1f%%')\n    ax.set_title('Status Distribution')\n\n    return ax [DOC_SEP]     if len(set(keys)) != 10:\n        raise ValueError(\"keys parameter must contain exactly 10 unique elements\")\n\n    for key in keys:\n        my_dict[key] = random.randint(1, 100)\n\n    json_filename = \"updated_dictionary.json\"\n    txt_filename = \"key_frequencies.txt\"\n\n    with open(json_filename, 'w') as json_file:\n        json.dump(my_dict, json_file, indent=4)\n\n    key_counts = Counter(my_dict.keys())\n    with open(txt_filename, 'w') as txt_file:\n        for key, count in key_counts.items():\n            txt_file.write(f\"{key}: {count}\\n\")\n\n    return my_dict, json_filename, txt_filename [DOC_SEP]     if not isinstance(my_dict[\"array\"], np.ndarray):\n        raise TypeError\n\n    SCALER = MinMaxScaler()\n    array = my_dict['array'].reshape(-1, 1)\n    normalized_array = SCALER.fit_transform(array).reshape(-1)\n\n    my_dict['normalized_array'] = normalized_array\n\n    return my_dict [DOC_SEP]     if len(numbers) == 0:\n        raise ValueError\n    my_dict = {'array': np.array(numbers)}\n    mode_value = mode(my_dict['array']).mode[0]\n    ent = entropy(my_dict['array'], base=2)\n    my_dict['mode'] = mode_value\n    my_dict['entropy'] = ent\n    return my_dict [DOC_SEP]     samples = np.random.normal(mu, sigma, sample_size)\n    \n    # Plotting the histogram of the samples\n    plt.hist(samples, bins=30, alpha=0.75, color='blue')\n    plt.title('Histogram of Generated Samples')\n    plt.xlabel('Sample values')\n    plt.ylabel('Frequency')\n    plt.grid(True)\n    plt.show()\n    \n    return samples [DOC_SEP]     if num_of_students <= 0:\n        raise ValueError(\"num_of_students must be positive.\")\n\n    set_seed(seed)\n    np.random.seed(seed)\n\n    name_list = name_list or ['John', 'Mike', 'Sara', 'Emma', 'Nick']\n    gender_list = gender_list or ['Male', 'Female']\n\n    data = []\n    for _ in range(num_of_students):\n        name = choice(name_list)\n        age = np.random.randint(age_range[0], age_range[1] + 1)\n        gender = choice(gender_list)\n        score = np.random.randint(score_range[0], score_range[1] + 1)\n        data.append([name, age, gender, score])\n\n    columns = ['Name', 'Age', 'Gender', 'Score']\n    df = pd.DataFrame(data, columns=columns)\n    return df [DOC_SEP]     copied_files = []\n\n    if not os.path.exists(backup_directory):\n        os.makedirs(backup_directory)\n\n    for filename in os.listdir(directory):\n        if filename.endswith('.json'):\n            src = os.path.join(directory, filename)\n            dst = os.path.join(backup_directory, filename)\n            shutil.copy(src, dst)\n            copied_files.append(dst)\n\n    return copied_files [DOC_SEP]     X = np.linspace(-10, 10, 400)\n    Y = X**2\n\n    plt.figure()\n    plt.plot(X, Y)\n    plt.title('y = x^2')\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.grid(True)\n    plt.show() [DOC_SEP]     \n    if not all(isinstance(date, datetime) for date in [start_date, end_date]):\n        raise ValueError(\"start_date and end_date must be datetime.datetime objects.\")\n    if start_date > end_date:\n        raise ValueError(\"start_date must not be later than end_date.\")\n\n    random_seed(seed)\n\n    num_days = (end_date - start_date).days\n    dates = pd.Series([start_date + timedelta(days=randint(0, num_days)) for _ in range(num_days)])\n    return dates [DOC_SEP]     if not isinstance(my_list, list):\n        raise TypeError(\"Input must be a list.\")\n\n    if seed is not None:\n        np.random.seed(seed)\n\n    my_list.append(12)\n    categories = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\n    sales_data = []\n    for category in categories:\n        sales = my_list[np.random.randint(0, len(my_list))] * np.random.randint(100, 1000)\n        sales_data.append([category, sales])\n\n    sales_df = pd.DataFrame(sales_data, columns=['Category', 'Sales'])\n\n    ax = sales_df.plot(kind='bar', x='Category', y='Sales', legend=False)\n    ax.set_title('Category-wise Sales Data')\n    ax.set_ylabel('Sales')\n\n    return sales_df, ax [DOC_SEP]     random_number = random.randint(0, 100)\n    my_list.append(random_number)\n\n    size = sum(my_list)\n    random_array = np.random.rand(size)\n\n    return random_array [DOC_SEP]     if not isinstance(my_list, list):\n        raise TypeError(\"my_list must be a list.\")\n\n    my_list.append(12)\n    num_files = sum(my_list)\n\n    files = glob.glob(os.path.join(file_dir, '*' + file_ext))[:num_files]\n    if not files:\n        raise FileNotFoundError(f\"No files with extension '{file_ext}' found in directory '{file_dir}'.\")\n\n    data_frames = [pd.read_csv(file) for file in files]\n    concatenated_df = pd.concat(data_frames, ignore_index=True)\n\n    return concatenated_df [DOC_SEP]     if not isinstance(my_list, list):\n        raise TypeError(\"Input must be a list.\")\n    if not all(isinstance(item, (int, float)) for item in my_list):\n        raise ValueError(\"All elements in 'my_list' must be numbers.\")\n    random_seed(seed)\n    my_list.append(12)\n\n    total_size = min(sum(my_list), size)\n\n    start_time = time.time()\n    random_list = [randint(1, 100) for _ in range(total_size)]\n    end_time = time.time()\n\n    fig, ax = plt.subplots()\n    ax.hist(random_list, bins=20)\n    ax.set_title('Histogram of Random Numbers')\n    ax.set_xlabel('Number')\n    ax.set_ylabel('Frequency')\n\n    return end_time - start_time, ax [DOC_SEP]     combinations = list(itertools.combinations(LETTERS, n))\n    letter_counts = defaultdict(int)\n\n    for combination in combinations:\n        for letter in combination:\n            letter_counts[letter] += 1\n\n    filename = f'letter_combinations_{random.randint(1, 100)}.json'\n    with open(filename, 'w') as f:\n        json.dump(letter_counts, f)\n\n    return filename [DOC_SEP]     random_seed(seed)\n    animals = animals or ['Lion', 'Elephant', 'Tiger', 'Giraffe', 'Panda']\n    report_data = []\n\n    for animal in animals:\n        counts = [randint(1, 100) for _ in range(10)]\n        mean = statistics.mean(counts)\n        median = statistics.median(counts)\n        mode = statistics.mode(counts)\n        std_dev = np.std(counts)\n        report_data.append([animal, mean, median, mode, std_dev])\n    \n    report_df = pd.DataFrame(report_data, columns=['Animal', 'Mean', 'Median', 'Mode', 'Standard Deviation'])\n\n    return report_df [DOC_SEP]     files_moved = 0\n\n    os.makedirs(DEST_DIR, exist_ok=True)\n    for filename in glob.glob(os.path.join(ROOT_DIR, '*')):\n        if not os.path.exists(filename) or os.path.isdir(filename):\n            continue\n        with open(filename, 'rb') as f:\n            file_hash = hashlib.md5(f.read()).hexdigest()\n        if file_hash == SPECIFIC_HASH:\n            shutil.move(filename, DEST_DIR)\n            files_moved += 1\n    return files_moved [DOC_SEP]     x = np.zeros(POINTS)\n    y = np.zeros(POINTS)\n\n    for i in range(1, POINTS):\n        val = randint(0, 1)\n        if val == 1:\n            x[i] = x[i - 1] + math.cos(2 * math.pi * val)\n            y[i] = y[i - 1] + math.sin(2 * math.pi * val)\n        else:\n            x[i] = x[i - 1] - math.cos(2 * math.pi * val)\n            y[i] = y[i - 1] - math.sin(2 * math.pi * val)\n\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    plt.show()\n    return fig [DOC_SEP]     try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.ConnectionError as e:\n        raise ConnectionError(f\"Could not connect to URL: {e}\")\n    except requests.HTTPError as e:\n        raise requests.HTTPError(f\"HTTP error occurred: {e}\")\n\n    try:\n        soup = BeautifulSoup(response.text, 'html.parser')\n        table = soup.find('table')  # Assuming only the first table is of interest\n        if table is None:\n            raise ValueError(\"No table found on the page.\")\n\n        # Extracting headers if present\n        headers = [th.text.strip() for th in table.find_all('th')]\n        \n        # Extracting data rows\n        data = []\n        for row in table.find_all('tr'):\n            cols = row.find_all('td')\n            if not cols:  # This skips rows without <td> (like header rows)\n                continue\n            cols = [ele.text.strip() for ele in cols]\n            data.append(cols)\n\n        if not data:\n            raise ValueError(\"No data found in the table.\")\n\n        df = pd.DataFrame(data, columns=headers if headers else None)\n    except Exception as e:\n        raise ValueError(f\"Error parsing the page content: {e}\")\n    return df [DOC_SEP]     salt = os.urandom(salt_size)\n    data = binascii.unhexlify(hex_str.replace('\\\\x', ''))\n    salted_data = salt + data\n    hash_value = hashlib.sha256(salted_data).hexdigest()\n\n    return (base64.b64encode(salt).decode('utf-8'), hash_value) [DOC_SEP]     salt = os.urandom(salt_size)\n    data = binascii.unhexlify(hex_str.replace('\\\\x', ''))\n    salted_data = salt + data\n    hash_value = hashlib.sha256(salted_data).hexdigest()\n\n    return (base64.b64encode(salt).decode('utf-8'), hash_value) [DOC_SEP]     hex_str_cleaned = hex_str.replace('\\\\x', '')\n    try:\n        bytes_data = binascii.unhexlify(hex_str_cleaned)\n    except binascii.Error:\n        raise ValueError(\"Invalid hex string\")\n\n    byte_values, byte_counts = np.unique(np.frombuffer(bytes_data, dtype=np.uint8), return_counts=True)\n    df = pd.DataFrame({'Byte Value': byte_values, 'Frequency': byte_counts})\n\n    fig, ax = plt.subplots()\n    ax.bar(df['Byte Value'], df['Frequency'])\n    ax.set_xlabel('Byte Value')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of Bytes in Hex String')\n\n    return df, ax [DOC_SEP]     if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input must be a non-empty DataFrame.\")\n    \n    last_col_name = df.columns[-1]\n    scaler = MinMaxScaler()\n    normalized_values = scaler.fit_transform(df[[last_col_name]])\n    normalized_df = df.copy()\n    normalized_df[last_col_name] = normalized_values.flatten()\n    \n    fig, ax = plt.subplots()\n    ax.plot(normalized_df.index, normalized_df[last_col_name])\n    ax.set_title(f'Normalized Data of {last_col_name}')\n    ax.set_xlabel(\"Index\")\n    ax.set_ylabel(\"Normalized Value\")\n\n    return normalized_df, ax [DOC_SEP] \n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"The input must be a non-empty pandas DataFrame.\")\n\n    last_col_name = df.columns[-1]\n    fig, ax = plt.subplots()\n    ax.hist(df[last_col_name], bins=bins)\n    ax.set_title(f'Histogram of {last_col_name}')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    plt.show()\n    return ax [DOC_SEP]     if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input must be a non-empty pandas DataFrame.\")\n\n    last_col = df.columns[-1]\n    imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n    df[last_col] = imp_mean.fit_transform(df[last_col].values.reshape(-1, 1))\n\n    fig, ax = plt.subplots()\n    sns.boxplot(x=df[last_col], ax=ax)\n    ax.set_title('Boxplot of Last Column')\n    ax.set_xlabel(last_col)\n    return df, ax [DOC_SEP]     if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input must be a DataFrame\")\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n\n    pca = PCA(n_components=2)\n    principal_components = pca.fit_transform(df)\n\n    pca_df = pd.DataFrame(data=principal_components, columns=['Principal Component 1', 'Principal Component 2'])\n\n    fig, ax = plt.subplots()\n    ax.scatter(pca_df['Principal Component 1'], pca_df['Principal Component 2'])\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n    ax.set_title('2 Component PCA')\n\n    return pca_df, ax [DOC_SEP]     if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input must be a non-empty pandas DataFrame.\")\n\n    last_col = df.columns[-1]\n    skewness = skew(df[last_col].dropna())  # dropna() to handle NaN values\n\n    return skewness [DOC_SEP]     if not isinstance(df, pd.DataFrame) or 'Letters' not in df.columns:\n        raise ValueError(\"The input must be a pandas DataFrame with a 'Letters' column.\")\n\n    letter_frequency = df['Letters'].value_counts().reindex(letters, fill_value=0)\n    ax = letter_frequency.plot(kind='bar')\n    ax.set_title('Letter Frequency')\n    ax.set_xlabel('Letters')\n    ax.set_ylabel('Frequency')\n    plt.show()\n    return ax [DOC_SEP]     if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"The input must be a non-empty pandas DataFrame.\")\n\n    numeric_cols = df.select_dtypes(include=np.number).columns\n    if not numeric_cols.size:\n        raise ValueError(\"DataFrame contains no numeric columns.\")\n\n    axes = []\n    for col in numeric_cols:\n        fig, ax = plt.subplots()\n        df[col].plot(kind='hist', title=col, ax=ax)\n        ax.set_xlabel('Value')\n        ax.set_ylabel('Frequency')\n        axes.append(ax)\n\n    return axes [DOC_SEP]     if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"The input df must be a pandas DataFrame.\")\n    if not isinstance(cols, list) or not all(isinstance(col, str) for col in cols):\n        raise ValueError(\"cols must be a list of column names.\")\n    if not all(col in df.columns for col in cols):\n        raise ValueError(\"All columns in cols must exist in the dataframe.\")\n\n    scaler = StandardScaler()\n    df[cols] = scaler.fit_transform(df[cols])\n\n    return df [DOC_SEP]     if not isinstance(rows, int) or rows <= 0:\n        raise ValueError(\"rows must be a positive integer greater than 0.\")\n\n    np.random.seed(seed)\n    data = np.random.randint(1, 101, size=(rows, len(columns)))\n    df = pd.DataFrame(data, columns=columns)\n    \n    stats_dict = {}\n    for col in columns:\n        stats_dict[col] = {\n            'mean': statistics.mean(df[col]),\n            'median': statistics.median(df[col])\n        }\n    \n    return df, stats_dict [DOC_SEP]     x_values = np.linspace(0, 2 * np.pi, 400)\n    fig, axs = plt.subplots(2)\n    \n    axs[0].plot(x_values, np.sin(x_values))\n    axs[0].set_title('Sine function')\n    axs[0].set_xlabel('x')\n    axs[0].set_ylabel('sin(x)')\n    \n    axs[1].plot(x_values, np.cos(x_values))\n    axs[1].set_title('Cosine function')\n    axs[1].set_xlabel('x')\n    axs[1].set_ylabel('cos(x)')\n    \n    plt.tight_layout()\n    \n    return fig, axs [DOC_SEP]     X = np.linspace(-10, 10, 400)  # X range specified\n    y = 2 * X + 1\n\n    fig, ax = plt.subplots()\n    ax.plot(X, y, '-r', label='y=2x+1')\n    \n    solution_y = 2 * 2 + 1  # y value at x = 2\n    ax.plot(2, solution_y, 'go', label='Solution at x=2')\n    \n    ax.set_title('Solution of the equation y=2x+1 at x=2')\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_xlim([-10, 10])  # Explicitly setting the x-axis range\n    # ax.set_ylim is optional and can be set if a specific y-range is desired\n    ax.legend(loc='best')\n    ax.grid()\n\n    return ax [DOC_SEP]     results = []\n    try:\n        network = ipaddress.IPv4Network(ip_range, strict=False)  # Note the `strict=False`\n    except ValueError as e:\n        raise ValueError(f\"Invalid IP range: {e}\")\n\n    for ip in network:\n        try:\n            response = requests.get(f\"http://{ip}\", timeout=timeout)\n            if response.status_code == 200:\n                results.append(str(ip))\n        except requests.exceptions.ConnectionError as e:\n            pass\n    return results [DOC_SEP]     with open(csv_path, 'w', newline='') as csvfile:\n        fieldnames = ['IP Address']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n        writer.writeheader()\n\n        for ip in IPv4Network(ip_range):\n            writer.writerow({'IP Address': str(ip)})\n\n    return csv_path [DOC_SEP]     active_ips = {}\n\n    for ip in IPv4Network(ip_range):\n        try:\n            subprocess.check_output(f'ping -c 1 {ip}', shell=True)\n            active_ips[str(ip)] = True\n        except subprocess.CalledProcessError:\n            active_ips[str(ip)] = False\n\n    return active_ips [DOC_SEP]     open_ports = {}\n\n    def check_port(ip):\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        sock.settimeout(1)\n        try:\n            sock.connect((str(ip), port))\n            open_ports[str(ip)] = True\n        except socket.error:\n            open_ports[str(ip)] = False\n        finally:\n            sock.close()\n\n    threads = []\n\n    for ip in IPv4Network(ip_range):\n        thread = Thread(target=check_port, args=(ip,))\n        thread.start()\n        threads.append(thread)\n\n    for thread in threads:\n        thread.join()\n\n    return open_ports [DOC_SEP]     le = LabelEncoder()\n    df[column_name] = le.fit_transform(df[column_name])\n    return df [DOC_SEP]     elements_series = pd.Series(elements)\n    count_series = elements_series.apply(lambda x: len(x))\n    data_dict = {'Element': elements_series, 'Count': count_series}\n    if include_index:\n        data_dict['Index'] = np.arange(len(elements))\n    count_df = pd.DataFrame(data_dict)\n    if include_index:\n        count_df = count_df[['Index', 'Element', 'Count']]  # Reordering columns to put 'Index' first\n    return count_df"
}