{
  "id": "v0_1_0-00000-of-00001_document_23",
  "text": "\n    # Handle empty input\n    if all(text.strip() == \"\" for text in texts):\n        return [], []\n\n    # Remove URLs\n    cleaned_texts = [re.sub('http[s]?://\\S+', '', text) for text in texts]\n\n    vectorizer = TfidfVectorizer()\n    tfidf_matrix = vectorizer.fit_transform(cleaned_texts)\n\n    # Convert the sparse matrix to a dense format, round the values, convert to tuples and return along with feature names\n    dense_matrix = [tuple(round(val, 8) for val in row) for row in tfidf_matrix.toarray().tolist()]\n    return dense_matrix, list(vectorizer.get_feature_names_out()) [DOC_SEP]     execution_times = {}\n    py_scripts = glob.glob(os.path.join(test_dir, '*.py'))\n\n    for py_script in py_scripts:\n        start_time = time.time()\n        subprocess.call(['python', py_script])\n        end_time = time.time()\n        execution_times[os.path.basename(py_script)] = end_time - start_time\n\n    return execution_times [DOC_SEP]     start_time = datetime.now()\n    process = subprocess.Popen(shlex.split(f\"/usr/bin/Rscript --vanilla {script_path}\"),\n                               stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    stdout, stderr = process.communicate()\n    end_time = datetime.now()\n\n    log_details = {\n        'Start Time': str(start_time),\n        'End Time': str(end_time),\n        'Stdout': stdout.decode('utf-8'),\n        'Stderr': stderr.decode('utf-8')\n    }\n    \n    return log_details [DOC_SEP]     try:\n        shutil.copy(script_path, temp_dir)\n        temp_script_path = os.path.join(temp_dir, os.path.basename(script_path))\n        result = subprocess.call([\"python\", temp_script_path])\n        print(result)\n        if result == 0:\n            return \"Script executed successfully!\"\n        else:\n            return \"Script execution failed!\"\n    except Exception as e:\n        return \"Script execution failed!\" [DOC_SEP]     def target():\n        subprocess.call(['python', script_path])\n\n    thread = threading.Thread(target=target)\n    thread.start()\n\n    thread.join(timeout)\n\n    if thread.is_alive():\n        os.system(f'pkill -f \"{script_path}\"')\n        thread.join()\n        return 'Terminating process due to timeout.'\n    else:\n        return 'Script executed successfully.' [DOC_SEP]     # Construct the command to run the R script\n    command = f'/usr/bin/Rscript --vanilla {r_script_path}'\n    \n    # Execute the R script\n    subprocess.call(command, shell=True)\n    \n    # Initialize the start time\n    start_time = time.time()\n    \n    # Construct the search pattern for the output CSV file\n    search_pattern = os.path.join(output_path, '*.csv')\n    \n    # Continuously check if the output file is generated within the specified duration\n    while time.time() - start_time < duration:\n        if glob.glob(search_pattern):\n            return True, 'File generated successfully within the specified duration.'\n        time.sleep(0.1)\n    \n    # Return False with a message if the file is not generated within the specified duration\n    return False, 'File not generated within the specified duration.' [DOC_SEP]     if not Path(file_path).exists():\n        raise FileNotFoundError(f\"No such file or directory: '{file_path}'\")\n\n    creation_time = os.path.getctime(file_path)\n    formatted_time = datetime.fromtimestamp(creation_time).strftime(DATE_FORMAT)\n    \n    return formatted_time [DOC_SEP]     # Convert the Unix timestamp to a UTC datetime object\n    datetime_utc = datetime.utcfromtimestamp(unix_timestamp).replace(tzinfo=pytz.utc)\n\n    # Convert the UTC datetime to the target timezone\n    datetime_in_target_timezone = datetime_utc.astimezone(pytz.timezone(target_timezone))\n\n    # Format the datetime object in the target timezone to the specified string format\n    formatted_datetime = datetime_in_target_timezone.strftime(DATE_FORMAT)\n\n    return formatted_datetime [DOC_SEP] \n    regex = re.compile(\n        r'^(?:http|ftp)s?://' # http:// or https://\n        r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?)|' #domain...\n        r'localhost|' #localhost...\n        r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})' # ...or ip\n        r'(?::\\d+)?' # optional port\n        r'(?:/?|[/?]\\S+)$', re.IGNORECASE)\n    \n    from_user_values = []\n    for l_res in result:\n        for j in l_res:\n            if re.match(regex, j):\n                from_user_values.append(l_res[j])\n           \n\n    counter = Counter(from_user_values)\n    most_common = dict(counter.most_common(1))\n\n    return most_common [DOC_SEP]     if not os.path.isfile(file_path):\n        raise FileNotFoundError(f\"File not found: {file_path}\")\n    \n    tokens = []\n\n    with open(file_path, 'r') as file:\n        for line in file:\n            tokens.extend(word_tokenize(line))\n\n    return tokens [DOC_SEP]     letters = list(itertools.chain.from_iterable(word_dict.keys()))\n    count_dict = dict(Counter(letters))\n    \n    sorted_dict = dict(sorted(count_dict.items(), key=itemgetter(1), reverse=True))\n    \n    return sorted_dict [DOC_SEP]     animal_dict_copy = {}\n    for i in animal_dict:\n        if i in ANIMAL:\n            animal_dict_copy[i] = animal_dict[i]\n    letters = list(itertools.chain.from_iterable(animal_dict_copy.keys()))\n    count_dict = dict(Counter(letters))\n    \n    sorted_dict = dict(sorted(count_dict.items(), key=itemgetter(1), reverse=True))\n    \n    return sorted_dict [DOC_SEP]     with open(file_name, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Time'] + DATA)\n        \n        for hour in range(24):\n            row = [f'{hour}:00']\n            for data_type in DATA:\n                min_val, max_val = RANGE[data_type]\n                row.append(random.uniform(min_val, max_val))\n            writer.writerow(row)\n\n    return file_name [DOC_SEP]     counter = collections.Counter()\n    \n    try:\n        with open(csv_file, 'r') as f:\n            reader = csv.reader(f)\n            for row in reader:\n                if row[0].startswith(emp_prefix):\n                    counter[row[0]] += 1\n    except FileNotFoundError:\n        return {\"error\": f\"The file {csv_file} was not found.\"}\n    except Exception as e:\n        return {\"error\": str(e)}\n    \n    return dict(counter) [DOC_SEP]     employee_data = defaultdict(list)\n    \n    for prefix, num_employees in dict1.items():\n        if not prefix.startswith('EMP$$'):\n            continue\n\n        salaries = [randint(1, 100) for _ in range(num_employees)]\n        employee_data[prefix].extend(salaries)\n\n    return dict(employee_data) [DOC_SEP]     employee_ids = []\n    \n    for prefix, num_employees in dict1.items():\n        for _ in range(num_employees):\n            random_str = ''.join(random.choice(ascii_uppercase) for _ in range(5))\n            employee_ids.append(f'{prefix}{random_str}')\n\n    return employee_ids [DOC_SEP]     emp_ages = []\n    \n    for prefix, num_employees in dict1.items():\n        if not prefix.startswith('EMP$$'):\n            continue\n\n        for _ in range(num_employees):\n            age = random.randint(*AGE_RANGE)\n            emp_ages.append(age)\n\n    # If no employees in EMP$$ department\n    if not emp_ages:\n        return 0, 0, []\n    \n    mean_age = statistics.mean(emp_ages)\n    median_age = statistics.median(emp_ages)\n    mode_age = statistics.multimode(emp_ages)\n\n    return mean_age, median_age, mode_age [DOC_SEP]     level_data = collections.defaultdict(list)\n    \n    for prefix, num_employees in department_data.items():\n        if prefix not in PREFICES:\n            continue\n\n        for _ in range(num_employees):\n            level = random.choice(LEVELS)\n            level_data[prefix].append(level)\n\n    return json.dumps(level_data) [DOC_SEP]     response = requests.get(csv_url)\n    csv_data = csv.reader(StringIO(response.text))\n\n    headers = next(csv_data)\n    json_data = [dict(zip(headers, row)) for row in csv_data]\n\n    with open(json_file_path, 'w') as json_file:\n        json.dump(json_data, json_file)\n\n    return json_file_path [DOC_SEP]     password_chars = string.ascii_letters + string.digits + string.punctuation\n    password = ''.join(random.choice(password_chars) for i in range(password_length))\n    password = codecs.encode(password, 'latin-1').decode('utf-8')\n    salted_password = (password + salt).encode('utf-8')\n    hashed_password = hashlib.sha256(salted_password).hexdigest()\n    \n    return hashed_password [DOC_SEP]     urls = re.findall(r'(https?://[^\\s,]+)', myString)\n    geo_data = {}\n\n    for url in urls:\n        domain = urllib.parse.urlparse(url).netloc\n        response = requests.get(f\"http://ip-api.com/json/{domain}?access_key={API_KEY}\")\n        geo_data[domain] = json.loads(response.text)\n\n    return geo_data [DOC_SEP]     urls = re.findall(r'(https?://[^\\s,]+)', myString)\n    geo_data = {}\n\n    for url in urls:\n        domain = urllib.parse.urlparse(url).netloc\n        response = requests.get(f\"http://ip-api.com/json/{domain}?access_key={API_KEY}\")\n        geo_data[domain] = json.loads(response.text)\n\n    return geo_data [DOC_SEP]     urls = re.findall(r'https?://[^\\s,]+', myString)\n    ip_addresses = {}\n\n    for url in urls:\n        domain = urllib.parse.urlparse(url).netloc\n        try:\n            ip_addresses[domain] = socket.gethostbyname(domain)\n        except socket.gaierror:\n            ip_addresses[domain] = None  # Handle domains that cannot be resolved\n\n    return ip_addresses [DOC_SEP]     urls = re.findall(r'https://[^\\s,]+', myString)\n    ssl_expiry_dates = {}\n\n    for url in urls:\n        try:\n            domain = urllib.parse.urlparse(url).netloc\n            context = ssl.create_default_context()\n            with socket.create_connection((domain, 443)) as sock:\n                with context.wrap_socket(sock, server_hostname=domain) as ssock:\n                    ssl_expiry_dates[domain] = ssock.getpeercert()['notAfter']\n        except ssl.SSLError:\n            continue  # Ignore SSL errors or log them if necessary\n\n    return ssl_expiry_dates [DOC_SEP]     # Constants\n    HEADERS = {'User-Agent': 'Mozilla/5.0'}\n    \n    # Extract URL from string\n    url_match = re.search(r'(https?://\\S+)', myString)\n    if not url_match:\n        return \"No valid URL found in the provided string.\"\n\n    url = url_match.group()\n    domain = urlparse(url).netloc\n\n    # Fetch webpage content\n    try:\n        response = requests.get(url, headers=HEADERS)\n        response.raise_for_status()\n    except requests.RequestException:\n        return f\"Unable to fetch the content of the URL: {url}\"\n\n    # Extract title from the webpage content\n    soup = BeautifulSoup(response.text, 'html.parser')\n    title = soup.title\n    if title:\n        return title.string\n    else:\n        return \"No title tag found in the webpage.\" [DOC_SEP]     url = re.search(r'(https?://\\S+)', myString).group()\n    headers = {'Authorization': 'Bearer ' + token}\n    data = {'url': url}\n    response = requests.post('https://api.example.com/urls', headers=headers, data=json.dumps(data))\n    return response.json() [DOC_SEP]     cleaned_str = re.sub('[^A-Za-z0-9]+', '', input_str)\n    hashed_str = hashlib.sha256(cleaned_str.encode()).hexdigest()\n\n    return hashed_str [DOC_SEP]     path_components = re.split(f'({delimiter})', path)\n    hashes = []\n\n    for component in path_components:\n        if not component:  # Remove empty components\n            continue\n        if component != delimiter and os.path.isfile(component):\n            with open(component, 'rb') as f:\n                hashes.append(hashlib.sha256(f.read()).hexdigest())\n        else:\n            hashes.append(None)\n\n    return list(zip(path_components, hashes)) [DOC_SEP]     with open(file_path, 'r') as f:\n        data = json.load(f)\n    \n    value = data['A'][unknown_key][\"maindata\"][0][\"Info\"]\n    hashed_value = hashlib.sha256(value.encode()).digest()\n    hashed_str = base64.b64encode(hashed_value).decode()\n\n    new_file_name = f\"{unknown_key}_hashed_{int(time.time())}.txt\"\n    new_file_path = os.path.join(os.getcwd(), new_file_name)\n\n    with open(new_file_path, 'w') as f:\n        f.write(hashed_str)\n\n    return new_file_path [DOC_SEP]     data = json.loads(json_data)\n    url = data[unknown_key]  # Assuming the key directly contains the URL\n    \n    response = requests.get(url)\n    \n    # Using datetime to include milliseconds in the timestamp\n    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S%f\")\n    filename = f\"{unknown_key}_{timestamp}.txt\"\n    save_dir = save_dir or os.getcwd()\n    file_path = os.path.join(save_dir, filename)\n    \n    with open(file_path, 'wb') as f:\n        f.write(response.content)\n    return file_path [DOC_SEP]     hash_dict = {}\n    \n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            file_path = Path(root) / file\n            with open(file_path, 'rb') as f:\n                bytes = f.read()  # read entire file as bytes\n                readable_hash = hashlib.sha256(bytes).hexdigest()\n                hash_dict[str(file_path)] = readable_hash\n                \n    # Save to JSON file\n    json_file = Path(directory) / 'hashes.json'\n    with open(json_file, 'w') as f:\n        json.dump(hash_dict, f)\n    return str(json_file) [DOC_SEP]     if not isinstance(salt, str):\n        raise TypeError\n    cursor.execute(\"SELECT id, password FROM users\")\n    users = cursor.fetchall()\n    count_updated = 0\n\n    for user in users:\n        password = user[1].encode('utf-8')\n        salted_password = password + salt.encode('utf-8')\n        hash_obj = hashlib.sha256(salted_password)\n        hashed_password = binascii.hexlify(hash_obj.digest()).decode('utf-8')\n\n        cursor.execute(f\"UPDATE users SET password = '{hashed_password}' WHERE id = {user[0]}\")\n        count_updated += 1\n\n    return count_updated [DOC_SEP]     if SALT_LENGTH < 0:\n        raise ValueError\n    \n    salt = os.urandom(SALT_LENGTH)\n    salted_password = PREFIX + password + salt.hex()\n    \n    hashed_password = hashlib.sha256(salted_password.encode()).digest()\n\n    return base64.b64encode(hashed_password).decode() [DOC_SEP]     try:\n        response = requests.get(API_URL + endpoint)\n        response.raise_for_status()  # Raises an HTTPError for bad responses\n        data = response.json()\n    except requests.RequestException as e:\n        raise RuntimeError(f\"Error fetching data from API: {e}\")\n\n    filename = PREFIX + endpoint + '.json'\n    with open(filename, 'w') as f:\n        json.dump(data, f)\n\n    return filename [DOC_SEP]     if not os.path.exists(source_dir):\n        raise FileNotFoundError(f\"Source directory '{source_dir}' does not exist.\")\n    \n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n    \n    new_files = []\n    for file_path in glob.glob(os.path.join(source_dir, '*')):\n        with open(file_path, 'r') as infile:\n            content = infile.read()\n        \n        hash_object = hashlib.md5(content.encode())\n        new_file_path = os.path.join(target_dir, os.path.basename(file_path))\n        \n        with open(new_file_path, 'w') as outfile:\n            outfile.write(f\"{prefix}{hash_object.hexdigest()}\\n{content}\")\n        \n        new_files.append(new_file_path)\n    \n    return new_files [DOC_SEP]     response = requests.get(API_URL + user + '/repos')\n    data = json.loads(response.text)\n    repos = {repo['name']: repo['created_at'] for repo in data}\n    sorted_repos = collections.OrderedDict(sorted(repos.items(), key=lambda x: x[1]))\n    return list(sorted_repos.keys()) [DOC_SEP]     response = requests.get(url, headers=headers)\n    soup = bs4.BeautifulSoup(response.text, 'html.parser')\n    text = soup.get_text()\n\n    emails = re.findall(regex, text)\n\n    with open(csv_path, 'w', newline='') as f:\n        write = csv.writer(f)\n        write.writerow(['Emails'])\n        for email in emails:\n            write.writerow([email])\n    \n    return csv_path [DOC_SEP]     HEADERS = {'User-Agent': 'Mozilla/5.0'}\n    PHONE_REGEX = r\"\\+\\d{1,3}?\\s?\\(?\\d{1,4}?\\)?\\s?\\d{1,4}?\\s?\\d{1,9}\"\n\n    # Handling local files separately\n    if url.startswith(\"file://\"):\n        with open(url[7:], 'r') as file:\n            text = file.read()\n    else:\n        response = requests.get(url, headers=HEADERS)\n        soup = bs4.BeautifulSoup(response.text, 'html.parser')\n        text = soup.get_text()\n\n    phone_numbers = re.findall(PHONE_REGEX, text)\n\n    with open(output_path, 'w') as f:\n        json.dump(phone_numbers, f)\n\n    return phone_numbers [DOC_SEP]     sorted_array = np.sort(matrix, axis=None)\n    \n    combinations = list(itertools.combinations(sorted_array, 2))\n    \n    return sorted_array, combinations [DOC_SEP]     df = pd.DataFrame(data)\n    \n    X = df[['Hours']]\n    y = df['Scores']\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    \n    predictions = model.predict(X_test)\n    \n    mse = np.mean((y_test - predictions) ** 2)\n    \n    return mse"
}