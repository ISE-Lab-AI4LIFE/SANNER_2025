{
  "id": "BigCodeBench_v0_1_0-00000-of-00001_document_16",
  "text": "    # Check for empty DataFrame\n    if df.empty:\n        return None\n\n    # Filter the DataFrame based on provided column names\n    selected_df = df[(df[columns[1]] > height) & (df[columns[2]] < weight)]\n    \n    # If no rows match the condition, return None\n    if selected_df.empty:\n        return None\n    \n    X = selected_df[columns[1:]]\n    y = selected_df[columns[0]]\n    X = sm.add_constant(X)\n    model = sm.OLS(y, X)\n    results = model.fit()\n    return results [DOC_SEP]     import random\n    samples = random.choices(values, weights=weights, k=n_samples)\n    histogram = dict(Counter(samples))\n\n    return histogram [DOC_SEP] \n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"data should be a DataFrame.\")\n    \n    if data.empty:\n        raise ValueError(\"data should contain at least one row.\")\n    \n    if target_column not in data.columns:\n        raise ValueError(\"target_column should be in the provided DataFrame.\")\n    \n    if not all(np.issubdtype(dtype, np.number) for dtype in data.dtypes):\n        raise ValueError(\"data values should be numeric only.\")\n    \n    if test_size <= 0 or test_size >= 1:\n        raise ValueError(\"test_size should be between 0 and 1: 0 < test_size < 1\")\n    \n    if isinstance(random_state, int) is not True:\n        raise ValueError(\"random_state should be an integer.\") \n    \n    \n    X = data.drop(columns=[target_column])\n    y = data[target_column]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n    model = LinearRegression().fit(X_train, y_train)\n\n    return model.score(X_test, y_test) [DOC_SEP]     distances = []\n\n    for _ in range(n):\n        theta = 2 * math.pi * random.random()\n        r = RADIUS * math.sqrt(random.random())\n        x = r * math.cos(theta)\n        y = r * math.sin(theta)\n        distance = math.sqrt(x**2 + y**2)\n        distances.append(distance)\n\n    return round(statistics.mean(distances), 4) [DOC_SEP]     from_user_values = np.array([d['from_user'] for d in result if 'from_user' in d])\n    # Handle edge case of empty array\n    if len(from_user_values) == 0:\n        summary = {\n            'mean': np.nan,\n            'median': np.nan,\n            'min': np.nan,\n            'max': np.nan,\n            'std': np.nan,\n            'current_time': datetime.now().strftime(DATE_FORMAT)\n        }\n    \n    elif not np.issubdtype(from_user_values.dtype, np.number):\n         raise ValueError(\"from_user values should be numeric only.\")\n\n\n    else:\n        summary = {\n            'mean': np.mean(from_user_values),\n            'median': np.median(from_user_values),\n            'min': np.min(from_user_values),\n            'max': np.max(from_user_values),\n            'std': np.std(from_user_values),\n            'current_time': datetime.now().strftime(DATE_FORMAT)\n        }\n\n    summary_series = pd.Series(summary)\n    return summary_series [DOC_SEP]     new_filenames = []\n    for filename in glob.glob(os.path.join(directory_path, '*')):\n        base_name = os.path.basename(filename)\n        new_base_name = '.'.join(base_name.split('.')[::-1])\n        os.rename(filename, os.path.join(directory_path, new_base_name))\n        new_filenames.append(new_base_name)\n    return new_filenames [DOC_SEP] \n    if Path(source_dir).is_dir() == False:\n        raise ValueError(\"source_dir does not exist.\")\n\n    if Path(target_dir).is_dir() == False:\n        raise ValueError(\"target_dir does not exist.\")\n\n    count = 0\n\n    for extension in extensions:\n        for file_name in Path(source_dir).glob(f'*{extension}'):\n            shutil.move(str(file_name), target_dir)\n            count += 1\n\n    return count [DOC_SEP]     vectorized_reverse = np.vectorize(lambda s: '.'.join(s.split('.')[::-1]))\n    \n    now = datetime.datetime.now()\n    \n    return vectorized_reverse(arr) [DOC_SEP] \n    if not isinstance(num_samples, int):\n        raise ValueError(\"num_samples should be an integer.\")\n\n    rng = np.random.default_rng(seed=rng_seed)\n    countries = rng.choice(countries, num_samples)\n    ages = rng.choice(ages, num_samples)\n    genders = rng.choice(genders, num_samples)\n\n    le = LabelEncoder()\n    encoded_genders = le.fit_transform(genders)\n\n    demographics = pd.DataFrame({\n        'Country': countries,\n        'Age': ages,\n        'Gender': encoded_genders\n    })\n\n    return demographics [DOC_SEP]     moved_files = []\n    for path, dirs, files in os.walk(source_directory):\n        for filename in fnmatch.filter(files, file_pattern):\n            shutil.move(os.path.join(path, filename), os.path.join(destination_directory, filename))\n            moved_files.append(filename)\n    return moved_files [DOC_SEP]     \n    # Correcting the encoding for Latin names\n    latin_names = [codecs.encode(name, 'utf-8').decode('utf-8') for name in latin_names]\n    \n    if rng_seed is not None:\n        np.random.seed(rng_seed)\n\n    data = []\n    for i in range(1, 101):\n        is_latin = np.random.choice([True, False])\n        name = np.random.choice(latin_names) if is_latin else np.random.choice(other_names)\n        birth_year = np.random.randint(start_year, end_year + 1)\n        dob = datetime.datetime(birth_year, np.random.randint(1, 13), np.random.randint(1, 29))\n        # Creating the email by removing spaces in names, converting to lowercase, and appending details\n        email = re.sub(r'\\s+', '.', name.lower()) + str(birth_year) + '@' + email_domain\n        data.append([i, name, dob, email])\n\n    df = pd.DataFrame(data, columns=['ID', 'Name', 'Date of Birth', 'Email'])\n\n    return df [DOC_SEP]     data = json.loads(json_str)\n    \n    # Remove None values and replace emails\n    processed_data = {}\n    for key, value in data.items():\n        if value is None:\n            continue\n        if isinstance(value, str) and re.match(r\"[^@]+@[^@]+\\.[^@]+\", value):\n            value = REPLACE_NONE\n        processed_data[key] = value\n\n    # Count frequency of each unique value\n    value_counts = Counter(processed_data.values())\n\n    return {\"data\": processed_data, \"value_counts\": value_counts} [DOC_SEP] \n    os.makedirs(directory_name, exist_ok=True)\n\n    for file_name in file_names:\n        with open(os.path.join(directory_name, file_name), 'wb') as f:\n            f.write(codecs.encode(content, encoding))\n\n    zipped_file = directory_name + '.zip'\n    with zipfile.ZipFile(zipped_file, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for root, dirs, files in os.walk(directory_name):\n            for file in files:\n                zipf.write(os.path.join(root, file))\n\n    return zipped_file  [DOC_SEP]     with open(input_file, 'r') as f:\n        data = json.load(f)\n    \n    stats = defaultdict(list)\n    for d in data:\n        for key, value in d.items():\n            stats[key].append(value)\n    \n    result = {k: {'mean': np.mean(v), 'median': np.median(v)} for k, v in stats.items()}\n\n    with open(output_file, 'w', newline='') as f:\n        writer = csv.DictWriter(f, fieldnames=['key', 'mean', 'median'])\n        writer.writeheader()\n        for key, values in result.items():\n            writer.writerow({'key': key, 'mean': values['mean'], 'median': values['median']})\n    \n    return result [DOC_SEP] \n    if not isinstance(csv_file, str):\n        raise TypeError(\"csv_file should be a string.\")\n    \n    if not isinstance(names, list):\n        raise TypeError(\"names should be a list.\")\n    \n    if not isinstance(latin_names, list):\n        raise TypeError(\"latin_names should be a list.\")\n\n    if rng_seed is not None:\n        random.seed(rng_seed)\n\n    with open(csv_file, 'w', newline='', encoding=encoding) as csvfile:\n        fieldnames = ['Name', 'Age']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        writer.writeheader()\n\n        for _ in range(50):\n            if latin_names:\n                writer.writerow({'Name': random.choice(latin_names), 'Age': random.randint(20, 50)})\n            if names:\n                writer.writerow({'Name': random.choice(names), 'Age': random.randint(20, 50)})\n\n    return csv_file [DOC_SEP]     # Check if the target directory exists, if not create it\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    copied_files = []\n\n    for file, content in kwargs.items():\n        if content is not None and os.path.isfile(file):\n            target_file = Path(target_dir) / Path(file).name\n            shutil.copyfile(file, target_file)\n            copied_files.append(str(target_file))\n\n    return copied_files [DOC_SEP] \n    if not isinstance(string, str):\n        raise TypeError(\"Input string should be of type string.\")\n\n    if not isinstance(patterns, list):\n        raise TypeError(\"patterns should be a list of strings.\")\n    \n    if not all(isinstance(s, str) for s in patterns):\n        raise TypeError(\"patterns should be a list of strings.\")\n\n    \n\n    pattern_counts = collections.defaultdict(int)\n\n    for pattern in patterns:\n        pattern_counts[pattern] = len(re.findall(pattern, string))\n\n    return dict(pattern_counts) [DOC_SEP]     flat_list = [random.choice(LETTERS) for _ in list_of_lists]\n\n    return dict(Counter(flat_list)) [DOC_SEP] \n    if not os.path.isdir(dir_path):\n        raise ValueError(\"Specified directory does not exist.\")\n\n    result = {}\n    file_paths = glob.glob(f'{dir_path}/**/*.txt', recursive=True)\n    for file_path in file_paths:\n        with open(file_path, 'r') as file:\n            content = file.read()\n        matches = re.findall(r'\\berror\\b', content, re.IGNORECASE)\n        # Always set the file's count in the result dictionary, even if it's 0\n        result[os.path.relpath(file_path, dir_path)] = len(matches)\n\n    return result [DOC_SEP]     flat_list = list(itertools.chain(*list_of_menuitems))\n\n    counter = Counter(flat_list)\n\n    return max(counter.items(), key=operator.itemgetter(1))[0] [DOC_SEP] \n    if num_samples * test_size < 2:\n        raise ValueError(\"Test set should contain at least 2 samples. num_samples * testsize >=2\")\n\n    if random_seed is not None:\n        np.random.seed(random_seed)\n\n    X = np.random.rand(num_samples, 1)\n    y = 2*X.squeeze() + 1 + np.random.randn(num_samples) * noise_strength\n\n    X_train, X_test, y_train, y_test = train_test_split(\n                                            X, y,\n                                            test_size=test_size,\n                                            random_state=random_seed\n                                            )\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    r_squared = model.score(X_test, y_test)\n\n    return r_squared, model [DOC_SEP] \n    file_dir = Path(directory)\n    file_pattern = re.compile(pattern)\n    new_files = []\n    \n    for filename in os.listdir(file_dir):\n        match = file_pattern.match(filename)\n        if match is not None:\n            prefix = match.group(1)\n            new_filename = f'{prefix}.csv'\n            with open(file_dir / filename, 'r') as infile, open(file_dir / new_filename, 'w') as outfile:\n                reader = csv.reader(infile)\n                writer = csv.writer(outfile)\n                writer.writerows(reader)\n            new_files.append(new_filename)\n    \n    return new_files [DOC_SEP] \n    if random_seed is not None:\n        np.random.seed(random_seed)\n    data = np.random.randn(num_samples, 1)*k + d\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(data)\n\n    mse = mean_squared_error(data, scaled_data)\n\n    return mse [DOC_SEP]     SOURCE_DIR = '/source/dir'\n    TARGET_DIR = '/target/dir'\n    FILE_PATTERN = re.compile(r'^(.*?)-\\d+\\.json$')\n    for filename in os.listdir(SOURCE_DIR):\n        match = FILE_PATTERN.match(filename)\n        if match is not None:\n            prefix = match.group(1)\n            new_filename = f'{prefix}.json'\n            shutil.move(os.path.join(SOURCE_DIR, filename), os.path.join(TARGET_DIR, new_filename)) [DOC_SEP]     \n    if num_samples / cv < 2:\n        raise ValueError(\"num_samples / cv should be greater than or equal to 2.\")\n\n    np.random.seed(random_seed)\n    X = np.random.randn(num_samples, 5)\n    y = np.sum(X, axis=1) + np.random.randn(num_samples)\n    \n    model = RandomForestRegressor(n_estimators=n_estimators,\n                                  random_state=random_seed\n                                  )\n    \n    cv_scores = cross_val_score(model, X, y, cv=cv)\n    \n    return np.mean(cv_scores), model [DOC_SEP]     # Match and extract the portion before the last hyphen\n    match = re.search(r'^(.*)-', string)\n    if match:\n        prefix = match.group(1)\n    else:\n        # If there's no hyphen, the whole string is considered if it is letters only\n        prefix = string if string.isalpha() else \"\"\n\n    # Count each letter in the prefix\n    letter_counts = Counter(prefix)\n    # Initialize a dictionary with all letters set to zero count\n    result = {letter: 0 for letter in ascii_lowercase}\n    # Update this dictionary with the actual counts from the prefix\n    result.update({letter: letter_counts.get(letter, 0) for letter in letter_counts if letter in result})\n\n    return result [DOC_SEP]     try:\n        df = pd.read_csv(file_path)\n        df.sort_values(by=[sort_key], inplace=True)\n\n        if linear_regression:\n            if x_column not in df.columns or y_column not in df.columns:\n                raise ValueError(\"Specified columns for linear regression do not exist in the dataframe\")\n\n            X = df[[x_column]]\n            y = df[y_column]\n            model = LinearRegression().fit(X, y)\n            return model\n\n        if output_path:\n            df.to_csv(output_path, index=False)\n            return output_path\n        else:\n            return df\n    except Exception as e:\n        raise Exception(f\"Error while processing the file: {str(e)}\") [DOC_SEP]     extracted_dirs = []\n    for filename in os.listdir(directory):\n        match = re.match(pattern, filename)\n        if match:\n            file_path = os.path.join(directory, filename)\n            # Use the part before the first '-' as the directory name.\n            base_name = match.group(1)\n            extract_path = os.path.join(directory, base_name)\n            with zipfile.ZipFile(file_path, 'r') as zip_ref:\n                zip_ref.extractall(extract_path)\n            if extract_path not in extracted_dirs:\n                extracted_dirs.append(extract_path)\n                os.makedirs(extract_path, exist_ok=True)  # Ensure the directory is created\n    return extracted_dirs [DOC_SEP]     if any(not sorted(dic.keys()) == ['category', 'id', 'title', 'title_url']  for dic in news_articles):\n        raise ValueError(\"input dictionaries must contain the following keys: 'category', 'id', 'title', 'title_url'\")\n\n    news_articles.sort(key=itemgetter('category', 'title'))\n\n    grouped_articles = defaultdict(list)\n    for category, group in groupby(news_articles, key=itemgetter('category')):\n        grouped_articles[category] = list(group)\n\n    return grouped_articles [DOC_SEP]     errors = []\n    if not os.path.exists(directory):\n        errors.append(f\"Directory does not exist: {directory}\")\n        return None, errors\n\n    if not os.path.exists(directory):\n        errors.append(f\"Directory does not exist: {directory}\")\n        return None, errors\n\n    try:\n        if not os.path.exists(BACKUP_DIR):\n            os.makedirs(BACKUP_DIR)\n\n        backup_dir = get_unique_backup_dir()\n        os.makedirs(backup_dir)\n        shutil.copytree(directory, os.path.join(backup_dir, os.path.basename(directory)))\n        try:\n            shutil.rmtree(directory)  # Deleting contents after backup\n        except PermissionError as e:\n            errors.append(f\"Permission denied: {e}\")\n            shutil.copytree(os.path.join(backup_dir, os.path.basename(directory)), directory)  # Restore original if cleanup fails\n        os.makedirs(directory, exist_ok=True)  # Recreating the original directory\n    except Exception as e:\n        errors.append(str(e))\n\n    return \"/fake/backup/path\", errors\n    \n    try:\n        shutil.copytree(directory, os.path.join(backup_dir, os.path.basename(directory)))\n        shutil.rmtree(directory)  # Deleting contents after backup\n        os.makedirs(directory)  # Recreating the original directory\n    except Exception as e:\n        errors.append(str(e))\n\n    return backup_dir, errors [DOC_SEP] \n    if not isinstance(articles, list):\n        raise TypeError(\"articles should be a list of dictionaries.\")\n\n    if not all(isinstance(item, dict) for item in articles):\n        raise TypeError(\"articles should be a list of dictionaries.\")\n\n    if len(articles) == 0:\n        raise ValueError(\"input articles list should contain at least one article.\")\n\n    if any(not sorted(dic.keys()) == ['category', 'id', 'published_time', 'title', 'title_url'] for dic in articles):\n        raise ValueError(\n            \"input dictionaries must contain the following keys: 'category', 'id', 'title', 'title_url', 'published_time'\")\n\n    tz = pytz.timezone(timezone)\n    for article in articles:\n        article['published_time'] = pd.to_datetime(article['published_time']).astimezone(tz)\n\n    df = pd.DataFrame(articles)\n    df['published_time'] = df['published_time'].dt.hour\n\n    analysis_df = df.groupby('category')['published_time'].agg(['count', 'mean', 'min', 'max'])\n\n    return analysis_df [DOC_SEP]     try:\n        size = os.path.getsize(filepath)\n        mtime = os.path.getmtime(filepath)\n        mtime = datetime.fromtimestamp(mtime).strftime('%Y-%m-%d %H:%M:%S')\n    except OSError as e:\n        raise Exception(f\"Error: {e}\")\n\n    return {'size': f\"{size} bytes\", 'last_modified': mtime} [DOC_SEP]     random.seed(random_seed)\n    np.random.seed(random_seed)\n\n    data = []\n    for _ in range(n):\n        title = f\"Article {_}\"\n        title_url = f\"{domain}/Article_{_}\"\n        id = _\n        category = random.choice(categories)\n        views = np.random.poisson(1000)\n        data.append({'title': title, 'title_url': title_url, 'id': id, 'category': category, 'views': views})\n\n    df = pd.DataFrame(data)\n    return df [DOC_SEP]     files_moved = 0\n\n    for file_name in os.listdir(src_dir):\n        if file_name.endswith(extension):\n            shutil.move(os.path.join(src_dir, file_name), os.path.join(dest_dir, file_name))\n            files_moved += 1\n\n    return files_moved [DOC_SEP]     survey_data = []\n\n    random.seed(random_seed)\n    \n    for _ in range(n):\n        site = random.choice(news_sites)\n        category = random.choice(categories)\n        response = random.choice(likert_scale)\n        value = likert_scale.index(response) + 1  # Assign a numerical value to the response\n        survey_data.append({'Site': site, 'Category': category, 'Response': response, 'Value': value})\n    \n    with open(file_path, 'w', newline='') as csvfile:\n        fieldnames = ['Site', 'Category', 'Response', 'Value']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        writer.writeheader()\n        writer.writerows(survey_data)\n        \n    df = pd.read_csv(file_path)\n    \n    return df [DOC_SEP]     # Create archive directory if it does not exist\n    if not os.path.exists(ARCHIVE_DIR):\n        os.makedirs(ARCHIVE_DIR)\n\n    # Get the list of files matching the pattern\n    file_list = glob.glob(pattern)\n    \n    if not file_list:\n        return \"No files found matching the pattern.\"\n\n    # Create a unique archive file name\n    archive_file_base = os.path.join(ARCHIVE_DIR, 'archive')\n    archive_file = archive_file_base + '.tar.gz'\n    counter = 1\n    while os.path.exists(archive_file):\n        archive_file = archive_file_base + f\"_{counter}.tar.gz\"\n        counter += 1\n    \n    # Create an archive file\n    subprocess.run(['tar', '-czf', archive_file] + file_list)\n    \n    # Delete the original files\n    for file in file_list:\n        os.remove(file)\n    \n    return archive_file [DOC_SEP]     \n    random.seed(random_seed)\n    \n    sales_data = []\n    \n    for _ in range(n):\n        country = random.choice(countries)\n        product = random.choice(products)\n        sales = random.randint(1, 100)\n        sales_data.append({'Country': country, 'Product': product, 'Sales': sales})\n\n    # If an output path is provided, save the data to a CSV file\n    if output_path:\n        with open(output_path, 'w', newline='') as csvfile:\n            fieldnames = ['Country', 'Product', 'Sales']\n            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n            writer.writeheader()\n            writer.writerows(sales_data)\n        \n    return pd.DataFrame(sales_data) [DOC_SEP]     if len(array1) != len(array2):\n        raise ValueError(\"The input arrays must have the same length.\")\n    \n    if len(array1) == 0:\n        return 0\n    \n    max_distance = 0\n    for comb in combinations(zip(array1, array2), 2):\n        distance = np.linalg.norm(np.array(comb[0]) - np.array(comb[1]))\n        if distance > max_distance:\n            max_distance = distance\n\n    return max_distance [DOC_SEP]     if N <= 1:\n        raise ValueError(f\"N should be greater than 1. Received N={N}.\")\n\n    # Ensure provided columns exist in the dataframe\n    if col1 not in df.columns or col2 not in df.columns:\n        raise ValueError(f\"Columns {col1} or {col2} not found in the DataFrame.\")\n    \n    # Extract values from the specified columns\n    l1 = df[col1].values\n    l2 = df[col2].values\n    \n    # Find the indices of the N largest differences\n    largest_diff_indices = heapq.nlargest(N, range(len(l1)), key=lambda i: abs(l1[i] - l2[i]))\n    \n    # Perform the t-Test and return the p-value\n    _, p_value = stats.ttest_ind(l1[largest_diff_indices], l2[largest_diff_indices])\n    return p_value [DOC_SEP]     np.random.seed(42)  # For reproducibility, as shown in your example\n    array = np.random.randint(0, 10, ARRAY_LENGTH).reshape(-1, 1)\n    scaler = MinMaxScaler()\n    scaled_array = scaler.fit_transform(array)\n    return scaled_array [DOC_SEP]     # Ensure provided columns exist in the dataframe\n    if col1 not in df.columns or col2 not in df.columns:\n        raise ValueError(f\"Columns {col1} or {col2} not found in the DataFrame.\")\n\n\n    scaler = StandardScaler()\n    df[[col1, col2]] = scaler.fit_transform(df[[col1, col2]])\n\n    l1 = df[col1].values\n    l2 = df[col2].values\n\n    largest_diff_indices = heapq.nlargest(N, range(len(l1)), key=lambda i: abs(l1[i] - l2[i]))\n\n    return largest_diff_indices [DOC_SEP]     if not l:  # Check if the list is empty\n        return Counter()  # Return an empty counter if the list is empty\n\n    random.shuffle(l)\n    l_cycled = cycle(l)\n    counter = Counter(next(l_cycled) for _ in range(30))\n    keys = list(counter.keys())\n    counter = Counter({k: counter[k] for k in keys[3:] + keys[:3]})\n    \n    return counter [DOC_SEP]     # Ensure provided columns exist in the dataframe\n    if feature not in df.columns or target not in df.columns:\n        raise ValueError(f\"Columns {feature} or {target} not found in the DataFrame.\")\n\n\n    X = df[feature].values.reshape(-1, 1)\n    y = df[target].values\n    model = LinearRegression()\n    model.fit(X, y)\n    residuals = y - model.predict(X)\n    largest_residual_indices = heapq.nlargest(n, range(len(residuals)), key=lambda i: abs(residuals[i]))\n    return largest_residual_indices, model [DOC_SEP]     if l is None:\n        l = ELEMENTS.copy()  # Use a copy to avoid modifying the original list\n    random.shuffle(l)\n    arr = np.array(l)\n    arr = np.concatenate((arr[3:], arr[:3]))\n    return arr [DOC_SEP]     random.seed(random_seed)\n    # Constants\n    BRACKETS = \"(){}[]\"\n    return ''.join(random.choice(string.ascii_lowercase + BRACKETS) for _ in range(length)) [DOC_SEP]     if not l:  # Handle empty list\n        return deque()\n    dq = deque(l)\n    dq.rotate(3)\n\n    # Calculate the square root of the sum of numeric elements in the deque for demonstration.\n    numeric_sum = sum(item for item in dq if isinstance(item, (int, float)))\n    if numeric_sum > 0:\n        print(f\"The square root of the sum of numeric elements: {math.sqrt(numeric_sum)}\")\n    \n    return dq [DOC_SEP]     BRACKET_PATTERN = '[(){}\\\\[\\\\]]'  # Corrected pattern to match any type of bracket\n    \n    file_list = []\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if re.search(BRACKET_PATTERN, file):\n                file_list.append(os.path.join(root, file))\n    return file_list [DOC_SEP] \n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"df should be a DataFrame.\")\n\n    # Constants\n    BRACKETS_PATTERN = '[(){}[\\]]'\n\n    return df.applymap(\n        lambda x: len(re.findall(BRACKETS_PATTERN, str(x)))\n        ).sum().sum() [DOC_SEP]     # Check if the backup directory exists\n    if not os.path.exists(BACKUP_DIR):\n        return f'Backup directory {BACKUP_DIR} does not exist. Cannot rollback update.'\n\n    backups = sorted(os.listdir(BACKUP_DIR))\n    latest_backup = backups[-1] if backups else None\n\n    if not latest_backup:\n        return f'No backups found in {BACKUP_DIR}. Cannot rollback update.'\n\n    if os.path.exists(directory):\n        shutil.rmtree(directory)\n\n    shutil.copytree(os.path.join(BACKUP_DIR, latest_backup), directory)\n    return directory [DOC_SEP]     if random_seed is not None:\n        seed(random_seed)\n\n    if len(L) == 0:\n        return pd.DataFrame(), []\n\n    LETTERS = list('abcdefghijklmnopqrstuvwxyz')\n    max_cols = min(len(LETTERS), len(L[0]))\n    col_names = choices(LETTERS, k=max_cols)\n    dataframes = []\n\n    for _ in range(num_dataframes):\n        # Randomly sample rows from L for each DataFrame\n        sampled_rows = choices(L, k=3)\n        dataframe = pd.DataFrame(sampled_rows, columns=col_names)\n        dataframes.append(dataframe)\n\n    # Finding common rows across all DataFrames\n    # Concatenate all DataFrames and find common rows\n    combined_df = pd.concat(dataframes, ignore_index=True)\n    common_rows = combined_df[combined_df.duplicated(keep=False)]\n\n    return common_rows.drop_duplicates(), dataframes"
}