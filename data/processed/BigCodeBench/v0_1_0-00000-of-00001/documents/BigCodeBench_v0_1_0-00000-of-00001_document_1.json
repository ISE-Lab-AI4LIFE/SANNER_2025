{
  "id": "BigCodeBench_v0_1_0-00000-of-00001_document_1",
  "text": "    permutations = list(itertools.permutations(numbers))\n    sum_diffs = 0\n\n    for perm in permutations:\n        perm = list(perm)\n        shuffle(perm)\n        diffs = [abs(perm[i] - perm[i+1]) for i in range(len(perm)-1)]\n        sum_diffs += sum(diffs)\n\n    avg_sum_diffs = sum_diffs / len(permutations)\n    \n    return avg_sum_diffs [DOC_SEP]     if length < 0:\n        raise ValueError\n    random_string = ''.join(random.choices(string.ascii_uppercase + string.ascii_lowercase, k=length))\n    char_counts = collections.Counter(random_string)\n    return dict(char_counts) [DOC_SEP]     random_dict = {k: [random.randint(0, 100) for _ in range(random.randint(1, 10))] for k in LETTERS}\n    sorted_dict = dict(sorted(random_dict.items(), key=lambda item: statistics.mean(item[1]), reverse=True))\n    return sorted_dict [DOC_SEP]     random_dict = {k: [random.randint(0, 100) for _ in range(random.randint(1, 10))] for k in LETTERS}\n    mean_dict = {k: np.mean(v) for k, v in random_dict.items()}\n    return mean_dict [DOC_SEP]     count_dict = Counter(itertools.chain.from_iterable(d.values()))\n    return dict(count_dict) [DOC_SEP]     random_dict = {k: [random.randint(0, 100) for _ in range(random.randint(1, 10))] for k in LETTERS}\n    sd_dict = {\n        k: math.sqrt(sum((i - sum(v) / len(v)) ** 2 for i in v) / len(v))\n        for k, v in random_dict.items()\n    }\n    return sd_dict [DOC_SEP]     log_files = [f for f in os.listdir(log_dir) if re.match(pattern, f)]\n    log_files = sorted(log_files, key=lambda f: os.path.getmtime(os.path.join(log_dir, f)), reverse=True)\n\n    return os.path.join(log_dir, log_files[0]) if log_files else None [DOC_SEP]     with open(csv_file_path, 'r') as f:\n        reader = csv.reader(f)\n        next(reader)  # Skip the header row\n        sales_data = collections.defaultdict(int)\n        for row in reader:\n            product, quantity = row[0], int(row[1])\n            sales_data[product] += quantity\n\n    top_selling_product = max(sales_data.items(), key=operator.itemgetter(1))[0]\n\n    return top_selling_product [DOC_SEP]     int_list = [list(map(int, x)) for x in T1]\n    flattened_list = list(itertools.chain(*int_list))\n    total_nums = sum(flattened_list)\n\n    random_nums = [randint(0, RANGE) for _ in range(total_nums)]\n    counts = Counter(random_nums)\n\n    return counts [DOC_SEP]     df = pd.DataFrame(list_of_pairs, columns=[\"Category\", \"Value\"])\n    plt.figure(figsize=(10, 5))\n    sns.barplot(x=\"Category\", y=\"Value\", data=df)\n    plt.title(\"Category vs Value\")\n    ax = plt.gca()\n    return df, ax [DOC_SEP]     if len(T1) <= 0:\n        raise statistics.StatisticsError\n    int_list = [list(map(int, x)) for x in T1]\n    flattened_list = list(itertools.chain(*int_list))\n    total_nums = sum(flattened_list)\n    random_nums = [random.randint(0, RANGE) for _ in range(total_nums)]\n    mean = np.mean(random_nums)\n    median = np.median(random_nums)\n    mode = statistics.mode(random_nums)\n    return mean, median, mode [DOC_SEP]     int_list = [list(map(int, x)) for x in T1]\n    flattened_list = list(itertools.chain(*int_list))\n    total_nums = sum(flattened_list)\n\n    random_nums = [random.randint(0, max_value) for _ in range(total_nums)]\n\n    p25 = np.percentile(random_nums, 25)\n    p50 = np.percentile(random_nums, 50)\n    p75 = np.percentile(random_nums, 75)\n\n    return p25, p50, p75 [DOC_SEP]     log_data = {}\n\n    if not os.path.isfile(script_name):\n        raise FileNotFoundError(f\"Script {script_name} does not exist.\")\n\n    start_time = datetime.now()\n    log_data['start_time'] = start_time.strftime('%Y-%m-%d %H:%M:%S')\n\n    try:\n        exit_status = subprocess.call(['./' + script_name])\n    except Exception as e:\n        raise RuntimeError(f\"Failed to run {script_name}: {str(e)}\")\n\n    end_time = datetime.now()\n    log_data['end_time'] = end_time.strftime('%Y-%m-%d %H:%M:%S')\n    log_data['exit_status'] = exit_status\n\n    with open(log_file, 'w') as f:\n        json.dump(log_data, f)\n    \n    return log_data [DOC_SEP]     # Attempt to connect to the FTP server\n    try:\n        ftp_obj = ftplib.FTP(ftp_server)\n    except Exception as e:\n        raise Exception(f'Failed to connect to FTP server {ftp_server}: {str(e)}')\n\n    # Attempt to login to the FTP server\n    try:\n        ftp_obj.login(ftp_user, ftp_password)\n    except Exception as e:\n        raise Exception(f'Failed to log into FTP server {ftp_server} with user {ftp_user}: {str(e)}')\n\n    # Attempt to change to the specified directory\n    try:\n        ftp_obj.cwd(ftp_dir)\n    except Exception as e:\n        raise Exception(f'Failed to change to directory {ftp_dir} on server {ftp_server}: {str(e)}')\n\n    # Directory to store downloaded files\n    download_dir = \"downloaded_files\"\n    if not os.path.exists(download_dir):\n        os.makedirs(download_dir)\n\n    downloaded_files = []\n    for filename in ftp_obj.nlst():\n        command = f'wget ftp://{ftp_user}:{ftp_password}@{ftp_server}{ftp_dir}/{filename} -P {download_dir}'\n        subprocess.call(command, shell=True)\n        downloaded_files.append(filename)\n\n    ftp_obj.quit()\n    return downloaded_files [DOC_SEP]     config = configparser.ConfigParser()\n    config.read(config_file_path)\n\n    project_dir = config.get('Project', 'directory')\n\n    if not os.path.isdir(project_dir):\n        raise FileNotFoundError(f'Directory {project_dir} does not exist.')\n\n    archive_file = f'{archieve_dir}/{os.path.basename(project_dir)}.zip'\n    \n    # Using shutil to create the zip archive\n    shutil.make_archive(base_name=os.path.splitext(archive_file)[0], format='zip', root_dir=project_dir)\n\n    if not os.path.isfile(archive_file):\n        raise Exception(f\"Failed to create archive {archive_file}\")\n\n    return True [DOC_SEP]     # Check if commands_file_path exists\n    if not os.path.exists(commands_file_path):\n        raise FileNotFoundError(f\"File '{commands_file_path}' not found.\")\n    \n    # Check if output_dir_path exists, if not, create it\n    if not os.path.exists(output_dir_path):\n        os.makedirs(output_dir_path)\n    \n    # Read commands from the CSV file\n    with open(commands_file_path, 'r') as f:\n        reader = csv.reader(f)\n        commands = [cmd[0] for cmd in list(reader)]\n    \n    output_files = []\n    for i, command in enumerate(commands):\n        output_file = f'{output_dir_path}/command_{i+1}_output.txt'\n        with open(output_file, 'w') as f:\n            ret_code = subprocess.call(command, shell=True, stdout=f, stderr=subprocess.STDOUT)\n            if ret_code != 0:\n                f.write(f\"\\nError executing command, exited with code {ret_code}\")\n        output_files.append(output_file)\n\n    return output_files [DOC_SEP]     if not os.path.exists(directory):\n        raise FileNotFoundError(f\"Directory '{directory}' not found.\")\n\n    log_files = glob.glob(os.path.join(directory, '*.log'))\n    if not log_files:\n        return \"No logs found to backup.\"\n\n    if not os.path.exists(backup_dir):\n        os.makedirs(backup_dir)\n\n    backup_file = os.path.join(backup_dir, 'logs_backup.tar.gz')\n    subprocess.call(['tar', '-czvf', backup_file] + log_files)\n\n    for file in log_files:\n        os.remove(file)\n\n    return backup_file [DOC_SEP]     # Check if the process is running\n    is_running = any([proc for proc in psutil.process_iter() if proc.name() == process_name])\n    \n    # If the process is running, terminate it\n    if is_running:\n        for proc in psutil.process_iter():\n            if proc.name() == process_name:\n                proc.terminate()\n                time.sleep(5)\n        subprocess.Popen(process_name)\n        return f\"Process found. Restarting {process_name}.\"\n    else:\n        subprocess.Popen(process_name)\n        return f\"Process not found. Starting {process_name}.\" [DOC_SEP]     # Check if file exists\n    if not os.path.exists(file):\n        print(\"Provided file does not exist.\")\n        return []\n    \n    # Check for CSV file extension\n    if not file.endswith('.csv'):\n        print(\"Provided file is not a CSV.\")\n        return []\n\n    try:\n        subprocess.call(['split', '-n', '5', '-d', file, 'split_'])\n        split_files = glob.glob('split_*')\n\n        for split_file in split_files:\n            with open(split_file, 'r') as f:\n                reader = csv.reader(f)\n                rows = list(reader)\n\n            random.shuffle(rows)\n\n            with open(split_file, 'w') as f:\n                writer = csv.writer(f)\n                writer.writerows(rows)\n\n        return split_files\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return [] [DOC_SEP]     if not os.path.exists(directory):\n        raise FileNotFoundError(f\"Directory '{directory}' not found.\")\n    files = [f for f in glob.glob(os.path.join(directory, '*')) if os.path.isfile(f)]\n    if not files:\n        return None\n    zip_file_path = os.path.join(directory, 'files.zip')\n    with zipfile.ZipFile(zip_file_path, 'w') as zipf:\n        for file in files:\n            zipf.write(file, os.path.basename(file))\n    \n    return zip_file_path [DOC_SEP]     df = pd.read_csv(csv_file)\n    df[\"dict_column\"] = df[\"dict_column\"].apply(ast.literal_eval)\n    # Convert 'dict_column' to string representation for plotting\n    df[\"hue_column\"] = df[\"dict_column\"].apply(str)\n    ax = sns.pairplot(df, hue=\"hue_column\")\n    return df, ax [DOC_SEP]     system_info = {}\n\n    system_info['OS'] = platform.system()\n    system_info['Architecture'] = platform.architecture()[0]\n\n    total_memory = psutil.virtual_memory().total\n    used_memory = psutil.virtual_memory().used\n    system_info['Memory Usage'] = f'{used_memory/total_memory*100:.2f}%'\n\n    return system_info [DOC_SEP]     combined = [val for pair in zip_longest(l1, l2) for val in pair if val is not None]\n    sample = choices(combined, k=K)\n    freq = collections.Counter(sample)\n    return freq [DOC_SEP]     combined = [val for pair in zip_longest(l1, l2) for val in pair if val is not None]\n    differences = np.abs(np.array(combined) - THRESHOLD)\n    closest_index = np.argmin(differences)\n    return combined[closest_index] [DOC_SEP]     if not password:\n        raise ValueError\n    salt = os.urandom(SALT_LENGTH)\n    hashed_password = hashlib.pbkdf2_hmac('sha256', password.encode(), salt, 100000)\n    return base64.b64encode(salt), base64.b64encode(hashed_password) [DOC_SEP]     json_str = json.dumps(data_dict)\n    compressed = zlib.compress(json_str.encode())\n    return base64.b64encode(compressed).decode() [DOC_SEP]     fernet = Fernet(base64.urlsafe_b64encode(encryption_key.encode()))\n    encrypted_message = fernet.encrypt(message.encode())\n    return base64.b64encode(encrypted_message).decode() [DOC_SEP]     # Adding current timestamp to the dictionary\n    data['timestamp'] = datetime.now().strftime(DATE_FORMAT)\n    \n    # Encoding the dictionary to a JSON-formatted string and then encoding it in ASCII using base64 encoding\n    json_data = json.dumps(data)\n    encoded_data = base64.b64encode(json_data.encode('ascii')).decode('ascii')\n    \n    return encoded_data [DOC_SEP]     json_data = json.dumps(data)\n    encoded_data = base64.b64encode(json_data.encode('ascii')).decode('ascii')\n    response = requests.post(url, json={\"payload\": encoded_data})\n    \n    return response [DOC_SEP]     scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(data)\n    standardized_data_str = np.array2string(standardized_data)\n    encoded_data = base64.b64encode(standardized_data_str.encode('ascii')).decode('ascii')\n    \n    return encoded_data [DOC_SEP]     if not os.path.isfile(file_path):\n        raise ValueError(f'{file_path} does not exist.')\n\n    with open(file_path, 'r') as f:\n        data = json.load(f)\n\n    for key in INPUT_JSON['required']:\n        if key not in data:\n            raise ValueError(f'{key} is missing from the JSON object.')\n        if not isinstance(data[key], INPUT_JSON['properties'][key]['type']):\n            raise ValueError(f'{key} is not of type {INPUT_JSON[\"properties\"][key][\"type\"]}.')\n\n    if 'email' in data and not re.fullmatch(EMAIL_REGEX, data['email']):\n        raise ValueError('Email is not valid.')\n\n    return data[attribute] [DOC_SEP]     words = text.split()\n    dollar_words = [\n        word\n        for word in words\n        if word.startswith(\"$\")\n        and not all(c in PUNCTUATION for c in word)\n        and len(word) > 1\n    ]\n    freq = nltk.FreqDist(dollar_words)\n    if not freq:  # If frequency distribution is empty, return None\n        return None\n    plt.figure(figsize=(10, 5))\n    sns.barplot(x=freq.keys(), y=freq.values())\n    return plt.gca() [DOC_SEP]     response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    tag_content = soup.find(tag)\n    \n    return tag_content.string if tag_content else None [DOC_SEP]     second_values = [pair[1] for pair in list_of_pairs]\n    product = reduce(np.multiply, second_values)\n    product_array = np.array([product])\n\n    return product_array [DOC_SEP]     # Remove URLs\n    text = re.sub(r\"http[s]?://\\S+\", \"\", text)\n    if not text.strip():  # Check if text is not empty after URL removal\n        raise ValueError(\n            \"No words available to generate a word cloud after removing URLs.\"\n        )\n    # Generate word cloud\n    wordcloud = WordCloud().generate(text)\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud)\n    plt.axis(\"off\")  # Do not show axis to make it visually appealing\n    return wordcloud [DOC_SEP]     df = df.applymap(lambda x: x if x in target_values else 0)\n    plt.figure(figsize=(10, 5))\n    for column in df.columns:\n        sns.kdeplot(df[column], label=column, warn_singular=False)\n    plt.legend()\n    return df, plt.gca() [DOC_SEP]     # Ensure the DataFrame contains only positive values\n    if (df <= 0).any().any():\n        raise ValueError(\"Input DataFrame should contain only positive values.\")\n\n    df = df.applymap(lambda x: x if x in TARGET_VALUES else 0)\n\n    transformed_df = pd.DataFrame()\n\n    fig, ax = plt.subplots()\n\n    for column in df.columns:\n        # Check if data is constant\n        if df[column].nunique() == 1:\n            transformed_df[column] = df[column]\n        else:\n            transformed_data, _ = stats.boxcox(\n                df[column] + 1\n            )  # Add 1 since the are some null values\n            transformed_df[column] = transformed_data\n\n            # Using matplotlib's kde method to plot the KDE\n            kde = stats.gaussian_kde(transformed_df[column])\n            x_vals = np.linspace(\n                min(transformed_df[column]), max(transformed_df[column]), 1000\n            )\n            ax.plot(x_vals, kde(x_vals), label=column)\n\n    ax.legend()\n    plt.show()\n    return transformed_df, fig [DOC_SEP] \n    X = df.drop(target_column, axis=1)\n    y = df[target_column]\n\n    model = RandomForestClassifier(random_state=42).fit(X, y)\n    feature_imp = pd.Series(model.feature_importances_, index=X.columns).sort_values(\n        ascending=False\n    )\n    plt.figure(figsize=(10, 5))\n    ax = sns.barplot(x=feature_imp, y=feature_imp.index)\n    ax.set_xlabel(\"Feature Importance Score\")\n    ax.set_ylabel(\"Features\")\n    ax.set_title(\"Visualizing Important Features\")\n    return model, ax [DOC_SEP]     scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(data_matrix)\n    df = pd.DataFrame(standardized_data, columns=FEATURE_NAMES)\n    df[\"Mean\"] = df.mean(axis=1)\n    plt.figure(figsize=(10, 5))\n    ax = df[\"Mean\"].plot(kind=\"hist\", title=\"Distribution of Means\")\n    return df, ax [DOC_SEP]     means = np.mean(data_matrix, axis=1)\n    population_mean = np.mean(data_matrix)\n\n    _, p_value = ttest_1samp(means, population_mean)\n    significant_indices = np.where(p_value < ALPHA)[0]\n\n    fig, ax = plt.subplots(figsize=(10, 5))\n    ax.plot(means, \"ro\", label=\"Means\")\n    ax.plot(\n        significant_indices, means[significant_indices], \"bo\", label=\"Significant Means\"\n    )\n    ax.axhline(y=population_mean, color=\"g\", linestyle=\"-\", label=\"Population Mean\")\n    ax.legend()\n    return significant_indices.tolist(), ax [DOC_SEP]     z_scores = zscore(data_matrix, axis=1)\n    feature_columns = [\"Feature \" + str(i + 1) for i in range(data_matrix.shape[1])]\n    df = pd.DataFrame(z_scores, columns=feature_columns)\n    df[\"Mean\"] = df.mean(axis=1)\n    correlation_matrix = df.corr()\n    ax = sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\")\n    return df, ax [DOC_SEP]     skewness = skew(data_matrix, axis=1)\n    df = pd.DataFrame(skewness, columns=[\"Skewness\"])\n    plt.figure(figsize=(10, 5))\n    df[\"Skewness\"].plot(kind=\"hist\", title=\"Distribution of Skewness\")\n    return df, plt.gca() [DOC_SEP]     pca = PCA(n_components=n_components)\n    transformed_data = pca.fit_transform(data_matrix)\n\n    df = pd.DataFrame(\n        transformed_data,\n        columns=[f\"Component {i+1}\" for i in range(transformed_data.shape[1])],\n    )\n    df[\"Mean\"] = df.mean(axis=1)\n\n    fig, ax = plt.subplots()\n    ax.plot(np.cumsum(pca.explained_variance_ratio_))\n    ax.set_xlabel(\"Number of Components\")\n    ax.set_ylabel(\"Cumulative Explained Variance\")\n    return df, ax [DOC_SEP]     df = df.fillna(df.mean(axis=0))\n    description = df.describe()\n    plots = []\n    for col in df.select_dtypes(include=[np.number]).columns:\n        plot = sns.displot(df[col], bins=10)\n        plots.append(plot.ax)\n    return description, plots [DOC_SEP]     df = df.fillna(df.mean(axis=0))\n    scaler = MinMaxScaler()\n    df[df.columns] = scaler.fit_transform(df[df.columns])\n    plt.figure(figsize=(10, 5))\n    df.boxplot(grid=False, vert=False, fontsize=15)\n    return df, plt.gca() [DOC_SEP]     # Select only numeric columns\n    df_numeric = df.select_dtypes(include=[np.number])\n    # Replace missing values\n    df_numeric = df_numeric.fillna(df_numeric.mean(axis=0))\n    # Perform PCA\n    pca = PCA(n_components=2)\n    principalComponents = pca.fit_transform(df_numeric)\n    principalDf = pd.DataFrame(\n        data=principalComponents,\n        columns=[\"Component 1\", \"Component 2\"],\n    )\n\n    # Plot scatter plot\n    ax = sns.scatterplot(data=principalDf, x=\"Component 1\", y=\"Component 2\")\n    plt.show()\n    return principalDf, ax [DOC_SEP]     # Fill missing values with column's average\n    df = df.fillna(df.mean(axis=0))\n    # Compute Z-scores\n    df = df.apply(zscore)\n    # Plot histograms for each numeric column\n    axes = df.hist(grid=False, bins=10, layout=(1, df.shape[1]))\n    plt.tight_layout()\n    return df, axes [DOC_SEP]     df = df.fillna(df.mean(axis=0))\n    scaler = StandardScaler()\n    df[df.columns] = scaler.fit_transform(df[df.columns])\n    plt.figure(figsize=(10, 5))\n    heatmap = sns.heatmap(df.corr(), annot=True, cmap=\"coolwarm\")\n    return df, heatmap [DOC_SEP]     timestamps = []\n    for _ in range(n):\n        timestamp = random.randint(0, int(time.time()))\n        formatted_time = datetime.utcfromtimestamp(timestamp).strftime(DATE_FORMAT)\n        timestamps.append(formatted_time)\n\n    plt.hist([datetime.strptime(t, DATE_FORMAT) for t in timestamps])\n\n    if output_path:\n        plt.savefig(output_path)\n    else:\n        plt.show()\n    return timestamps [DOC_SEP]     if not timestamps:\n        raise ValueError(\"Input list of timestamps is empty.\")\n    datetimes = [datetime.fromtimestamp(t).strftime(DATE_FORMAT) for t in timestamps]\n    df = pd.DataFrame({\"Timestamp\": timestamps, \"Datetime\": datetimes})\n    ax = plt.hist(pd.to_datetime(df[\"Datetime\"]))\n    plt.close()\n    return df, ax"
}