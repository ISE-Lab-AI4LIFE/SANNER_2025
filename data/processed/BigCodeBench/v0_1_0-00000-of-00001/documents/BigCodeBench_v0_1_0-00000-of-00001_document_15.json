{
  "id": "BigCodeBench_v0_1_0-00000-of-00001_document_15",
  "text": "    df = pd.DataFrame(data, columns=cols)\n    \n    df_np = np.array(df)\n    df = pd.DataFrame(df_np, columns=cols)\n    \n    correlation_matrix = df.corr()\n    return correlation_matrix [DOC_SEP]     X = pd.DataFrame.drop(df, target, axis=1)  \n    y = pd.Series(df[target])  \n    \n    model = LinearRegression()\n    model.fit(X, y)\n\n    return model.score(X, y) [DOC_SEP]     pca = PCA(n_components=2)\n    df_pca = pca.fit_transform(df)\n    \n    df_pca = pd.DataFrame(df_pca, columns=['PC1', 'PC2'])\n    \n    return df_pca [DOC_SEP]     df = pd.DataFrame(data, columns=cols)\n    dbscan = DBSCAN(eps=3, min_samples=2)\n    df['Cluster'] = dbscan.fit_predict(df)\n    return df [DOC_SEP]     if not 0 <= percentage <= 1:\n        raise ValueError('Percentage must be between 0 and 1')\n    df = pd.DataFrame(data, columns=cols)\n    corr_matrix = df.corr().abs()\n    columns = corr_matrix.columns\n    corr_combinations = []\n\n    for col1, col2 in combinations(columns, 2):\n        if corr_matrix.loc[col1, col2] > percentage:\n            corr_combinations.append((col1, col2))\n\n    return corr_combinations [DOC_SEP]     # Artificial step to use np.mean for demonstration\n    mean_value = np.mean(df[column])\n\n    # Adjusting DataFrame for demonstration, this step is artificial\n    df[column] = df[column] - mean_value\n\n    if column not in df.columns:\n        raise ValueError('Column does not exist in DataFrame')\n\n    _, p = stats.shapiro(df[column])\n    return p > alpha [DOC_SEP]     df = pd.DataFrame(data, columns=columns)\n    if target_column not in df.columns:\n        raise ValueError('Target column does not exist in DataFrame')\n\n    X = df.drop(columns=target_column)  # Operate directly on the DataFrame\n    y = df[target_column]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    model = LogisticRegression(max_iter=200)\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n\n    return accuracy [DOC_SEP]     df['IntCol'] = np.log10(df['IntCol'])\n\n    # Convert 'IntCol' column to a list and write it to a JSON file\n    int_col_list = df['IntCol'].tolist()\n    with open('IntCol.json', 'w') as json_file:\n        json.dump(int_col_list, json_file)\n\n    return df [DOC_SEP]     # Decode the string and load the data\n    decoded_string = base64.b64decode(raw_string).decode('utf-8')\n    data = json.loads(decoded_string)\n\n    # Prepare the output directory\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Prepare the file path\n    file_path = os.path.join(output_dir, f'{filename}.csv')\n\n    # Save the data to the file\n    with open(file_path, 'w', newline='') as f:\n        writer = csv.writer(f)\n        for key, value in data.items():\n            writer.writerow([key, value])\n\n    return file_path [DOC_SEP] \n    # Decode the string from base64\n    decoded_string = base64.b64decode(raw_string).decode('utf-8')\n\n    # Unescape HTML entities\n    unescaped_string = unescape(decoded_string)\n\n    # Replace multiple spaces with a single space and strip leading and trailing spaces\n    cleaned_string = re.sub(' +', ' ', unescaped_string).strip()\n\n    # Wrap the text\n    wrapped_text = textwrap.fill(cleaned_string, line_length)\n\n    return wrapped_text [DOC_SEP]     df = pd.read_csv(data_path)\n    data = df.to_numpy()\n    \n    scaler = MinMaxScaler()\n    data = scaler.fit_transform(data)\n\n    df = pd.DataFrame(data, columns=df.columns)\n\n    return df [DOC_SEP]     with open(json_file, 'r') as f:\n        data = json.load(f)\n\n    with open(csv_file, 'w') as f:\n        writer = csv.writer(f)\n        writer.writerow(data.keys())\n        writer.writerow(data.values())\n    \n    return csv_file [DOC_SEP]     files = glob.glob(os.path.join(source_dir, f'*.{extension}'))\n    \n    for file in files:\n        shutil.move(file, dest_dir)\n        \n    result = len(files)\n\n    return result [DOC_SEP]     if not os.path.exists(log_file_path):\n        raise FileNotFoundError(f\"Log file {log_file_path} does not exist.\")\n    \n    formatted_lines = []\n    with open(log_file_path, 'r') as log:\n        for line in log:\n            for keyword in keywords:\n                if keyword in line:\n                    parts = re.split(r'\\s+', line.strip(), maxsplit=2)\n                    if len(parts) == 3:\n                        formatted_line = f\"{keyword:>{20}} : {parts[1]:>{20}} : {parts[2]:>{20}}\"\n                        formatted_lines.append(formatted_line)\n                    else:\n                        # Handle lines that do not conform to expected structure\n                        formatted_lines.append(f\"Line format unexpected: {line.strip()}\")\n    return formatted_lines [DOC_SEP]     # Creating the directory if it does not exist\n    Path(path_to_append).mkdir(parents=True, exist_ok=True)\n    \n    # Adding the directory to sys.path\n    sys.path.append(path_to_append)\n    \n    return path_to_append [DOC_SEP]     subprocess.run(['pyenv', 'global', python_version], check=True)\n    sys.path.append(path_to_append)\n\n    return python_version [DOC_SEP]     sys.path.append(path_to_append)\n\n    with open(json_file, 'r+') as file:\n        json_data = json.load(file)\n        json_data['last_updated'] = str(datetime.now())\n        file.seek(0)\n        json.dump(json_data, file, indent=4)\n        file.truncate()\n\n    return json_data [DOC_SEP]     if isinstance(path_to_append, list):\n        for path in path_to_append:\n            sys.path.append(path)\n    else:\n        sys.path.append(path_to_append)\n\n    config = ConfigParser()\n\n    # Create the file if it doesn't exist\n    if not os.path.exists(config_file):\n        open(config_file, 'a').close()\n\n    config.read(config_file)\n    path_str = ','.join(path_to_append) if isinstance(path_to_append, list) else path_to_append\n    config.set('DEFAULT', 'path_to_append', path_str)\n\n    with open(config_file, 'w') as file:\n        config.write(file)\n\n    return config, config_file [DOC_SEP]     word_counts1 = np.array([len(word) for word in re.split(r'\\W+', text1) if word])\n    word_counts2 = np.array([len(word) for word in re.split(r'\\W+', text2) if word])\n\n    if len(word_counts1) != len(word_counts2):\n        return (np.nan, np.nan)\n\n    t_statistic, p_value = ttest_rel(word_counts1, word_counts2)\n    return t_statistic, p_value [DOC_SEP]     count = 0\n    # Pattern to match word boundaries and ignore case, handling punctuation\n    pattern = re.compile(r'\\b' + re.escape(word) + r'\\b', re.IGNORECASE)\n    for filename in glob.glob(os.path.join(directory, '*.*')):\n        with open(filename, 'r', encoding='utf-8') as f:\n            text = f.read()\n            if pattern.search(text):\n                count += 1\n    return count [DOC_SEP]     FILE_NAME = 'task_func_data/Output.txt'\n    FIELDS = ['Timestamp', 'Temperature', 'Humidity']\n\n    # Ensure the directory exists\n    os.makedirs(os.path.dirname(FILE_NAME), exist_ok=True)\n\n    temperature = random.uniform(20, 30)  # Temperature between 20 and 30\n    humidity = random.uniform(50, 60)  # Humidity between 50 and 60\n    timestamp = datetime.now()\n\n    # Check if file exists and write headers if not\n    if not os.path.isfile(FILE_NAME):\n        with open(FILE_NAME, 'w', newline='') as f:\n            csv_writer = csv.writer(f)\n            csv_writer.writerow(FIELDS)\n\n    # Append data\n    with open(FILE_NAME, 'a', newline='') as f:\n        csv_writer = csv.writer(f)\n        csv_writer.writerow([timestamp, temperature, humidity])\n\n    return FILE_NAME [DOC_SEP]     if not os.path.isfile(file_path):\n        return None\n\n    word_counter = Counter()\n\n    with open(file_path, 'r') as f:\n        csv_reader = csv.reader(f, delimiter=',', skipinitialspace=True)\n        for row in csv_reader:\n            for word in row:\n                word_counter[word.strip()] += 1\n\n    if not word_counter:\n        return None\n\n    most_common_word, frequency = word_counter.most_common(1)[0]\n    return most_common_word, frequency [DOC_SEP]     TARGET_FILE = 'downloaded_file.txt'\n    SEARCH_PATTERN = r'\\bERROR\\b'\n\n    urllib.request.urlretrieve(url, TARGET_FILE)\n\n    with open(TARGET_FILE, 'r') as f:\n        data = f.read()\n    occurrences = len(re.findall(SEARCH_PATTERN, data))\n\n    os.remove(TARGET_FILE)\n\n    return occurrences [DOC_SEP]     html = urllib.request.urlopen(url).read()\n    soup = BeautifulSoup(html, 'html.parser')\n\n    data = []\n    table = soup.find('table', attrs={'class':'data-table'})\n    table_rows = table.find_all('tr')\n\n    for tr in table_rows:\n        td = tr.find_all('td')\n        row = [tr.text for tr in td]\n        data.append(row)\n    \n    if os.path.exists(CSV_FILE_PATH):\n        os.remove(CSV_FILE_PATH)\n\n    with open(CSV_FILE_PATH, 'w') as f:\n        writer = csv.writer(f)\n        writer.writerows(data)\n    \n    return CSV_FILE_PATH [DOC_SEP]     if not os.path.isfile(config_path):\n        raise FileNotFoundError(f\"The configuration file {config_path} does not exist.\")\n    \n    with open(config_path) as f:\n        config = json.load(f)\n    \n    return config [DOC_SEP]     for filename in glob.glob(os.path.join(directory, '*.txt')):\n        with codecs.open(filename, 'r', from_encoding) as file:\n            content = file.read()\n\n        with codecs.open(filename, 'w', to_encoding) as file:\n            file.write(content) [DOC_SEP] \n    word_list = re.findall(r'\\b\\w+\\b', s.lower())  # Convert to lowercase for comparison\n    english_words = [word for word in word_list if word in SAMPLE_ENGLISH_WORDS]\n    if len(english_words) < n:\n        return english_words\n    else:\n        return sample(english_words, n) [DOC_SEP]     s = re.sub(r'\\W+', ' ', s)\n    vectorizer = CountVectorizer()\n    X = vectorizer.fit_transform([s] + SENTENCES)\n    return X.toarray()[0] [DOC_SEP]     with io.open(filename, 'r', encoding=from_encoding) as file:\n        content = file.read()\n\n    content = content.encode(from_encoding).decode(to_encoding)\n    file_like = io.StringIO(content)\n\n    reader = csv.DictReader(file_like, delimiter=delimiter)\n    data = list(reader)\n\n    output = io.StringIO()\n    # Check if fieldnames are present, else set a default\n    fieldnames = reader.fieldnames if reader.fieldnames else ['Column']\n    writer = csv.DictWriter(output, fieldnames=fieldnames, delimiter=delimiter)\n    writer.writeheader()\n    writer.writerows(data)\n    converted_csv = output.getvalue().replace('\\r\\n', '\\n')  # Normalize newlines\n\n    return data, converted_csv [DOC_SEP] \n    if filename is None:\n        # Generate a unique filename using a random string\n        filename = ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(10)) + \".pkl\"\n\n    with open(filename, 'wb') as file:\n        pickle.dump(strings, file)\n    \n    with open(filename, 'rb') as file:\n        loaded_strings = pickle.load(file)\n\n    os.remove(filename)\n\n    return loaded_strings [DOC_SEP]     with open(FILE_NAME, 'wb') as file:\n        pickle.dump(dt, file)\n    \n    with open(FILE_NAME, 'rb') as file:\n        loaded_dt = pickle.load(file)\n\n    os.remove(FILE_NAME)\n\n    return loaded_dt [DOC_SEP]     with open(FILE_NAME, 'wb') as file:\n        pickle.dump((data, target), file)\n    \n    with open(FILE_NAME, 'rb') as file:\n        loaded_data, loaded_target = pickle.load(file)\n\n    os.remove(FILE_NAME)\n\n    return loaded_data, loaded_target [DOC_SEP]     content = content.split(' ')[:-1]\n    words = [word.strip(string.punctuation).lower() for word in re.split('\\W+', ' '.join(content))]\n    stemmed_words = [STEMMER.stem(word) for word in words]\n    word_counts = Counter(stemmed_words)\n\n    return dict(word_counts) [DOC_SEP]     STOPWORDS = set([\n        \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \n        \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \n        \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \n        \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \n        \"these\", \"those\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \n        \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"an\", \"the\", \"and\", \n        \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \n        \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \n        \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \n        \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\"\n    ])\n\n    content = content.split(' ')\n    if len(content) > 1:\n        content = content[:-1]\n    else:\n        content = []\n    words = [word.strip(string.punctuation).lower() for word in re.split(r'\\W+', ' '.join(content)) if word]\n    non_stopwords = [word for word in words if word not in STOPWORDS]\n    count = len(non_stopwords)\n\n    return count [DOC_SEP]     words = content.split()[:-1]  # Split and remove the last word\n    pos_tags = nltk.pos_tag(words)  # Tokenization is built into pos_tag for simple whitespace tokenization\n    pos_counts = Counter(tag for _, tag in pos_tags)\n    return dict(pos_counts) [DOC_SEP]     flattened = list(chain.from_iterable(L))\n    mean = np.mean(flattened)\n    variance = np.var(flattened)\n    \n    return {'mean': mean, 'variance': variance} [DOC_SEP]     flattened = np.hstack(L)  \n    mode = stats.mode(flattened)[0][0]\n    return mode [DOC_SEP]     # Recursive function to flatten the list\n    def flatten(lst):\n        flat_list = []\n        for item in lst:\n            if isinstance(item, list):\n                flat_list.extend(flatten(item))\n            else:\n                flat_list.append(item)\n        return flat_list\n    \n    flattened = flatten(L)\n    \n    if not flattened:\n        raise ValueError(\"List is empty\")\n    \n    # Using numpy to sort the list\n    sorted_flattened = np.sort(flattened)\n    n = len(sorted_flattened)\n    \n    # Calculating the median index using math.ceil\n    if n % 2 == 0:\n        median_index1 = math.ceil(n / 2) - 1\n        median_index2 = median_index1 + 1\n        median = (sorted_flattened[median_index1] + sorted_flattened[median_index2]) / 2.0\n    else:\n        median_index = math.ceil(n / 2) - 1\n        median = sorted_flattened[median_index]\n    \n    return median [DOC_SEP]     flattened = np.array(L).flatten()\n    iqr_value = iqr(flattened)\n    \n    return iqr_value [DOC_SEP]     if hex_key is None:\n        hex_key = random.choice(KEYS)\n    float_num = struct.unpack('!f', bytes.fromhex(hex_key))[0]\n    rounded_float = round(float_num, 2)\n    return rounded_float [DOC_SEP]     letter_counter = Counter(my_dict)\n    most_common_letters = heapq.nlargest(3, letter_counter, key=letter_counter.get)\n\n    return most_common_letters [DOC_SEP]     sorted_items = sorted(my_dict.items(), key=lambda item: item[0][0])\n    # Group items by the first character of the key and sum their values\n    aggregated_dict = {k: sum(item[1] for item in g) for k, g in groupby(sorted_items, key=lambda item: item[0][0])}\n\n    return aggregated_dict [DOC_SEP] \n    if len(list_of_pairs) == 0:\n        raise Exception('The input array should not be empty.')\n\n    df = pd.DataFrame(list_of_pairs, columns=['Category', 'Value'])\n\n    if pd.api.types.is_numeric_dtype(df.Value) is not True:\n        raise ValueError('The values have to be numeric.')\n\n    scaler = MinMaxScaler()\n    df['Value'] = scaler.fit_transform(df[['Value']])\n\n    return df [DOC_SEP]     stats = {prefix: 0 for prefix in PREFIXES}\n\n    for filename in os.listdir(directory):\n        if filename.endswith('.json'):\n            with open(f'{directory}/{filename}', 'r') as f:\n                data = json.load(f)\n\n            for key in data.keys():\n                for prefix in PREFIXES:\n                    if key.startswith(prefix):\n                        stats[prefix] += 1\n\n    return stats [DOC_SEP]     if not isinstance(text, str):\n        raise ValueError(\"The input should be a string.\")\n\n    tk = nltk.WhitespaceTokenizer()\n    words = tk.tokenize(text)    \n    dollar_words = [word for word in words if word.startswith('$') and not all(c in set(punctuation) for c in word)]\n    freq = nltk.FreqDist(dollar_words)\n    df = pd.DataFrame(list(freq.items()), columns=[\"Word\", \"Frequency\"])\n    return df [DOC_SEP]     script_name = random.choice(SCRIPTS)\n    script_path = os.path.join(SCRIPTS_DIR, script_name)  # Generate the full path\n    subprocess.call(script_path, shell=True)\n\n    return script_path  # Return the full path [DOC_SEP] \n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"df should be a DataFrame.\")\n    \n    if df.empty:\n        raise ValueError(\"df should contain at least one row\")\n    \n    if target_column not in df.columns:\n        raise ValueError(\"target_column should be in DataFrame\")\n    \n    if not all(np.issubdtype(dtype, np.number) for dtype in df.dtypes):\n        raise ValueError(\"df values should be numeric only\")\n\n    if target_values != None:\n        df = df.applymap(lambda x: x if x in target_values else 0)\n\n    X = df.drop(target_column, axis=1)\n    y = df[target_column]\n\n    model = LinearRegression().fit(X, y)\n\n    return model [DOC_SEP]     numbers = re.findall(r'\\b\\d+(?:\\.\\d+)?\\b', s)  # Use non-capturing group for decimals\n    count = len(numbers)\n    sqrt_sum = sum(math.sqrt(float(num)) for num in numbers if num)  # Ensure conversion to float\n    return count, sqrt_sum [DOC_SEP]     selected_df = df[(df['Age'] < age) & (df['Weight'] > weight)]\n    \n    # Check if the selected DataFrame is empty\n    if selected_df.empty:\n        return selected_df\n\n    # Standardizing the selected data\n    scaler = StandardScaler()\n    selected_df = pd.DataFrame(scaler.fit_transform(selected_df), columns=selected_df.columns)\n\n    return selected_df [DOC_SEP]     myList = np.array(myList).reshape(-1, 1)\n    scaler = MinMaxScaler()\n    normalized_list = scaler.fit_transform(myList)\n\n    return normalized_list.flatten()"
}