{
  "id": "BigCodeBench_v0_1_0-00000-of-00001_document_22",
  "text": "    if not os.path.exists(DIRECTORY):\n        os.makedirs(DIRECTORY)\n\n    file_paths = []\n    lines = input_string.split(\"\\n\")\n    for line in lines:\n        if line:  # Check if line is not empty\n            line_hash = hashlib.sha256(line.encode()).hexdigest()\n            filename = line_hash[:10] + \".txt\"\n            filepath = os.path.join(DIRECTORY, filename)\n            with open(filepath, \"w\", encoding=\"utf-8\") as file:\n                file.write(line_hash)\n            file_paths.append(filepath)\n\n    return file_paths [DOC_SEP]     if not data_dict:\n        return None, \"The distribution is uniform.\"\n\n    data_counter = collections.Counter(data_dict)\n    counts = list(data_counter.values())\n    avg_count = sum(counts) / len(counts)\n    uniform = all(abs(count - avg_count) <= 1e-5 for count in counts)\n    message = (\n        \"The distribution is uniform.\"\n        if uniform\n        else \"The distribution is not uniform.\"\n    )\n\n    _, ax = plt.subplots()\n    ax.hist(\n        counts,\n        bins=np.linspace(min(counts), max(counts), min(10, len(counts))),\n        rwidth=0.8,\n    )\n    ax.set_xticks(np.arange(len(data_dict)) + 1)\n    ax.set_xticklabels(list(data_dict.keys()))\n    return ax, message [DOC_SEP]     df = pd.read_csv(file_path, header=None, names=[\"Text\"])\n    df[\"Text\"] = df[\"Text\"].str.split(\"\\\\n\").str.join(\" \")\n\n    vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n    try:\n        word_count = vectorizer.fit_transform(df[\"Text\"])\n    except ValueError:\n        # Handle the case where the DataFrame is empty or contains only stop words\n        print(\"No valid words to plot. Returning None.\")\n        return None\n\n    sum_words = word_count.sum(axis=0)\n    words_freq = [\n        (word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()\n    ]\n    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n\n    top_words = words_freq[:10]\n    df = pd.DataFrame(top_words, columns=[\"Word\", \"Count\"])\n\n    ax = df.plot.bar(x=\"Word\", y=\"Count\", rot=0)\n\n    # Saving or displaying the plot\n    if save_path:\n        plt.savefig(save_path)\n        plt.close()\n        return None\n    else:\n        return ax [DOC_SEP]     try:\n        # Reading the CSV file into a DataFrame\n        df = pd.read_csv(file_path, usecols=[0], names=[\"Text\"], header=None)\n\n        # Vectorizing the text\n        vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n        word_count = vectorizer.fit_transform(df[\"Text\"].dropna())\n\n        # Calculating word frequency\n        sum_words = word_count.sum(axis=0)\n        words_freq = [\n            (word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()\n        ]\n        words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n\n        # Preparing data for the top 10 words\n        top_words = words_freq[:10]\n        df_top = pd.DataFrame(top_words, columns=[\"Word\", \"Count\"])\n\n        # Plotting\n        ax = df_top.plot.bar(x=\"Word\", y=\"Count\", rot=0, legend=False)\n\n        # Saving or displaying the plot\n        if save_path:\n            plt.savefig(save_path)\n            plt.close()\n\n        return None if save_path else ax\n\n    except FileNotFoundError as exc:\n        raise FileNotFoundError(f\"File not found: {file_path}\") from exc\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None [DOC_SEP]     try:\n        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n            reader = csv.reader(file)\n            population = [int(row[0]) for row in reader]\n    except IOError as exc:\n        raise IOError(\n            \"Error reading the file. Please check the file path and permissions.\"\n        ) from exc\n\n    sample = np.random.choice(population, 30, replace=False)\n    mean = np.mean(sample)\n    std_dev = np.std(sample, ddof=1)\n\n    plt.hist(sample, bins=\"auto\", density=True, alpha=0.7, rwidth=0.85)\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mean, std_dev)\n    plt.plot(x, p, \"k\", linewidth=2)\n    plt.xlabel(\"Sample Values\")\n    plt.ylabel(\"Frequency\")\n    plt.title(\"Sample Histogram with Normal Distribution Overlay\")\n    ax = plt.gca()\n\n    return mean, std_dev, ax [DOC_SEP]     combinations = list(itertools.product(colors, states))\n    random.seed(42)\n    random.shuffle(combinations)\n    num_columns = min(len(colors), len(states))\n\n    data = {\n        f\"Color:State {i+1}\": [\n            f\"{comb[0]}:{comb[1]}\" for comb in combinations[i::num_columns]\n        ]\n        for i in range(num_columns)\n    }\n    df = pd.DataFrame(data)\n\n    return df [DOC_SEP]     if n_pairs > 26 or n_pairs < 1:\n        raise ValueError(\"n_pairs should be between 1 and 26\")\n\n    pairs = [f\"{letter}:{number}\" for letter, number in zip(LETTERS, NUMBERS)][:n_pairs]\n    random.seed(42)\n    random.shuffle(pairs)\n    counts = np.random.randint(1, 10, size=n_pairs)\n\n    bars = plt.bar(pairs, counts)\n\n    # Set label for each bar\n    for bar, pair in zip(bars, pairs):\n        bar.set_label(pair)\n\n    plt.xlabel(\"Letter:Number Pairs\")\n    plt.ylabel(\"Counts\")\n    plt.title(\"Random Letter:Number Pairs Chart\")\n\n    return bars [DOC_SEP] \n    # Default lists if not provided\n    if animals is None:\n        animals = [\n            \"Dog\",\n            \"Cat\",\n            \"Elephant\",\n            \"Tiger\",\n            \"Lion\",\n            \"Zebra\",\n            \"Giraffe\",\n            \"Bear\",\n            \"Monkey\",\n            \"Kangaroo\",\n        ]\n    if foods is None:\n        foods = [\"Meat\", \"Fish\", \"Grass\", \"Fruits\", \"Insects\", \"Seeds\", \"Leaves\"]\n\n    # Handling edge case of empty lists\n    if not animals or not foods:\n        return pd.DataFrame()\n\n    pairs = [f\"{a}:{f}\" for a, f in itertools.product(animals, foods)]\n\n    # Reshape the data and create a DataFrame\n    data = np.array(pairs).reshape(-1, len(foods))\n    df = pd.DataFrame(data, columns=foods)\n\n    return df [DOC_SEP]     max_pairs = len(SHAPES) * len(COLORS)\n    num_pairs = min(num_pairs, max_pairs)\n    \n    pairs = [f\"{s}:{c}\" for s, c in itertools.product(SHAPES, COLORS)][:num_pairs]\n    \n    # Drawing the countplot\n    ax = sns.countplot(x=pairs, hue=pairs, palette=\"Set3\", legend=False)\n    plt.xticks(rotation=90)\n    \n    return ax [DOC_SEP]     # Generate all possible pairs\n    pairs = [\n        f\"{planet}:{element}\"\n        for planet, element in itertools.product(PLANETS, ELEMENTS)\n    ]\n    # Shuffle the pairs to ensure randomness\n    random.shuffle(pairs)\n\n    # Convert the list of pairs into a numpy array, then reshape it to fit the DataFrame dimensions\n    data = np.array(pairs).reshape(len(PLANETS), len(ELEMENTS))\n    # Create the DataFrame with ELEMENTS as column headers\n    df = pd.DataFrame(data, columns=ELEMENTS)\n\n    return df [DOC_SEP]     if df.empty or column_name not in df.columns or df[column_name].isnull().all():\n        message = \"The DataFrame is empty or the specified column has no data.\"\n        _, ax = plt.subplots()\n        ax.set_title(f\"Distribution of values in {column_name} (No Data)\")\n        return message, ax\n\n    unique_values_count = df[column_name].nunique()\n    total_values = len(df[column_name])\n    is_uniform = total_values % unique_values_count == 0 and all(\n        df[column_name].value_counts() == total_values / unique_values_count\n    )\n\n    message = (\n        \"The distribution of values is uniform.\"\n        if is_uniform\n        else \"The distribution of values is not uniform.\"\n    )\n\n    _, ax = plt.subplots()\n    ax.hist(df[column_name], bins=unique_values_count, edgecolor=\"black\", alpha=0.7)\n    ax.set_xticks(range(unique_values_count))\n    ax.set_xlabel(\"Values\")\n    ax.set_ylabel(\"Frequency\")\n    ax.set_title(f\"Distribution of values in {column_name}\")\n\n    return message, ax [DOC_SEP]     # Calculating row sums\n    row_sums = arr.sum(axis=1)\n\n    # Normalizing the data\n    mean = np.mean(row_sums)\n    std_dev = np.std(row_sums)\n    normalized_data = (\n        (row_sums - mean) / std_dev if std_dev != 0 else np.zeros_like(row_sums)\n    )\n\n    # Plotting the histogram\n    _, ax = plt.subplots()\n    ax.hist(normalized_data, bins=30, density=True, alpha=0.6, color=\"g\")\n\n    # Plotting the PDF of a standard normal distribution\n    x = np.linspace(norm.ppf(0.01), norm.ppf(0.99), 100)\n    ax.plot(x, norm.pdf(x), \"r-\", lw=2)\n    ax.set_title(\"Histogram of Normalized Data with Standard Normal PDF\")\n\n    return ax, normalized_data [DOC_SEP]     if not arr.size:  # Check for empty array\n        _, ax = plt.subplots()\n        ax.set_title(\"Time Series of Row Sums\")\n        return ax\n\n    row_sums = arr.sum(axis=1)\n    df = pd.DataFrame(row_sums, columns=[\"Sum\"])\n    df.index = pd.date_range(start=\"1/1/2020\", periods=df.shape[0])\n    ax = df.plot(title=\"Time Series of Row Sums\")\n    return ax [DOC_SEP]     row_sums = arr.sum(axis=1)\n    pca = PCA(n_components=1)\n    pca.fit(row_sums.reshape(-1, 1))\n\n    # Plotting (requires matplotlib and sklearn)\n\n    _, ax = plt.subplots()\n    ax.bar([0], pca.explained_variance_ratio_)\n    ax.set_title(\"Explained Variance Ratio of Principal Components\")\n    ax.set_xticks([0])\n    ax.set_xticklabels([\"PC1\"])\n\n    return ax [DOC_SEP]     row_sums = arr.sum(axis=1)\n    vmax = np.max(arr)  # Set vmax to the maximum value in the array\n    vmin = np.min(arr)  # Set vmin to the minimum value in the array\n    ax = sns.heatmap(\n        arr, annot=True, vmax=vmax, vmin=vmin\n    )  # Include both vmin and vmax in the heatmap call\n    ax.set_title(\"Heatmap of the 2D Array\")\n\n    return ax [DOC_SEP]     row_sums = arr.sum(axis=1)\n    fft_coefficients = fftpack.fft(row_sums)\n\n    _, ax = plt.subplots()\n    ax.plot(np.abs(fft_coefficients))\n    ax.set_title(\"Absolute values of FFT coefficients\")\n\n    return ax [DOC_SEP]     normal_data = np.random.normal(size=num_samples)\n    outliers = np.random.uniform(low=-10, high=10, size=num_outliers)\n    data = np.concatenate([normal_data, outliers]) if num_samples > 0 else outliers\n\n    # Identify outliers using IQR (only if there is normal data)\n    outliers_detected = np.array([])\n    if num_samples > 0:\n        q75, q25 = np.percentile(normal_data, [75, 25])\n        iqr = q75 - q25\n        lower_bound = q25 - (iqr * 1.5)\n        upper_bound = q75 + (iqr * 1.5)\n        outliers_detected = data[(data < lower_bound) | (data > upper_bound)]\n\n    # Plot histogram\n    _, ax = plt.subplots()\n    ax.hist(data, bins=30)\n\n    return data, outliers_detected, ax [DOC_SEP]     try:\n        response = requests.get(repo_url, timeout=2)\n        response.raise_for_status()  # Raises HTTPError for bad requests\n        repo_info = response.json()\n        if (\n            response.status_code == 403\n            and repo_info.get(\"message\") == \"API rate limit exceeded\"\n        ):\n            raise requests.exceptions.HTTPError(\"API rate limit exceeded\")\n\n        if repo_info.get(\"open_issues_count\", 0) > 10000:\n            logging.warning(\"The repository has more than 10000 open issues.\")\n\n        return repo_info\n\n    except requests.exceptions.RequestException as e:\n        raise requests.exceptions.RequestException(\n            f\"Error fetching repo info: {e}\"\n        ) from e [DOC_SEP]     if warn_large_dataset:\n        warnings.simplefilter(\"always\")\n\n    try:\n        with sqlite3.connect(db_path) as conn:\n            data = pd.read_sql_query(query, conn)\n\n        if warn_large_dataset and data.shape[0] > 10000:\n            warnings.warn(\"The data contains more than 10000 rows.\")\n\n        return data\n\n    except Exception as e:\n        raise Exception(f\"Error fetching data from the database: {str(e)}\") from e [DOC_SEP]     df = pd.DataFrame(data_dict)\n    axes_list = []\n    for column in df.columns:\n        counts = df[column].value_counts()\n        uniform = (\n            len(set(counts)) == 1\n        )  # Check if all counts are the same (uniform distribution)\n\n        if not uniform:\n            print(f\"The distribution of values in column '{column}' is not uniform.\")\n\n        ax = counts.plot(kind=\"bar\")\n        ax.set_title(column)\n        axes_list.append(ax)\n        plt.close()\n\n    return axes_list [DOC_SEP]     dataframes = []\n\n    for list_ in list_of_lists:\n        df_dict = {col: POSSIBLE_VALUES.copy() for col in list_}\n        for col in df_dict:\n            shuffle(df_dict[col])\n        df = pd.DataFrame(df_dict)\n        dataframes.append(df)\n\n    return dataframes [DOC_SEP]     fig, ax = plt.subplots()\n    color_cycle = cycle(COLORS)\n\n    for list_ in list_of_lists:\n        y_values = np.arange(1, len(list_) + 1)\n        shuffle(y_values)\n        ax.plot(y_values, next(color_cycle))\n\n    return fig, ax [DOC_SEP]     series_list = []\n    for sublist in list_of_lists:\n        values = np.arange(1, len(sublist) + 1)\n        np.random.shuffle(values)\n        s = pd.Series(values, index=sublist)\n        series_list.append(s)\n\n    return series_list [DOC_SEP]     try:\n        seconds = [time.strptime(ts, time_format).tm_sec for ts in time_strings]\n        _, ax = plt.subplots()\n        ax.hist(seconds, bins=60, rwidth=0.8)\n        return ax\n    except ValueError as e:\n        print(f\"Error parsing time strings: {e}\")\n        return None [DOC_SEP]     from_zone = pytz.timezone(from_tz)\n    to_zone = pytz.timezone(to_tz)\n    dt = parse(time_string, dayfirst=True)\n    dt = from_zone.localize(dt)\n    dt = dt.astimezone(to_zone)\n\n    return dt.strftime(TIME_FORMAT) [DOC_SEP]     # Calculate time differences\n    differences = (\n        np.diff([datetime.datetime.strptime(t, TIME_FORMAT) for t in time_strings])\n        .astype(\"timedelta64[s]\")\n        .astype(int)\n    )\n\n    # Plotting the bar chart\n    _ = plt.bar(range(len(differences)), differences)\n    plt.xlabel(\"Index\")\n    plt.ylabel(\"Time Difference (seconds)\")\n    plt.title(\"Time Differences Between Consecutive Timestamps\")\n    return plt.gca() [DOC_SEP]     data = []\n\n    for time_string in time_strings:\n        utc_time = datetime.strptime(time_string, TIME_FORMAT)\n        converted_time = utc_time.replace(tzinfo=ZoneInfo(\"UTC\")).astimezone(\n            ZoneInfo(target_tz)\n        )\n        data.append([time_string, converted_time.strftime(TIME_FORMAT)])\n\n    df = pd.DataFrame(data, columns=[\"Original Time\", \"Converted Time\"])\n    return df [DOC_SEP]     if len(time_strings) < 2:\n        return 0.0\n\n    time_zone = pytz.timezone(timezone)\n    parsed_times = [\n        datetime.strptime(ts, \"%d/%m/%y %H:%M:%S.%f\")\n        .replace(tzinfo=pytz.UTC)\n        .astimezone(time_zone)\n        for ts in time_strings\n    ]\n\n    differences = [\n        abs((t2 - t1).total_seconds()) for t1, t2 in zip(parsed_times, parsed_times[1:])\n    ]\n\n    return np.mean(differences) if differences else 0.0 [DOC_SEP]     unique, counts = np.unique(arr, return_counts=True)\n    uniform_distribution = len(set(counts)) == 1\n\n    _, ax = plt.subplots()\n    ax.hist(arr, bins=np.arange(len(unique) + 1) - 0.5, rwidth=0.8, align=\"mid\")\n    ax.set_xticks(range(len(unique)))\n    ax.set_xticklabels(unique)\n\n    return uniform_distribution, ax [DOC_SEP]     df = pd.DataFrame(data)\n    # Correctly convert string prices to float, accounting for commas\n    df[\"Price_Float\"] = df[\"Price_String\"].apply(lambda x: float(x.replace(\",\", \"\")))\n\n    mean_price = np.mean(df[\"Price_Float\"])\n    median_price = np.median(df[\"Price_Float\"])\n    # Use ddof=1 for sample standard deviation\n    std_dev_price = np.std(df[\"Price_Float\"], ddof=1)\n\n    # Histogram plot settings can be refined for better visualization\n    ax = plt.hist(df[\"Price_Float\"], bins=\"auto\", color=\"blue\", alpha=0.7, rwidth=0.85)\n    plt.title(\"Histogram of Product Prices\")\n    plt.xlabel(\"Price\")\n    plt.ylabel(\"Frequency\")\n\n    return {\"mean\": mean_price, \"median\": median_price, \"std_dev\": std_dev_price}, ax [DOC_SEP]     # Convert area strings to float and prepare data for the model\n    df = pd.DataFrame(data)\n    df[\"Area_Float\"] = df[\"Area_String\"].str.replace(\",\", \"\").astype(float)\n\n    # Train the linear regression model\n    X = df[[\"Area_Float\"]]\n    Y = df[\"Price\"]\n    model = LinearRegression()\n    model.fit(X, Y)\n\n    # Predict the price for the given area string\n    area_float = float(area_string.replace(\",\", \"\"))\n    prediction_data = pd.DataFrame([area_float], columns=[\"Area_Float\"])\n    price_predicted = model.predict(prediction_data)\n\n    return price_predicted[0] [DOC_SEP]     if data is None:\n        data = {\n            \"Weight_String\": [\"60.5\", \"65.7\", \"70.2\", \"75.9\", \"80.1\"],\n            \"Height\": [160, 165, 170, 175, 180],\n        }\n\n    df = pd.DataFrame(data)\n\n    # Validate weight values are strings\n    if not all(isinstance(weight, str) for weight in df[\"Weight_String\"]):\n        raise ValueError(\"Weights must be provided as strings.\")\n\n    # Convert string weights to floats\n    df[\"Weight_Float\"] = df[\"Weight_String\"].astype(float)\n\n    # Plotting the scatter plot\n    ax = sns.scatterplot(data=df, x=\"Weight_Float\", y=\"Height\")\n    ax.set_title(\"Weight vs Height\")\n    return ax [DOC_SEP]     df = pd.DataFrame(data)\n    if len(df) < 2:  # Check if the data frame has less than 2 rows\n        return float(\"nan\")  # or return None\n\n    df[\"Score_Float\"] = df[\"Score_String\"].astype(float)\n    df[\"Grade_Encoded\"] = df[\"Grade\"].astype(\"category\").cat.codes\n    correlation = pearsonr(df[\"Score_Float\"], df[\"Grade_Encoded\"])[0]\n    return correlation [DOC_SEP]     # Validate input data\n    if not all(key in data for key in [\"Salary_String\", \"Experience\"]):\n        raise ValueError(\n            \"Input data must contain 'Salary_String' and 'Experience' keys.\"\n        )\n\n    # Convert data to DataFrame\n    df = pd.DataFrame(data)\n\n    # Check if the data is empty\n    if df.empty:\n        # Handle empty data case (e.g., return a default Axes instance or raise an error)\n        _, ax = plt.subplots()\n        ax.set_title(\"Normalized Salary vs Experience\")\n        ax.set_xlabel(\"Experience\")\n        ax.set_ylabel(\"Normalized Salary\")\n        return ax\n\n    # Convert Salary_String to float and handle potential conversion errors\n    try:\n        df[\"Salary_Float\"] = df[\"Salary_String\"].str.replace(\",\", \"\").astype(float)\n    except ValueError:\n        raise ValueError(\"Error converting Salary_String to float.\")\n\n    # Normalize the Salary_Float values\n    scaler = MinMaxScaler()\n    df[\"Normalized_Salary\"] = scaler.fit_transform(df[[\"Salary_Float\"]])\n\n    # Plot the data\n    _, ax = plt.subplots()\n    ax.scatter(df[\"Experience\"], df[\"Normalized_Salary\"])\n    ax.set_title(\"Normalized Salary vs Experience\")\n    ax.set_xlabel(\"Experience\")\n    ax.set_ylabel(\"Normalized Salary\")\n\n    return ax [DOC_SEP]     df = pd.read_csv(data_file_path)\n    # Convert strings with commas to float, if applicable\n    for col in df.columns:\n        df[col] = pd.to_numeric(df[col].replace(\",\", \"\", regex=True), errors=\"coerce\")\n    # drop columns with NaN values\n    df = df.dropna(axis=1)\n    means = df.mean()\n    std_devs = df.std()\n\n    # Creating a histogram for each numerical column\n    axes = []\n    for col in df.columns:\n        ax = df[col].hist(bins=50)\n        ax.set_title(col)\n        axes.append(ax)\n\n    plt.show()\n\n    # ANOVA Test if more than one numerical column\n    anova_results = None\n    if len(df.columns) > 1:\n        anova_results = pd.DataFrame(f_oneway(*[df[col] for col in df.columns if df[col].dtype != 'object']),\n                                     index=['F-value', 'P-value'], \n                                     columns=['ANOVA Results'])\n\n    return means, std_devs, axes, anova_results [DOC_SEP]     # Process text and count words\n    cleaned_text = re.sub(f\"[{punctuation}]\", \"\", text).lower()\n    words = cleaned_text.split()\n    word_counts = Counter(words)\n    most_common_words = word_counts.most_common(10)\n\n    # Plotting\n    _, ax = plt.subplots()\n    if most_common_words:  # Check if the list is not empty\n        ax.bar(*zip(*most_common_words))\n    else:  # Handle empty case\n        ax.bar([], [])\n\n    return most_common_words, ax [DOC_SEP]     data = {\n        \"String Field\": [\n            \"\".join(random.choices(string.ascii_letters, k=10))\n            for _ in range(NUM_SAMPLES)\n        ],\n        \"Float Field\": [f\"{x:,.2f}\" for x in np.random.uniform(0, 10000, NUM_SAMPLES)],\n    }\n\n    df = pd.DataFrame(data)\n\n    return df [DOC_SEP]     sample = np.random.normal(mean, std_dev, 1000)\n    plot_paths = []\n\n    # Plotting histogram\n    plt.figure()\n    plt.hist(sample, bins=50)\n    if save_plots:\n        hist_path = \"histogram_plot.png\"\n        plt.savefig(hist_path)\n        plt.close()\n        plot_paths.append(hist_path)\n\n    # Plotting QQ diagram\n    plt.figure()\n    stats.probplot(sample, plot=plt)\n    if save_plots:\n        qq_path = \"qq_plot.png\"\n        plt.savefig(qq_path)\n        plt.close()\n        plot_paths.append(qq_path)\n\n    skewness = stats.skew(sample)\n    kurtosis = stats.kurtosis(sample)\n\n    return skewness, kurtosis, plot_paths [DOC_SEP]     if data is None:\n        data = np.random.rand(100, 5)\n\n    df = pd.DataFrame(data)\n    df[df < 0.5] = 0\n\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(df)\n    standardized_df = pd.DataFrame(scaled_data, columns=df.columns)\n\n    return standardized_df [DOC_SEP] \n    numeric_values = [pair[0] for pair in list_of_tuples]\n    categories = [pair[1] for pair in list_of_tuples]\n\n    total_sum = np.sum(numeric_values)\n    category_counts = Counter(categories)\n\n    return total_sum, dict(category_counts) [DOC_SEP] \n    data = json.load(file_pointer)\n    key_frequency_counter = Counter()\n\n    for item in data:\n        if isinstance(item, str):\n            try:\n                item = ast.literal_eval(item)\n            except ValueError:\n                continue\n\n        if isinstance(item, dict):\n            key_frequency_counter.update(item.keys())\n\n    return key_frequency_counter [DOC_SEP]     path = os.path.join(directory, '*.txt')\n    files = glob.glob(path)\n\n    results = []\n    for file in files:\n        with open(file, 'r') as f:\n            for line in f:\n                results.append(ast.literal_eval(line.strip()))\n\n    return results [DOC_SEP]     try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.RequestException:\n        return []\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    results = []\n    for script in soup.find_all('script'):\n        try:\n            results.append(ast.literal_eval(script.string))\n        except (ValueError, SyntaxError):\n            continue\n\n    return results [DOC_SEP]     with open(text_file, 'r') as file:\n        text = file.read()\n\n    # Updated regex pattern to handle nested dictionaries more robustly\n    pattern = re.compile(r\"\\{[^{}]*\\{[^{}]*\\}[^{}]*\\}|\\{[^{}]*\\}\")\n    matches = pattern.findall(text)\n\n    results = [ast.literal_eval(match) for match in matches]\n\n    return results [DOC_SEP] \n    tokenizer = RegexpTokenizer(r'\\$\\$+\\w*|\\$\\w+')\n    dollar_prefixed_words = tokenizer.tokenize(text)\n    normalized_words = [word.lstrip(\"$\") if len(word.lstrip(\"$\")) > 0 else word for word in dollar_prefixed_words]\n    word_counts = Counter(normalized_words)\n    return word_counts.most_common(5) [DOC_SEP] \n    punctuation_set = set(punctuation)\n    tokenizer = RegexpTokenizer(r'\\$\\w+')\n    dollar_prefixed_words = tokenizer.tokenize(text)\n    valid_dollar_words = [word for word in dollar_prefixed_words if\n                          not all(char in punctuation_set for char in word[1:])]\n\n    with open(output_filename, 'w') as file:\n        for word in valid_dollar_words:\n            file.write(word + '\\n')\n\n    return os.path.abspath(output_filename) [DOC_SEP]     \n\n    punctuation_set = set(punctuation)\n    tokenizer = RegexpTokenizer(r'\\$\\w+')\n    dollar_prefixed_words = tokenizer.tokenize(text)\n    dollar_words = [word for word in dollar_prefixed_words if\n                          not all(char in punctuation_set for char in word[1:])]\n\n    with open(filename, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow([\"Word\"])\n        for word in dollar_words:\n            writer.writerow([word])\n    return os.path.abspath(filename) [DOC_SEP]     # Constants\n    PUNCTUATION = set(punctuation)\n\n    # Remove URLs\n    text = re.sub('http[s]?://\\S+', '', text)\n\n    # Remove punctuation\n    text = re.sub('[{}]'.format(re.escape(''.join(PUNCTUATION))), '', text)\n\n    # Tokenize the text\n    words = text.split()\n\n    # Remove stopwords\n    cleaned_words = [word for word in words if word.lower() not in PREDEFINED_STOPWORDS]\n\n    return ' '.join(cleaned_words) [DOC_SEP]     # Remove URLs\n    text = re.sub('http[s]?://\\S+', '', text)\n\n    # Tokenize the text using regex (improved tokenization)\n    words = re.findall(r'\\b\\w+\\b', text)\n\n    # Count the frequency of each word\n    word_freq = Counter(words)\n\n    return word_freq.most_common(top_n) [DOC_SEP]     # Remove URLs\n    text = re.sub('http[s]?://\\S+', '', text)\n    # Tokenize the text using regex (improved tokenization)\n    words = re.findall(r'\\b\\w+\\b', text)\n    # Count the frequency of each word\n    word_freq = Counter(words)\n    result = Counter(words)\n    for i in word_freq:\n        if i not in PREDEFINED_STOPWORDS:\n            del result[i]\n    return list(result.items())"
}