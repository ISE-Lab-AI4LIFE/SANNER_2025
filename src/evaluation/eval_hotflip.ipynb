{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7qjaOyI-Ig8",
        "outputId": "0bcd78b4-a8a3-4dbc-80b8-0bc730b0a483"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GitHub username: hieunguyen-cyber\n",
            "GitHub token: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "‚úÖ Repo cloned successfully!\n"
          ]
        }
      ],
      "source": [
        "import getpass\n",
        "import subprocess\n",
        "from urllib.parse import quote\n",
        "\n",
        "username = input(\"GitHub username: \")\n",
        "token = getpass.getpass(\"GitHub token: \")\n",
        "repo_url = \"https://github.com/ISE-Lab-AI4LIFE/SANNER_2025.git\"\n",
        "\n",
        "auth_url = repo_url.replace(\"https://\", f\"https://{quote(username)}:{quote(token)}@\")\n",
        "\n",
        "try:\n",
        "    subprocess.run([\"git\", \"clone\", auth_url], check=True)\n",
        "    print(\"‚úÖ Repo cloned successfully!\")\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(\"‚ùå Clone failed. Check error message below:\")\n",
        "    print(e.stderr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96l3HgHV-F8N",
        "outputId": "4593984a-0517-4e52-de04-757396717331"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "T·ªïng s·ªë document trong pool: 5446\n",
            "S·ªë poisoned_doc: 157\n",
            "S·ªë targeted_doc: 157\n",
            "S·ªë clean_doc: 5289\n",
            "‚úÖ ƒê√£ l∆∞u 3 file v√†o: /content/SANNER_2025/data/hotflip_result/split_docs\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "DATA_DIR = Path(\"/content/SANNER_2025/data\")  # <-- s·ª≠a ·ªü ƒë√¢y\n",
        "HOTFLIP_FILE = DATA_DIR / \"hotflip_result\" / \"merged_hotflip_results.csv\"\n",
        "POOL_FILE = DATA_DIR / \"pool.csv\"\n",
        "\n",
        "# --- ƒê·ªçc d·ªØ li·ªáu ---\n",
        "hotflip_df = pd.read_csv(HOTFLIP_FILE)\n",
        "pool_df = pd.read_csv(POOL_FILE)\n",
        "\n",
        "# --- L·∫•y danh s√°ch id ---\n",
        "hotflip_ids = set(hotflip_df[\"document_id\"])\n",
        "pool_ids = set(pool_df[\"document_id\"])\n",
        "\n",
        "# --- Ph√¢n lo·∫°i ---\n",
        "poisoned_doc = hotflip_df.copy()\n",
        "targeted_doc = pool_df[pool_df[\"document_id\"].isin(hotflip_ids)]\n",
        "clean_doc = pool_df[~pool_df[\"document_id\"].isin(hotflip_ids)]\n",
        "\n",
        "# --- In th·ªëng k√™ ---\n",
        "print(f\"T·ªïng s·ªë document trong pool: {len(pool_df)}\")\n",
        "print(f\"S·ªë poisoned_doc: {len(poisoned_doc)}\")\n",
        "print(f\"S·ªë targeted_doc: {len(targeted_doc)}\")\n",
        "print(f\"S·ªë clean_doc: {len(clean_doc)}\")\n",
        "\n",
        "# --- L∆∞u ra file ---\n",
        "OUTPUT_DIR = DATA_DIR / \"hotflip_result\" / \"split_docs\"\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "poisoned_doc.to_csv(OUTPUT_DIR / \"poisoned_doc.csv\", index=False)\n",
        "targeted_doc.to_csv(OUTPUT_DIR / \"targeted_doc.csv\", index=False)\n",
        "clean_doc.to_csv(OUTPUT_DIR / \"clean_doc.csv\", index=False)\n",
        "\n",
        "print(f\"‚úÖ ƒê√£ l∆∞u 3 file v√†o: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OLi6uLP_emf",
        "outputId": "d783b84c-18f9-4c2c-b8c6-7b160ed43893"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ ratio=0.1%: 5446 poisoned_pool (157 poison, 5289 clean) | 5446 base_pool (157 target, 5289 clean)\n",
            "‚úÖ ratio=0.5%: 5446 poisoned_pool (157 poison, 5289 clean) | 5446 base_pool (157 target, 5289 clean)\n",
            "‚úÖ ratio=1.0%: 5446 poisoned_pool (157 poison, 5289 clean) | 5446 base_pool (157 target, 5289 clean)\n",
            "‚úÖ ratio=1.5%: 5446 poisoned_pool (157 poison, 5289 clean) | 5446 base_pool (157 target, 5289 clean)\n",
            "‚úÖ ratio=2.0%: 5446 poisoned_pool (157 poison, 5289 clean) | 5446 base_pool (157 target, 5289 clean)\n",
            "‚úÖ ratio=2.5%: 5446 poisoned_pool (157 poison, 5289 clean) | 5446 base_pool (157 target, 5289 clean)\n",
            "‚úÖ ratio=3.0%: 5233 poisoned_pool (157 poison, 5076 clean) | 5233 base_pool (157 target, 5076 clean)\n",
            "\n",
            "üéØ Ho√†n t·∫•t t·∫°o pool cho t·∫•t c·∫£ c√°c ratio (0.1% ‚Üí 3%)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "DATA_DIR = Path(\"/content/SANNER_2025/data\")\n",
        "HOTFLIP_DIR = DATA_DIR / \"hotflip_result\"\n",
        "SPLIT_DIR = HOTFLIP_DIR / \"split_docs\"\n",
        "POISONED_POOL_DIR = HOTFLIP_DIR / \"poisoned_pool\"\n",
        "BASE_POOL_DIR = HOTFLIP_DIR / \"base_pool\"\n",
        "\n",
        "POISONED_POOL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "BASE_POOL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- ƒê·ªçc 3 file ---\n",
        "poisoned_doc = pd.read_csv(SPLIT_DIR / \"poisoned_doc.csv\")\n",
        "targeted_doc = pd.read_csv(SPLIT_DIR / \"targeted_doc.csv\")\n",
        "clean_doc = pd.read_csv(SPLIT_DIR / \"clean_doc.csv\")\n",
        "\n",
        "# --- T·∫°o pool v·ªõi nhi·ªÅu ratio ---\n",
        "ratios = np.array([0.001, 0.005, 0.01, 0.015, 0.02, 0.025, 0.03])\n",
        "\n",
        "for ratio in ratios:\n",
        "    # --- S·ªë l∆∞·ª£ng clean c·∫ßn ch·ªçn ---\n",
        "    n_poison = len(poisoned_doc)\n",
        "    n_target = len(targeted_doc)\n",
        "\n",
        "    n_clean_poisoned_pool = int(n_poison * (1 - ratio) / ratio)\n",
        "    n_clean_base_pool = int(n_target * (1 - ratio) / ratio)\n",
        "\n",
        "    # --- Gi·ªõi h·∫°n n·∫øu clean_doc kh√¥ng ƒë·ªß ---\n",
        "    n_clean_poisoned_pool = min(n_clean_poisoned_pool, len(clean_doc))\n",
        "    n_clean_base_pool = min(n_clean_base_pool, len(clean_doc))\n",
        "\n",
        "    # --- Sample clean_doc ---\n",
        "    clean_sample_for_poisoned = clean_doc.sample(\n",
        "        n=n_clean_poisoned_pool,\n",
        "        random_state=42,\n",
        "        replace=False\n",
        "    )\n",
        "    clean_sample_for_base = clean_doc.sample(\n",
        "        n=n_clean_base_pool,\n",
        "        random_state=42,\n",
        "        replace=False\n",
        "    )\n",
        "\n",
        "    # --- G·∫Øn nh√£n chosen ---\n",
        "    poisoned_part = poisoned_doc[[\"document_id\", \"final_poisoned_doc\"]].rename(\n",
        "        columns={\"final_poisoned_doc\": \"document\"}\n",
        "    )\n",
        "    poisoned_part[\"chosen\"] = 1\n",
        "\n",
        "    targeted_part = targeted_doc[[\"document_id\", \"document\"]].copy()\n",
        "    targeted_part[\"chosen\"] = 1\n",
        "\n",
        "    clean_part_for_poisoned = clean_sample_for_poisoned[[\"document_id\", \"document\"]].copy()\n",
        "    clean_part_for_poisoned[\"chosen\"] = 0\n",
        "\n",
        "    clean_part_for_base = clean_sample_for_base[[\"document_id\", \"document\"]].copy()\n",
        "    clean_part_for_base[\"chosen\"] = 0\n",
        "\n",
        "    # --- G·ªôp l·∫°i ---\n",
        "    poisoned_pool = pd.concat([poisoned_part, clean_part_for_poisoned], ignore_index=True)\n",
        "    base_pool = pd.concat([targeted_part, clean_part_for_base], ignore_index=True)\n",
        "\n",
        "    # --- L∆∞u ra CSV ---\n",
        "    r_str = f\"{ratio*100:.1f}\".replace('.', '_')  # v√≠ d·ª• 0_1%, 0_2%, ...\n",
        "    poisoned_pool.to_csv(POISONED_POOL_DIR / f\"ratio_{r_str}_percent.csv\", index=False)\n",
        "    base_pool.to_csv(BASE_POOL_DIR / f\"ratio_{r_str}_percent.csv\", index=False)\n",
        "\n",
        "    print(f\"‚úÖ ratio={ratio*100:.1f}%: \"\n",
        "          f\"{len(poisoned_pool)} poisoned_pool ({len(poisoned_part)} poison, {len(clean_part_for_poisoned)} clean) | \"\n",
        "          f\"{len(base_pool)} base_pool ({len(targeted_part)} target, {len(clean_part_for_base)} clean)\")\n",
        "\n",
        "print(\"\\nüéØ Ho√†n t·∫•t t·∫°o pool cho t·∫•t c·∫£ c√°c ratio (0.1% ‚Üí 3%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80zMEQL7_rGd",
        "outputId": "b80d01a0-7122-4a0c-a1df-7406ff82c5b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîπ Encoding poisoned_doc (157 docs) using column 'final_poisoned_doc'...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Encoding poisoned_doc:   0%|          | 0/157 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (403 > 384). Running this sequence through the model will result in indexing errors\n",
            "Encoding poisoned_doc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:11<00:00, 13.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Done: poisoned_doc | shape = (157, 768)\n",
            "\n",
            "üîπ Encoding targeted_doc (157 docs) using column 'document'...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Encoding targeted_doc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:07<00:00, 21.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Done: targeted_doc | shape = (157, 768)\n",
            "\n",
            "üîπ Encoding clean_doc (5289 docs) using column 'document'...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Encoding clean_doc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5289/5289 [05:18<00:00, 16.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Done: clean_doc | shape = (5289, 768)\n",
            "\n",
            "üéØ Ho√†n t·∫•t t√≠nh v√† l∆∞u embedding cho 3 file g·ªëc (clean / targeted / poisoned)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "DATA_DIR = Path(\"/content/SANNER_2025/data\")\n",
        "HOTFLIP_DIR = DATA_DIR / \"hotflip_result\"\n",
        "SPLIT_DIR = HOTFLIP_DIR / \"split_docs\"\n",
        "EMB_DIR = HOTFLIP_DIR / \"embeddings\"\n",
        "EMB_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- Thi·∫øt b·ªã ---\n",
        "if torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "elif torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\", device=device)\n",
        "\n",
        "# --- Sliding window config ---\n",
        "MAX_TOKENS = 256\n",
        "STRIDE = 128\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "def sliding_window_embed(text, model, max_len=MAX_TOKENS, stride=STRIDE):\n",
        "    \"\"\"C·∫Øt vƒÉn b·∫£n d√†i v√† trung b√¨nh embedding.\"\"\"\n",
        "    tokens = model.tokenizer.tokenize(text)\n",
        "    if len(tokens) <= max_len:\n",
        "        return model.encode([text], convert_to_tensor=True)\n",
        "    else:\n",
        "        chunks = []\n",
        "        for i in range(0, len(tokens), stride):\n",
        "            chunk = tokens[i:i + max_len]\n",
        "            chunk_text = model.tokenizer.convert_tokens_to_string(chunk)\n",
        "            chunks.append(chunk_text)\n",
        "            if i + max_len >= len(tokens):\n",
        "                break\n",
        "        embeddings = model.encode(chunks, convert_to_tensor=True, batch_size=BATCH_SIZE)\n",
        "        return embeddings.mean(dim=0, keepdim=True)\n",
        "\n",
        "# --- Danh s√°ch file v√† c·ªôt t∆∞∆°ng ·ª©ng ---\n",
        "file_configs = {\n",
        "    \"poisoned_doc\": \"final_poisoned_doc\",\n",
        "    \"targeted_doc\": \"document\",\n",
        "    \"clean_doc\": \"document\"\n",
        "}\n",
        "\n",
        "for name, text_col in file_configs.items():\n",
        "    path = SPLIT_DIR / f\"{name}.csv\"\n",
        "    df = pd.read_csv(path)\n",
        "\n",
        "    print(f\"\\nüîπ Encoding {name} ({len(df)} docs) using column '{text_col}'...\")\n",
        "    all_embs = []\n",
        "    for text in tqdm(df[text_col], desc=f\"Encoding {name}\"):\n",
        "        emb = sliding_window_embed(str(text), model)\n",
        "        all_embs.append(emb.cpu().numpy())\n",
        "\n",
        "    all_embs = np.vstack(all_embs)\n",
        "    np.save(EMB_DIR / f\"{name}_emb.npy\", all_embs)\n",
        "\n",
        "    # ch·ªâ gi·ªØ l·∫°i c√°c c·ªôt li√™n quan ƒë·ªÉ truy map theo id\n",
        "    meta_cols = [\"document_id\", text_col]\n",
        "    df[meta_cols].to_csv(EMB_DIR / f\"{name}_meta.csv\", index=False)\n",
        "\n",
        "    print(f\"‚úÖ Done: {name} | shape = {all_embs.shape}\")\n",
        "\n",
        "print(\"\\nüéØ Ho√†n t·∫•t t√≠nh v√† l∆∞u embedding cho 3 file g·ªëc (clean / targeted / poisoned)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4kwQ2H68EwH",
        "outputId": "2b14c2d2-6073-4cf0-e3d1-81b5d4ddb373"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîπ Encoding queries (15232 queries) using column 'queries'...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Encoding queries: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15232/15232 [05:59<00:00, 42.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Done: queries embeddings saved with shape = (15232, 768)\n"
          ]
        }
      ],
      "source": [
        "# --- Encode queries (th√™m v√†o sau khi ƒë√£ encode 3 file g·ªëc) ---\n",
        "TEST_FILE = DATA_DIR / \"test.csv\"  # ƒë∆∞·ªùng d·∫´n t·ªõi file queries\n",
        "\n",
        "if TEST_FILE.exists():\n",
        "    queries_df = pd.read_csv(TEST_FILE)\n",
        "    # ƒë·∫£m b·∫£o c·ªôt t√™n ƒë√∫ng\n",
        "    assert \"queries\" in queries_df.columns or \"query\" in queries_df.columns, \\\n",
        "        \"File test.csv c·∫ßn c√≥ c·ªôt 'queries' (ho·∫∑c 'query')\"\n",
        "\n",
        "    # ch·ªçn t√™n c·ªôt ƒë√∫ng n·∫øu kh√°c\n",
        "    q_col = \"queries\" if \"queries\" in queries_df.columns else \"query\"\n",
        "\n",
        "    print(f\"\\nüîπ Encoding queries ({len(queries_df)} queries) using column '{q_col}'...\")\n",
        "    query_embs = []\n",
        "    for text in tqdm(queries_df[q_col].fillna(\"\").astype(str), desc=\"Encoding queries\"):\n",
        "        emb = sliding_window_embed(text, model)\n",
        "        query_embs.append(emb.cpu().numpy())\n",
        "\n",
        "    query_embs = np.vstack(query_embs)  # (n_queries, dim)\n",
        "    np.save(EMB_DIR / \"queries_emb.npy\", query_embs)\n",
        "    queries_df[[\"queries_id\", q_col]] if \"queries_id\" in queries_df.columns else queries_df[[q_col]]\n",
        "    # l∆∞u metadata (gi·ªØ queries_id n·∫øu c√≥)\n",
        "    meta_cols = [\"queries_id\", q_col] if \"queries_id\" in queries_df.columns else [q_col]\n",
        "    queries_df[meta_cols].to_csv(EMB_DIR / \"queries_meta.csv\", index=False)\n",
        "\n",
        "    print(f\"‚úÖ Done: queries embeddings saved with shape = {query_embs.shape}\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file queries ·ªü: {TEST_FILE}. B·ªè qua b∆∞·ªõc encode queries.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0-0l6tWBUTr",
        "outputId": "be28a284-aac1-4413-fca4-6ba1237987e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîπ Loading embeddings ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating poisoned_pool ratios: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:11<00:00,  1.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Saved results to /content/SANNER_2025/data/hotflip_result/custom_p_at_k_poison.csv\n",
            "               ratio   k  custom_precision@k  num_queries  num_poisoned_docs  \\\n",
            "0  ratio_0_1_percent   1            3.571429        15232                157   \n",
            "1  ratio_0_1_percent   3            3.437938        15232                157   \n",
            "2  ratio_0_1_percent   5            3.453256        15232                157   \n",
            "3  ratio_0_1_percent  10            3.409270        15232                157   \n",
            "4  ratio_0_5_percent   1            3.571429        15232                157   \n",
            "5  ratio_0_5_percent   3            3.437938        15232                157   \n",
            "6  ratio_0_5_percent   5            3.453256        15232                157   \n",
            "7  ratio_0_5_percent  10            3.409270        15232                157   \n",
            "8  ratio_1_0_percent   1            3.571429        15232                157   \n",
            "9  ratio_1_0_percent   3            3.437938        15232                157   \n",
            "\n",
            "   num_pool_docs  \n",
            "0           5446  \n",
            "1           5446  \n",
            "2           5446  \n",
            "3           5446  \n",
            "4           5446  \n",
            "5           5446  \n",
            "6           5446  \n",
            "7           5446  \n",
            "8           5446  \n",
            "9           5446  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ====================== C·∫§U H√åNH ======================\n",
        "\n",
        "DATA_DIR = Path(\"/content/SANNER_2025/data\")\n",
        "HOTFLIP_DIR = DATA_DIR / \"hotflip_result\"\n",
        "POISONED_POOL_DIR = HOTFLIP_DIR / \"poisoned_pool\"\n",
        "EMB_DIR = HOTFLIP_DIR / \"embeddings\"\n",
        "TEST_FILE = DATA_DIR / \"test.csv\"\n",
        "\n",
        "# ch·ªçn thi·∫øt b·ªã\n",
        "if torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "elif torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "TOP_K_LIST = [1, 3, 5, 10]\n",
        "\n",
        "# ====================== H√ÄM CH√çNH ======================\n",
        "def custom_precision_at_k_poison(query_embeds, pool_embs, pool_ids, poison_ids, K=10):\n",
        "    sim = torch.matmul(query_embeds, pool_embs.T)\n",
        "    topk_idx = torch.topk(sim, k=K, dim=1).indices\n",
        "\n",
        "    total_hits = 0\n",
        "    for i in range(sim.shape[0]):\n",
        "        topk_docs = [pool_ids[idx] for idx in topk_idx[i].tolist()]\n",
        "        total_hits += sum(d in poison_ids for d in topk_docs)\n",
        "\n",
        "    N = query_embeds.shape[0]\n",
        "    # ‚úÖ scaled ƒë·ªÉ k·∫øt qu·∫£ n·∫±m trong kho·∫£ng 1‚Äì10\n",
        "    return (100 * total_hits) / (K * N)\n",
        "\n",
        "# ====================== T·∫¢I EMBEDDING ======================\n",
        "print(\"üîπ Loading embeddings ...\")\n",
        "\n",
        "# queries\n",
        "queries_emb = torch.tensor(np.load(EMB_DIR / \"queries_emb.npy\"), dtype=torch.float32, device=device)\n",
        "queries_meta = pd.read_csv(EMB_DIR / \"queries_meta.csv\")\n",
        "\n",
        "# poisoned / targeted / clean\n",
        "meta_poisoned = pd.read_csv(EMB_DIR / \"poisoned_doc_meta.csv\")\n",
        "meta_targeted = pd.read_csv(EMB_DIR / \"targeted_doc_meta.csv\")\n",
        "meta_clean = pd.read_csv(EMB_DIR / \"clean_doc_meta.csv\")\n",
        "\n",
        "emb_poisoned = torch.tensor(np.load(EMB_DIR / \"poisoned_doc_emb.npy\"), dtype=torch.float32, device=device)\n",
        "emb_targeted = torch.tensor(np.load(EMB_DIR / \"targeted_doc_emb.npy\"), dtype=torch.float32, device=device)\n",
        "emb_clean = torch.tensor(np.load(EMB_DIR / \"clean_doc_emb.npy\"), dtype=torch.float32, device=device)\n",
        "\n",
        "# Map ID ‚Üí embedding tensor\n",
        "embedding_map = {\n",
        "    **{doc_id: emb_poisoned[i] for i, doc_id in enumerate(meta_poisoned[\"document_id\"])},\n",
        "    **{doc_id: emb_targeted[i] for i, doc_id in enumerate(meta_targeted[\"document_id\"])},\n",
        "    **{doc_id: emb_clean[i] for i, doc_id in enumerate(meta_clean[\"document_id\"])},\n",
        "}\n",
        "\n",
        "# ====================== CH·∫†Y CHO T·ª™NG RATIO ======================\n",
        "results = []\n",
        "\n",
        "ratio_files = sorted(POISONED_POOL_DIR.glob(\"ratio_*percent.csv\"))\n",
        "\n",
        "for f in tqdm(ratio_files, desc=\"Evaluating poisoned_pool ratios\"):\n",
        "    ratio_name = f.stem\n",
        "    df = pd.read_csv(f)\n",
        "\n",
        "    # document_id ‚Üí embedding\n",
        "    valid_ids = [doc_id for doc_id in df[\"document_id\"] if doc_id in embedding_map]\n",
        "    pool_embs = torch.stack([embedding_map[doc_id] for doc_id in valid_ids]).to(device)\n",
        "    pool_ids = list(valid_ids)\n",
        "\n",
        "    # danh s√°ch poisoned docs\n",
        "    poison_ids = set(df.loc[df[\"chosen\"] == 1, \"document_id\"].tolist())\n",
        "\n",
        "    # t√≠nh cho t·ª´ng K\n",
        "    for K in TOP_K_LIST:\n",
        "        p_at_k = custom_precision_at_k_poison(\n",
        "            queries_emb, pool_embs, pool_ids, poison_ids, K=K\n",
        "        )\n",
        "        results.append({\n",
        "            \"ratio\": ratio_name,\n",
        "            \"k\": K,\n",
        "            \"custom_precision@k\": round(float(p_at_k), 6),   # ‚úÖ b·ªè .item(), √©p v·ªÅ float an to√†n\n",
        "            \"num_queries\": queries_emb.shape[0],\n",
        "            \"num_poisoned_docs\": len(poison_ids),\n",
        "            \"num_pool_docs\": len(pool_ids),\n",
        "        })\n",
        "\n",
        "# ====================== L∆ØU K·∫æT QU·∫¢ ======================\n",
        "results_df = pd.DataFrame(results)\n",
        "out_path = HOTFLIP_DIR / \"custom_p_at_k_poison.csv\"\n",
        "results_df.to_csv(out_path, index=False)\n",
        "\n",
        "print(f\"\\n‚úÖ Saved results to {out_path}\")\n",
        "print(results_df.head(10))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
