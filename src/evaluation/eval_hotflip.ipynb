{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Clone repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7qjaOyI-Ig8",
        "outputId": "0bcd78b4-a8a3-4dbc-80b8-0bc730b0a483"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GitHub username: hieunguyen-cyber\n",
            "GitHub token: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "‚úÖ Repo cloned successfully!\n"
          ]
        }
      ],
      "source": [
        "import getpass\n",
        "import subprocess\n",
        "from urllib.parse import quote\n",
        "\n",
        "username = input(\"GitHub username: \")\n",
        "token = getpass.getpass(\"GitHub token: \")\n",
        "repo_url = \"https://github.com/ISE-Lab-AI4LIFE/SANNER_2025.git\"\n",
        "\n",
        "auth_url = repo_url.replace(\"https://\", f\"https://{quote(username)}:{quote(token)}@\")\n",
        "\n",
        "try:\n",
        "    subprocess.run([\"git\", \"clone\", auth_url], check=True)\n",
        "    print(\"‚úÖ Repo cloned successfully!\")\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(\"‚ùå Clone failed. Check error message below:\")\n",
        "    print(e.stderr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Merge Hotflip result and dvide into pools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96l3HgHV-F8N",
        "outputId": "4593984a-0517-4e52-de04-757396717331"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "T·ªïng s·ªë document trong pool: 5446\n",
            "S·ªë poisoned_doc: 157\n",
            "S·ªë targeted_doc: 157\n",
            "S·ªë clean_doc: 5289\n",
            "‚úÖ ƒê√£ l∆∞u 3 file v√†o: /content/SANNER_2025/data/hotflip_result/split_docs\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "DATA_DIR = Path(\"/content/SANNER_2025/data\")  # <-- s·ª≠a ·ªü ƒë√¢y\n",
        "HOTFLIP_FILE = DATA_DIR / \"hotflip_result\" / \"merged_hotflip_results.csv\"\n",
        "POOL_FILE = DATA_DIR / \"pool.csv\"\n",
        "\n",
        "# --- ƒê·ªçc d·ªØ li·ªáu ---\n",
        "hotflip_df = pd.read_csv(HOTFLIP_FILE)\n",
        "pool_df = pd.read_csv(POOL_FILE)\n",
        "\n",
        "# --- L·∫•y danh s√°ch id ---\n",
        "hotflip_ids = set(hotflip_df[\"document_id\"])\n",
        "pool_ids = set(pool_df[\"document_id\"])\n",
        "\n",
        "# --- Ph√¢n lo·∫°i ---\n",
        "poisoned_doc = hotflip_df.copy()\n",
        "targeted_doc = pool_df[pool_df[\"document_id\"].isin(hotflip_ids)]\n",
        "clean_doc = pool_df[~pool_df[\"document_id\"].isin(hotflip_ids)]\n",
        "\n",
        "# --- In th·ªëng k√™ ---\n",
        "print(f\"T·ªïng s·ªë document trong pool: {len(pool_df)}\")\n",
        "print(f\"S·ªë poisoned_doc: {len(poisoned_doc)}\")\n",
        "print(f\"S·ªë targeted_doc: {len(targeted_doc)}\")\n",
        "print(f\"S·ªë clean_doc: {len(clean_doc)}\")\n",
        "\n",
        "# --- L∆∞u ra file ---\n",
        "OUTPUT_DIR = DATA_DIR / \"hotflip_result\" / \"split_docs\"\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "poisoned_doc.to_csv(OUTPUT_DIR / \"poisoned_doc.csv\", index=False)\n",
        "targeted_doc.to_csv(OUTPUT_DIR / \"targeted_doc.csv\", index=False)\n",
        "clean_doc.to_csv(OUTPUT_DIR / \"clean_doc.csv\", index=False)\n",
        "\n",
        "print(f\"‚úÖ ƒê√£ l∆∞u 3 file v√†o: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Divide Poison Pool into different ratios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OLi6uLP_emf",
        "outputId": "d783b84c-18f9-4c2c-b8c6-7b160ed43893"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ ratio=0.1%: 5446 poisoned_pool (157 poison, 5289 clean) | 5446 base_pool (157 target, 5289 clean)\n",
            "‚úÖ ratio=0.5%: 5446 poisoned_pool (157 poison, 5289 clean) | 5446 base_pool (157 target, 5289 clean)\n",
            "‚úÖ ratio=1.0%: 5446 poisoned_pool (157 poison, 5289 clean) | 5446 base_pool (157 target, 5289 clean)\n",
            "‚úÖ ratio=1.5%: 5446 poisoned_pool (157 poison, 5289 clean) | 5446 base_pool (157 target, 5289 clean)\n",
            "‚úÖ ratio=2.0%: 5446 poisoned_pool (157 poison, 5289 clean) | 5446 base_pool (157 target, 5289 clean)\n",
            "‚úÖ ratio=2.5%: 5446 poisoned_pool (157 poison, 5289 clean) | 5446 base_pool (157 target, 5289 clean)\n",
            "‚úÖ ratio=3.0%: 5233 poisoned_pool (157 poison, 5076 clean) | 5233 base_pool (157 target, 5076 clean)\n",
            "\n",
            "üéØ Ho√†n t·∫•t t·∫°o pool cho t·∫•t c·∫£ c√°c ratio (0.1% ‚Üí 3%)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "DATA_DIR = Path(\"/content/SANNER_2025/data\")\n",
        "HOTFLIP_DIR = DATA_DIR / \"hotflip_result\"\n",
        "SPLIT_DIR = HOTFLIP_DIR / \"split_docs\"\n",
        "POISONED_POOL_DIR = HOTFLIP_DIR / \"poisoned_pool\"\n",
        "BASE_POOL_DIR = HOTFLIP_DIR / \"base_pool\"\n",
        "\n",
        "POISONED_POOL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "BASE_POOL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- ƒê·ªçc 3 file ---\n",
        "poisoned_doc = pd.read_csv(SPLIT_DIR / \"poisoned_doc.csv\")\n",
        "targeted_doc = pd.read_csv(SPLIT_DIR / \"targeted_doc.csv\")\n",
        "clean_doc = pd.read_csv(SPLIT_DIR / \"clean_doc.csv\")\n",
        "\n",
        "# --- T·∫°o pool v·ªõi nhi·ªÅu ratio ---\n",
        "ratios = np.array([0.001, 0.005, 0.01, 0.015, 0.02, 0.025, 0.03])\n",
        "\n",
        "for ratio in ratios:\n",
        "    # --- S·ªë l∆∞·ª£ng clean c·∫ßn ch·ªçn ---\n",
        "    n_poison = len(poisoned_doc)\n",
        "    n_target = len(targeted_doc)\n",
        "\n",
        "    n_clean_poisoned_pool = int(n_poison * (1 - ratio) / ratio)\n",
        "    n_clean_base_pool = int(n_target * (1 - ratio) / ratio)\n",
        "\n",
        "    # --- Gi·ªõi h·∫°n n·∫øu clean_doc kh√¥ng ƒë·ªß ---\n",
        "    n_clean_poisoned_pool = min(n_clean_poisoned_pool, len(clean_doc))\n",
        "    n_clean_base_pool = min(n_clean_base_pool, len(clean_doc))\n",
        "\n",
        "    # --- Sample clean_doc ---\n",
        "    clean_sample_for_poisoned = clean_doc.sample(\n",
        "        n=n_clean_poisoned_pool,\n",
        "        random_state=42,\n",
        "        replace=False\n",
        "    )\n",
        "    clean_sample_for_base = clean_doc.sample(\n",
        "        n=n_clean_base_pool,\n",
        "        random_state=42,\n",
        "        replace=False\n",
        "    )\n",
        "\n",
        "    # --- G·∫Øn nh√£n chosen ---\n",
        "    poisoned_part = poisoned_doc[[\"document_id\", \"final_poisoned_doc\"]].rename(\n",
        "        columns={\"final_poisoned_doc\": \"document\"}\n",
        "    )\n",
        "    poisoned_part[\"chosen\"] = 1\n",
        "\n",
        "    targeted_part = targeted_doc[[\"document_id\", \"document\"]].copy()\n",
        "    targeted_part[\"chosen\"] = 1\n",
        "\n",
        "    clean_part_for_poisoned = clean_sample_for_poisoned[[\"document_id\", \"document\"]].copy()\n",
        "    clean_part_for_poisoned[\"chosen\"] = 0\n",
        "\n",
        "    clean_part_for_base = clean_sample_for_base[[\"document_id\", \"document\"]].copy()\n",
        "    clean_part_for_base[\"chosen\"] = 0\n",
        "\n",
        "    # --- G·ªôp l·∫°i ---\n",
        "    poisoned_pool = pd.concat([poisoned_part, clean_part_for_poisoned], ignore_index=True)\n",
        "    base_pool = pd.concat([targeted_part, clean_part_for_base], ignore_index=True)\n",
        "\n",
        "    # --- L∆∞u ra CSV ---\n",
        "    r_str = f\"{ratio*100:.1f}\".replace('.', '_')  # v√≠ d·ª• 0_1%, 0_2%, ...\n",
        "    poisoned_pool.to_csv(POISONED_POOL_DIR / f\"ratio_{r_str}_percent.csv\", index=False)\n",
        "    base_pool.to_csv(BASE_POOL_DIR / f\"ratio_{r_str}_percent.csv\", index=False)\n",
        "\n",
        "    print(f\"‚úÖ ratio={ratio*100:.1f}%: \"\n",
        "          f\"{len(poisoned_pool)} poisoned_pool ({len(poisoned_part)} poison, {len(clean_part_for_poisoned)} clean) | \"\n",
        "          f\"{len(base_pool)} base_pool ({len(targeted_part)} target, {len(clean_part_for_base)} clean)\")\n",
        "\n",
        "print(\"\\nüéØ Ho√†n t·∫•t t·∫°o pool cho t·∫•t c·∫£ c√°c ratio (0.1% ‚Üí 3%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Divide Targetted Pool into different ratios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "# --- C·∫•u h√¨nh th∆∞ m·ª•c ---\n",
        "DATA_DIR = Path(\"/content/SANNER_2025/data\")\n",
        "HOTFLIP_DIR = DATA_DIR / \"hotflip_result\"\n",
        "SPLIT_DIR = HOTFLIP_DIR / \"split_docs\"\n",
        "TARGETTED_POOL_DIR = HOTFLIP_DIR / \"targetted_pool\"\n",
        "BASE_POOL_DIR = HOTFLIP_DIR / \"base_pool_targetted\"\n",
        "\n",
        "TARGETTED_POOL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "BASE_POOL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- ƒê·ªçc d·ªØ li·ªáu ---\n",
        "poisoned_doc = pd.read_csv(SPLIT_DIR / \"poisoned_doc.csv\")\n",
        "targeted_doc = pd.read_csv(SPLIT_DIR / \"targeted_doc.csv\")\n",
        "clean_doc = pd.read_csv(SPLIT_DIR / \"clean_doc.csv\")\n",
        "\n",
        "# --- C√°c t·ª∑ l·ªá c·∫ßn t·∫°o ---\n",
        "ratios = np.array([0.001, 0.005, 0.01, 0.015, 0.02, 0.025, 0.03])\n",
        "\n",
        "for ratio in ratios:\n",
        "    # --- S·ªë l∆∞·ª£ng doc ---\n",
        "    n_target = len(targeted_doc)\n",
        "    n_poison = len(poisoned_doc)\n",
        "\n",
        "    n_clean_targetted_pool = int(n_target * (1 - ratio) / ratio)\n",
        "    n_clean_base_pool = int(n_poison * (1 - ratio) / ratio)\n",
        "\n",
        "    # --- Gi·ªõi h·∫°n n·∫øu clean_doc kh√¥ng ƒë·ªß ---\n",
        "    n_clean_targetted_pool = min(n_clean_targetted_pool, len(clean_doc))\n",
        "    n_clean_base_pool = min(n_clean_base_pool, len(clean_doc))\n",
        "\n",
        "    # --- Sample clean doc ---\n",
        "    clean_sample_for_targetted = clean_doc.sample(\n",
        "        n=n_clean_targetted_pool,\n",
        "        random_state=42,\n",
        "        replace=False\n",
        "    )\n",
        "    clean_sample_for_base = clean_doc.sample(\n",
        "        n=n_clean_base_pool,\n",
        "        random_state=42,\n",
        "        replace=False\n",
        "    )\n",
        "\n",
        "    # --- Chu·∫©n b·ªã ph·∫ßn targeted ---\n",
        "    targeted_part = targeted_doc[[\"document_id\", \"document\"]].copy()\n",
        "    targeted_part[\"chosen\"] = 1\n",
        "\n",
        "    poisoned_part = poisoned_doc[[\"document_id\", \"final_poisoned_doc\"]].rename(\n",
        "        columns={\"final_poisoned_doc\": \"document\"}\n",
        "    )\n",
        "    poisoned_part[\"chosen\"] = 1\n",
        "\n",
        "    clean_part_for_targetted = clean_sample_for_targetted[[\"document_id\", \"document\"]].copy()\n",
        "    clean_part_for_targetted[\"chosen\"] = 0\n",
        "\n",
        "    clean_part_for_base = clean_sample_for_base[[\"document_id\", \"document\"]].copy()\n",
        "    clean_part_for_base[\"chosen\"] = 0\n",
        "\n",
        "    # --- G·ªôp l·∫°i ---\n",
        "    targetted_pool = pd.concat([targeted_part, clean_part_for_targetted], ignore_index=True)\n",
        "    base_pool = pd.concat([poisoned_part, clean_part_for_base], ignore_index=True)\n",
        "\n",
        "    # --- L∆∞u file ---\n",
        "    r_str = f\"{ratio*100:.1f}\".replace('.', '_')\n",
        "    targetted_pool.to_csv(TARGETTED_POOL_DIR / f\"ratio_{r_str}_percent.csv\", index=False)\n",
        "    base_pool.to_csv(BASE_POOL_DIR / f\"ratio_{r_str}_percent.csv\", index=False)\n",
        "\n",
        "    print(f\"üéØ ratio={ratio*100:.1f}%: \"\n",
        "          f\"{len(targetted_pool)} targetted_pool ({len(targeted_part)} target, {len(clean_part_for_targetted)} clean) | \"\n",
        "          f\"{len(base_pool)} base_pool ({len(poisoned_part)} poison, {len(clean_part_for_base)} clean)\")\n",
        "\n",
        "print(\"\\n‚úÖ Ho√†n t·∫•t t·∫°o pool targetted cho t·∫•t c·∫£ c√°c ratio (0.1% ‚Üí 3%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Embedding Calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80zMEQL7_rGd",
        "outputId": "b80d01a0-7122-4a0c-a1df-7406ff82c5b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîπ Encoding poisoned_doc (157 docs) using column 'final_poisoned_doc'...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Encoding poisoned_doc:   0%|          | 0/157 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (403 > 384). Running this sequence through the model will result in indexing errors\n",
            "Encoding poisoned_doc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:11<00:00, 13.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Done: poisoned_doc | shape = (157, 768)\n",
            "\n",
            "üîπ Encoding targeted_doc (157 docs) using column 'document'...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Encoding targeted_doc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:07<00:00, 21.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Done: targeted_doc | shape = (157, 768)\n",
            "\n",
            "üîπ Encoding clean_doc (5289 docs) using column 'document'...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Encoding clean_doc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5289/5289 [05:18<00:00, 16.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Done: clean_doc | shape = (5289, 768)\n",
            "\n",
            "üéØ Ho√†n t·∫•t t√≠nh v√† l∆∞u embedding cho 3 file g·ªëc (clean / targeted / poisoned)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "DATA_DIR = Path(\"/content/SANNER_2025/data\")\n",
        "HOTFLIP_DIR = DATA_DIR / \"hotflip_result\"\n",
        "SPLIT_DIR = HOTFLIP_DIR / \"split_docs\"\n",
        "EMB_DIR = HOTFLIP_DIR / \"embeddings\"\n",
        "EMB_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- Thi·∫øt b·ªã ---\n",
        "if torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "elif torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\", device=device)\n",
        "\n",
        "# --- Sliding window config ---\n",
        "MAX_TOKENS = 256\n",
        "STRIDE = 128\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "def sliding_window_embed(text, model, max_len=MAX_TOKENS, stride=STRIDE):\n",
        "    \"\"\"C·∫Øt vƒÉn b·∫£n d√†i v√† trung b√¨nh embedding.\"\"\"\n",
        "    tokens = model.tokenizer.tokenize(text)\n",
        "    if len(tokens) <= max_len:\n",
        "        return model.encode([text], convert_to_tensor=True)\n",
        "    else:\n",
        "        chunks = []\n",
        "        for i in range(0, len(tokens), stride):\n",
        "            chunk = tokens[i:i + max_len]\n",
        "            chunk_text = model.tokenizer.convert_tokens_to_string(chunk)\n",
        "            chunks.append(chunk_text)\n",
        "            if i + max_len >= len(tokens):\n",
        "                break\n",
        "        embeddings = model.encode(chunks, convert_to_tensor=True, batch_size=BATCH_SIZE)\n",
        "        return embeddings.mean(dim=0, keepdim=True)\n",
        "\n",
        "# --- Danh s√°ch file v√† c·ªôt t∆∞∆°ng ·ª©ng ---\n",
        "file_configs = {\n",
        "    \"poisoned_doc\": \"final_poisoned_doc\",\n",
        "    \"targeted_doc\": \"document\",\n",
        "    \"clean_doc\": \"document\"\n",
        "}\n",
        "\n",
        "for name, text_col in file_configs.items():\n",
        "    path = SPLIT_DIR / f\"{name}.csv\"\n",
        "    df = pd.read_csv(path)\n",
        "\n",
        "    print(f\"\\nüîπ Encoding {name} ({len(df)} docs) using column '{text_col}'...\")\n",
        "    all_embs = []\n",
        "    for text in tqdm(df[text_col], desc=f\"Encoding {name}\"):\n",
        "        emb = sliding_window_embed(str(text), model)\n",
        "        all_embs.append(emb.cpu().numpy())\n",
        "\n",
        "    all_embs = np.vstack(all_embs)\n",
        "    np.save(EMB_DIR / f\"{name}_emb.npy\", all_embs)\n",
        "\n",
        "    # ch·ªâ gi·ªØ l·∫°i c√°c c·ªôt li√™n quan ƒë·ªÉ truy map theo id\n",
        "    meta_cols = [\"document_id\", text_col]\n",
        "    df[meta_cols].to_csv(EMB_DIR / f\"{name}_meta.csv\", index=False)\n",
        "\n",
        "    print(f\"‚úÖ Done: {name} | shape = {all_embs.shape}\")\n",
        "\n",
        "print(\"\\nüéØ Ho√†n t·∫•t t√≠nh v√† l∆∞u embedding cho 3 file g·ªëc (clean / targeted / poisoned)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4kwQ2H68EwH",
        "outputId": "2b14c2d2-6073-4cf0-e3d1-81b5d4ddb373"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîπ Encoding queries (15232 queries) using column 'queries'...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Encoding queries: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15232/15232 [05:59<00:00, 42.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Done: queries embeddings saved with shape = (15232, 768)\n"
          ]
        }
      ],
      "source": [
        "# --- Encode queries (th√™m v√†o sau khi ƒë√£ encode 3 file g·ªëc) ---\n",
        "TEST_FILE = DATA_DIR / \"test.csv\"  # ƒë∆∞·ªùng d·∫´n t·ªõi file queries\n",
        "\n",
        "if TEST_FILE.exists():\n",
        "    queries_df = pd.read_csv(TEST_FILE)\n",
        "    # ƒë·∫£m b·∫£o c·ªôt t√™n ƒë√∫ng\n",
        "    assert \"queries\" in queries_df.columns or \"query\" in queries_df.columns, \\\n",
        "        \"File test.csv c·∫ßn c√≥ c·ªôt 'queries' (ho·∫∑c 'query')\"\n",
        "\n",
        "    # ch·ªçn t√™n c·ªôt ƒë√∫ng n·∫øu kh√°c\n",
        "    q_col = \"queries\" if \"queries\" in queries_df.columns else \"query\"\n",
        "\n",
        "    print(f\"\\nüîπ Encoding queries ({len(queries_df)} queries) using column '{q_col}'...\")\n",
        "    query_embs = []\n",
        "    for text in tqdm(queries_df[q_col].fillna(\"\").astype(str), desc=\"Encoding queries\"):\n",
        "        emb = sliding_window_embed(text, model)\n",
        "        query_embs.append(emb.cpu().numpy())\n",
        "\n",
        "    query_embs = np.vstack(query_embs)  # (n_queries, dim)\n",
        "    np.save(EMB_DIR / \"queries_emb.npy\", query_embs)\n",
        "    queries_df[[\"queries_id\", q_col]] if \"queries_id\" in queries_df.columns else queries_df[[q_col]]\n",
        "    # l∆∞u metadata (gi·ªØ queries_id n·∫øu c√≥)\n",
        "    meta_cols = [\"queries_id\", q_col] if \"queries_id\" in queries_df.columns else [q_col]\n",
        "    queries_df[meta_cols].to_csv(EMB_DIR / \"queries_meta.csv\", index=False)\n",
        "\n",
        "    print(f\"‚úÖ Done: queries embeddings saved with shape = {query_embs.shape}\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file queries ·ªü: {TEST_FILE}. B·ªè qua b∆∞·ªõc encode queries.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1st test\n",
        "## üìä Explanation of Metrics\n",
        "\n",
        "- **ratio** ‚Üí Poison ratio of the document pool (e.g., 10%, 30%, 50%)\n",
        "- **k** ‚Üí Top-K documents considered for each query\n",
        "- **custom_precision@k** ‚Üí % of poisoned documents appearing in top-K results  \n",
        "  ‚Üí Higher = stronger poisoning effect\n",
        "- **num_queries** ‚Üí Number of evaluated query embeddings\n",
        "- **num_poisoned_docs** ‚Üí Number of poisoned (inserted) documents in the pool\n",
        "- **num_pool_docs** ‚Üí Total number of documents in the retrieval pool\n",
        "## Poison Doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0-0l6tWBUTr",
        "outputId": "be28a284-aac1-4413-fca4-6ba1237987e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîπ Loading embeddings ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating poisoned_pool ratios: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:11<00:00,  1.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Saved results to /content/SANNER_2025/data/hotflip_result/custom_p_at_k_poison.csv\n",
            "               ratio   k  custom_precision@k  num_queries  num_poisoned_docs  \\\n",
            "0  ratio_0_1_percent   1            3.571429        15232                157   \n",
            "1  ratio_0_1_percent   3            3.437938        15232                157   \n",
            "2  ratio_0_1_percent   5            3.453256        15232                157   \n",
            "3  ratio_0_1_percent  10            3.409270        15232                157   \n",
            "4  ratio_0_5_percent   1            3.571429        15232                157   \n",
            "5  ratio_0_5_percent   3            3.437938        15232                157   \n",
            "6  ratio_0_5_percent   5            3.453256        15232                157   \n",
            "7  ratio_0_5_percent  10            3.409270        15232                157   \n",
            "8  ratio_1_0_percent   1            3.571429        15232                157   \n",
            "9  ratio_1_0_percent   3            3.437938        15232                157   \n",
            "\n",
            "   num_pool_docs  \n",
            "0           5446  \n",
            "1           5446  \n",
            "2           5446  \n",
            "3           5446  \n",
            "4           5446  \n",
            "5           5446  \n",
            "6           5446  \n",
            "7           5446  \n",
            "8           5446  \n",
            "9           5446  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ====================== C·∫§U H√åNH ======================\n",
        "\n",
        "DATA_DIR = Path(\"/content/SANNER_2025/data\")\n",
        "HOTFLIP_DIR = DATA_DIR / \"hotflip_result\"\n",
        "POISONED_POOL_DIR = HOTFLIP_DIR / \"poisoned_pool\"\n",
        "EMB_DIR = HOTFLIP_DIR / \"embeddings\"\n",
        "TEST_FILE = DATA_DIR / \"test.csv\"\n",
        "\n",
        "# ch·ªçn thi·∫øt b·ªã\n",
        "if torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "elif torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "TOP_K_LIST = [1, 3, 5, 10]\n",
        "\n",
        "# ====================== H√ÄM CH√çNH ======================\n",
        "def custom_precision_at_k_poison(query_embeds, pool_embs, pool_ids, poison_ids, K=10):\n",
        "    sim = torch.matmul(query_embeds, pool_embs.T)\n",
        "    topk_idx = torch.topk(sim, k=K, dim=1).indices\n",
        "\n",
        "    total_hits = 0\n",
        "    for i in range(sim.shape[0]):\n",
        "        topk_docs = [pool_ids[idx] for idx in topk_idx[i].tolist()]\n",
        "        total_hits += sum(d in poison_ids for d in topk_docs)\n",
        "\n",
        "    N = query_embeds.shape[0]\n",
        "    # ‚úÖ scaled ƒë·ªÉ k·∫øt qu·∫£ n·∫±m trong kho·∫£ng 1‚Äì10\n",
        "    return (100 * total_hits) / (K * N)\n",
        "\n",
        "# ====================== T·∫¢I EMBEDDING ======================\n",
        "print(\"üîπ Loading embeddings ...\")\n",
        "\n",
        "# queries\n",
        "queries_emb = torch.tensor(np.load(EMB_DIR / \"queries_emb.npy\"), dtype=torch.float32, device=device)\n",
        "queries_meta = pd.read_csv(EMB_DIR / \"queries_meta.csv\")\n",
        "\n",
        "# poisoned / targeted / clean\n",
        "meta_poisoned = pd.read_csv(EMB_DIR / \"poisoned_doc_meta.csv\")\n",
        "meta_targeted = pd.read_csv(EMB_DIR / \"targeted_doc_meta.csv\")\n",
        "meta_clean = pd.read_csv(EMB_DIR / \"clean_doc_meta.csv\")\n",
        "\n",
        "emb_poisoned = torch.tensor(np.load(EMB_DIR / \"poisoned_doc_emb.npy\"), dtype=torch.float32, device=device)\n",
        "emb_targeted = torch.tensor(np.load(EMB_DIR / \"targeted_doc_emb.npy\"), dtype=torch.float32, device=device)\n",
        "emb_clean = torch.tensor(np.load(EMB_DIR / \"clean_doc_emb.npy\"), dtype=torch.float32, device=device)\n",
        "\n",
        "# Map ID ‚Üí embedding tensor\n",
        "embedding_map = {\n",
        "    **{doc_id: emb_poisoned[i] for i, doc_id in enumerate(meta_poisoned[\"document_id\"])},\n",
        "    **{doc_id: emb_targeted[i] for i, doc_id in enumerate(meta_targeted[\"document_id\"])},\n",
        "    **{doc_id: emb_clean[i] for i, doc_id in enumerate(meta_clean[\"document_id\"])},\n",
        "}\n",
        "\n",
        "# ====================== CH·∫†Y CHO T·ª™NG RATIO ======================\n",
        "results = []\n",
        "\n",
        "ratio_files = sorted(POISONED_POOL_DIR.glob(\"ratio_*percent.csv\"))\n",
        "\n",
        "for f in tqdm(ratio_files, desc=\"Evaluating poisoned_pool ratios\"):\n",
        "    ratio_name = f.stem\n",
        "    df = pd.read_csv(f)\n",
        "\n",
        "    # document_id ‚Üí embedding\n",
        "    valid_ids = [doc_id for doc_id in df[\"document_id\"] if doc_id in embedding_map]\n",
        "    pool_embs = torch.stack([embedding_map[doc_id] for doc_id in valid_ids]).to(device)\n",
        "    pool_ids = list(valid_ids)\n",
        "\n",
        "    # danh s√°ch poisoned docs\n",
        "    poison_ids = set(df.loc[df[\"chosen\"] == 1, \"document_id\"].tolist())\n",
        "\n",
        "    # t√≠nh cho t·ª´ng K\n",
        "    for K in TOP_K_LIST:\n",
        "        p_at_k = custom_precision_at_k_poison(\n",
        "            queries_emb, pool_embs, pool_ids, poison_ids, K=K\n",
        "        )\n",
        "        results.append({\n",
        "            \"ratio\": ratio_name,\n",
        "            \"k\": K,\n",
        "            \"custom_precision@k\": round(float(p_at_k), 6),   # ‚úÖ b·ªè .item(), √©p v·ªÅ float an to√†n\n",
        "            \"num_queries\": queries_emb.shape[0],\n",
        "            \"num_poisoned_docs\": len(poison_ids),\n",
        "            \"num_pool_docs\": len(pool_ids),\n",
        "        })\n",
        "\n",
        "# ====================== L∆ØU K·∫æT QU·∫¢ ======================\n",
        "results_df = pd.DataFrame(results)\n",
        "out_path = HOTFLIP_DIR / \"result\" / \"custom_p_at_k_poison.csv\"\n",
        "results_df.to_csv(out_path, index=False)\n",
        "\n",
        "print(f\"\\n‚úÖ Saved results to {out_path}\")\n",
        "print(results_df.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Targetted Doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ====================== C·∫§U H√åNH ======================\n",
        "\n",
        "DATA_DIR = Path(\"/content/SANNER_2025/data\")\n",
        "HOTFLIP_DIR = DATA_DIR / \"hotflip_result\"\n",
        "TARGETTED_POOL_DIR = HOTFLIP_DIR / \"targetted_pool\"\n",
        "EMB_DIR = HOTFLIP_DIR / \"embeddings\"\n",
        "TEST_FILE = DATA_DIR / \"test.csv\"\n",
        "\n",
        "# ch·ªçn thi·∫øt b·ªã\n",
        "if torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "elif torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "TOP_K_LIST = [1, 3, 5, 10]\n",
        "\n",
        "# ====================== H√ÄM CH√çNH ======================\n",
        "def custom_precision_at_k_target(query_embeds, pool_embs, pool_ids, target_ids, K=10):\n",
        "    \"\"\"\n",
        "    T√≠nh custom precision@k cho targeted attack:\n",
        "    - query_embeds: embedding c·ªßa queries\n",
        "    - pool_embs: embedding c·ªßa c√°c document trong targetted_pool\n",
        "    - target_ids: t·∫≠p document_id b·ªã target (chosen == 1)\n",
        "    \"\"\"\n",
        "    sim = torch.matmul(query_embeds, pool_embs.T)\n",
        "    topk_idx = torch.topk(sim, k=K, dim=1).indices\n",
        "\n",
        "    total_hits = 0\n",
        "    for i in range(sim.shape[0]):\n",
        "        topk_docs = [pool_ids[idx] for idx in topk_idx[i].tolist()]\n",
        "        total_hits += sum(d in target_ids for d in topk_docs)\n",
        "\n",
        "    N = query_embeds.shape[0]\n",
        "    # ‚úÖ scale ƒë·ªÉ k·∫øt qu·∫£ c√≥ th·ªÉ so s√°nh ƒë∆∞·ª£c, n·∫±m trong kho·∫£ng 1‚Äì10\n",
        "    return (100 * total_hits) / (K * N)\n",
        "\n",
        "# ====================== T·∫¢I EMBEDDING ======================\n",
        "print(\"üîπ Loading embeddings ...\")\n",
        "\n",
        "# queries\n",
        "queries_emb = torch.tensor(np.load(EMB_DIR / \"queries_emb.npy\"), dtype=torch.float32, device=device)\n",
        "queries_meta = pd.read_csv(EMB_DIR / \"queries_meta.csv\")\n",
        "\n",
        "# poisoned / targeted / clean\n",
        "meta_poisoned = pd.read_csv(EMB_DIR / \"poisoned_doc_meta.csv\")\n",
        "meta_targeted = pd.read_csv(EMB_DIR / \"targeted_doc_meta.csv\")\n",
        "meta_clean = pd.read_csv(EMB_DIR / \"clean_doc_meta.csv\")\n",
        "\n",
        "emb_poisoned = torch.tensor(np.load(EMB_DIR / \"poisoned_doc_emb.npy\"), dtype=torch.float32, device=device)\n",
        "emb_targeted = torch.tensor(np.load(EMB_DIR / \"targeted_doc_emb.npy\"), dtype=torch.float32, device=device)\n",
        "emb_clean = torch.tensor(np.load(EMB_DIR / \"clean_doc_emb.npy\"), dtype=torch.float32, device=device)\n",
        "\n",
        "# Map ID ‚Üí embedding tensor\n",
        "embedding_map = {\n",
        "    **{doc_id: emb_poisoned[i] for i, doc_id in enumerate(meta_poisoned[\"document_id\"])},\n",
        "    **{doc_id: emb_targeted[i] for i, doc_id in enumerate(meta_targeted[\"document_id\"])},\n",
        "    **{doc_id: emb_clean[i] for i, doc_id in enumerate(meta_clean[\"document_id\"])},\n",
        "}\n",
        "\n",
        "# ====================== CH·∫†Y CHO T·ª™NG RATIO ======================\n",
        "results = []\n",
        "\n",
        "ratio_files = sorted(TARGETTED_POOL_DIR.glob(\"ratio_*percent.csv\"))\n",
        "\n",
        "for f in tqdm(ratio_files, desc=\"Evaluating targetted_pool ratios\"):\n",
        "    ratio_name = f.stem\n",
        "    df = pd.read_csv(f)\n",
        "\n",
        "    # document_id ‚Üí embedding\n",
        "    valid_ids = [doc_id for doc_id in df[\"document_id\"] if doc_id in embedding_map]\n",
        "    pool_embs = torch.stack([embedding_map[doc_id] for doc_id in valid_ids]).to(device)\n",
        "    pool_ids = list(valid_ids)\n",
        "\n",
        "    # danh s√°ch targeted docs\n",
        "    target_ids = set(df.loc[df[\"chosen\"] == 1, \"document_id\"].tolist())\n",
        "\n",
        "    # t√≠nh cho t·ª´ng K\n",
        "    for K in TOP_K_LIST:\n",
        "        p_at_k = custom_precision_at_k_target(\n",
        "            queries_emb, pool_embs, pool_ids, target_ids, K=K\n",
        "        )\n",
        "        results.append({\n",
        "            \"ratio\": ratio_name,\n",
        "            \"k\": K,\n",
        "            \"custom_precision@k\": round(float(p_at_k), 6),\n",
        "            \"num_queries\": queries_emb.shape[0],\n",
        "            \"num_target_docs\": len(target_ids),\n",
        "            \"num_pool_docs\": len(pool_ids),\n",
        "        })\n",
        "\n",
        "# ====================== L∆ØU K·∫æT QU·∫¢ ======================\n",
        "results_df = pd.DataFrame(results)\n",
        "out_path = HOTFLIP_DIR / \"result\" / \"custom_p_at_k_targetted.csv\"\n",
        "results_df.to_csv(out_path, index=False)\n",
        "\n",
        "print(f\"\\n‚úÖ Saved results to {out_path}\")\n",
        "print(results_df.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2nd test\n",
        "## üìä Explanation of Metrics\n",
        "\n",
        "- **ratio** ‚Üí Poison ratio of the document pool (e.g., 10%, 30%, 50%)\n",
        "- **avg_poison_in_topK** ‚Üí Average number of poisoned documents appearing in the top-K results per query  \n",
        "  ‚Üí Higher = stronger poisoning or retrieval corruption\n",
        "- **num_queries** ‚Üí Total number of query embeddings evaluated\n",
        "- **num_poisoned_docs** ‚Üí Number of poisoned (manipulated) documents in the pool\n",
        "- **num_pool_docs** ‚Üí Total number of documents available for retrieval\n",
        "## Poisoned Doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- C·∫•u h√¨nh ---\n",
        "DATA_DIR = Path(\"/content/SANNER_2025/data\") \n",
        "HOTFLIP_DIR = DATA_DIR / \"hotflip_result\"\n",
        "POISONED_POOL_DIR = HOTFLIP_DIR / \"poisoned_pool\"\n",
        "EMB_DIR = HOTFLIP_DIR / \"embeddings\"\n",
        "TEST_FILE = DATA_DIR / \"test.csv\"\n",
        "\n",
        "# --- Tham s·ªë ---\n",
        "K = 10\n",
        "\n",
        "# --- Thi·∫øt b·ªã ---\n",
        "if torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "elif torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "# --- Load queries ---\n",
        "queries_emb = torch.tensor(np.load(EMB_DIR / \"queries_emb.npy\"), dtype=torch.float32, device=device)\n",
        "queries_meta = pd.read_csv(EMB_DIR / \"queries_meta.csv\")\n",
        "\n",
        "# --- Load embeddings g·ªëc ---\n",
        "meta_poisoned = pd.read_csv(EMB_DIR / \"poisoned_doc_meta.csv\")\n",
        "meta_targeted = pd.read_csv(EMB_DIR / \"targeted_doc_meta.csv\")\n",
        "meta_clean = pd.read_csv(EMB_DIR / \"clean_doc_meta.csv\")\n",
        "\n",
        "emb_poisoned = torch.tensor(np.load(EMB_DIR / \"poisoned_doc_emb.npy\"), dtype=torch.float32, device=device)\n",
        "emb_targeted = torch.tensor(np.load(EMB_DIR / \"targeted_doc_emb.npy\"), dtype=torch.float32, device=device)\n",
        "emb_clean = torch.tensor(np.load(EMB_DIR / \"clean_doc_emb.npy\"), dtype=torch.float32, device=device)\n",
        "\n",
        "embedding_map = {\n",
        "    **{doc_id: emb_poisoned[i] for i, doc_id in enumerate(meta_poisoned[\"document_id\"])},\n",
        "    **{doc_id: emb_targeted[i] for i, doc_id in enumerate(meta_targeted[\"document_id\"])},\n",
        "    **{doc_id: emb_clean[i] for i, doc_id in enumerate(meta_clean[\"document_id\"])},\n",
        "}\n",
        "\n",
        "# --- T√≠nh cho t·ª´ng ratio ---\n",
        "ratio_files = sorted(POISONED_POOL_DIR.glob(\"ratio_*percent.csv\"))\n",
        "results = []\n",
        "\n",
        "for f in tqdm(ratio_files, desc=\"Computing avg poison-in-topK\"):\n",
        "    ratio_name = f.stem\n",
        "    df = pd.read_csv(f)\n",
        "\n",
        "    valid_ids = [doc_id for doc_id in df[\"document_id\"] if doc_id in embedding_map]\n",
        "    pool_embs = torch.stack([embedding_map[doc_id] for doc_id in valid_ids]).to(device)\n",
        "    pool_ids = list(valid_ids)\n",
        "    poison_ids = set(df.loc[df[\"chosen\"] == 1, \"document_id\"].tolist())\n",
        "\n",
        "    # Cosine similarity\n",
        "    sim = torch.matmul(queries_emb, pool_embs.T)\n",
        "    topk_idx = torch.topk(sim, k=K, dim=1).indices\n",
        "\n",
        "    poison_counts = []\n",
        "    for i in range(sim.shape[0]):\n",
        "        topk_docs = [pool_ids[idx] for idx in topk_idx[i].tolist()]\n",
        "        n_poison = sum(d in poison_ids for d in topk_docs)\n",
        "        poison_counts.append(n_poison)\n",
        "\n",
        "    avg_poison_topk = float(np.mean(poison_counts))\n",
        "    results.append({\n",
        "        \"ratio\": ratio_name,\n",
        "        f\"avg_poison_in_top{K}\": round(avg_poison_topk, 4),\n",
        "        \"num_queries\": len(queries_meta),\n",
        "        \"num_poisoned_docs\": len(poison_ids),\n",
        "        \"num_pool_docs\": len(pool_ids),\n",
        "    })\n",
        "\n",
        "# --- Xu·∫•t k·∫øt qu·∫£ ---\n",
        "results_df = pd.DataFrame(results)\n",
        "out_path = HOTFLIP_DIR / \"result\" / f\"avg_poison_in_top{K}.csv\"\n",
        "results_df.to_csv(out_path, index=False)\n",
        "\n",
        "print(f\"\\n‚úÖ Saved results to {out_path}\")\n",
        "print(results_df.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Targetted Doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ====================== C·∫§U H√åNH ======================\n",
        "DATA_DIR = Path(\"/content/SANNER_2025/data\")\n",
        "HOTFLIP_DIR = DATA_DIR / \"hotflip_result\"\n",
        "TARGETTED_POOL_DIR = HOTFLIP_DIR / \"targetted_pool\"   # ‚úÖ ƒë·ªïi sang targetted\n",
        "EMB_DIR = HOTFLIP_DIR / \"embeddings\"\n",
        "TEST_FILE = DATA_DIR / \"test.csv\"\n",
        "\n",
        "# ====================== THAM S·ªê ======================\n",
        "K = 10  # top-K\n",
        "\n",
        "# ====================== THI·∫æT B·ªä ======================\n",
        "if torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "elif torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "print(f\"‚úÖ Using device: {device}\")\n",
        "\n",
        "# ====================== LOAD QUERY ======================\n",
        "queries_emb = torch.tensor(np.load(EMB_DIR / \"queries_emb.npy\"), dtype=torch.float32, device=device)\n",
        "queries_meta = pd.read_csv(EMB_DIR / \"queries_meta.csv\")\n",
        "\n",
        "# ====================== LOAD EMBEDDING V√Ä META ======================\n",
        "def load_emb_and_meta(name):\n",
        "    meta = pd.read_csv(EMB_DIR / f\"{name}_doc_meta.csv\")\n",
        "    emb = torch.tensor(np.load(EMB_DIR / f\"{name}_doc_emb.npy\"), dtype=torch.float32, device=device)\n",
        "    return meta, emb\n",
        "\n",
        "meta_poisoned, emb_poisoned = load_emb_and_meta(\"poisoned\")\n",
        "meta_targeted, emb_targeted = load_emb_and_meta(\"targeted\")\n",
        "meta_clean, emb_clean = load_emb_and_meta(\"clean\")\n",
        "\n",
        "# Map ID ‚Üí embedding\n",
        "embedding_map = {\n",
        "    **{doc_id: emb_poisoned[i] for i, doc_id in enumerate(meta_poisoned[\"document_id\"])},\n",
        "    **{doc_id: emb_targeted[i] for i, doc_id in enumerate(meta_targeted[\"document_id\"])},\n",
        "    **{doc_id: emb_clean[i] for i, doc_id in enumerate(meta_clean[\"document_id\"])},\n",
        "}\n",
        "\n",
        "print(f\"üì¶ Total embeddings loaded: {len(embedding_map):,}\")\n",
        "\n",
        "# ====================== DUY·ªÜT C√ÅC FILE RATIO ======================\n",
        "ratio_files = sorted(TARGETTED_POOL_DIR.glob(\"ratio_*percent.csv\"))\n",
        "if not ratio_files:\n",
        "    raise FileNotFoundError(f\"No ratio_*percent.csv found in {TARGETTED_POOL_DIR}\")\n",
        "\n",
        "results = []\n",
        "\n",
        "for f in tqdm(ratio_files, desc=\"Computing avg targeted-in-topK\"):\n",
        "    ratio_name = f.stem\n",
        "    df = pd.read_csv(f)\n",
        "\n",
        "    # L·ªçc doc c√≥ embedding\n",
        "    valid_ids = [doc_id for doc_id in df[\"document_id\"] if doc_id in embedding_map]\n",
        "    if not valid_ids:\n",
        "        print(f\"‚ö†Ô∏è Skip {ratio_name} ‚Äî no valid document IDs found.\")\n",
        "        continue\n",
        "\n",
        "    pool_embs = torch.stack([embedding_map[doc_id] for doc_id in valid_ids]).to(device)\n",
        "    pool_ids = list(valid_ids)\n",
        "\n",
        "    # L·∫•y targeted docs\n",
        "    targeted_ids = set(df.loc[df[\"chosen\"] == 1, \"document_id\"])\n",
        "\n",
        "    # --- Cosine similarity ---\n",
        "    sim = torch.matmul(queries_emb, pool_embs.T)\n",
        "    topk_idx = torch.topk(sim, k=min(K, sim.shape[1]), dim=1).indices\n",
        "\n",
        "    # --- ƒê·∫øm s·ªë doc targeted trong top-K ---\n",
        "    targeted_counts = []\n",
        "    for i in range(sim.shape[0]):\n",
        "        topk_docs = [pool_ids[idx] for idx in topk_idx[i].tolist()]\n",
        "        n_targeted = sum(doc_id in targeted_ids for doc_id in topk_docs)\n",
        "        targeted_counts.append(n_targeted)\n",
        "\n",
        "    avg_targeted_topk = float(np.mean(targeted_counts))\n",
        "    results.append({\n",
        "        \"ratio\": ratio_name,\n",
        "        f\"avg_targeted_in_top{K}\": round(avg_targeted_topk, 4),\n",
        "        \"num_queries\": len(queries_meta),\n",
        "        \"num_targeted_docs\": len(targeted_ids),\n",
        "        \"num_pool_docs\": len(pool_ids),\n",
        "    })\n",
        "\n",
        "# ====================== XU·∫§T K·∫æT QU·∫¢ ======================\n",
        "results_df = pd.DataFrame(results)\n",
        "out_path = HOTFLIP_DIR / \"result\" / f\"avg_targeted_in_top{K}.csv\"\n",
        "results_df.to_csv(out_path, index=False)\n",
        "\n",
        "print(f\"\\n‚úÖ Saved results to {out_path}\")\n",
        "print(results_df.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3rd test\n",
        "## üìä Explanation of Attack Evaluation Metrics\n",
        "\n",
        "- **ratio** ‚Üí Poison ratio of the document pool (e.g., 10%, 30%, 50%)\n",
        "- **k** ‚Üí Top-K documents considered for retrieval evaluation\n",
        "- **precision@k_raw** ‚Üí Total number of poisoned docs found across all queries (not normalized)\n",
        "- **precision@k_norm** ‚Üí Normalized precision = hits / (K √ó number of queries)\n",
        "- **precision@k_scaled** ‚Üí Precision scaled to percentage (0‚Äì100%) for easier comparison\n",
        "- **recall@k** ‚Üí Fraction of all poisoned docs successfully retrieved within top-K\n",
        "- **attack_success@k** ‚Üí Attack Success Rate (ASR): proportion of queries that retrieved at least one poisoned doc\n",
        "- **mrr@k** ‚Üí Mean Reciprocal Rank ‚Äî average of 1/rank of the first poisoned doc found  \n",
        "  ‚Üí Higher = poisoned docs appear earlier in ranking\n",
        "- **avg_rank_first_poison** ‚Üí Average rank position of the first poisoned doc  \n",
        "  ‚Üí Lower = stronger attack effect\n",
        "- **num_queries** ‚Üí Number of evaluated queries\n",
        "- **num_poisoned_docs** ‚Üí Total poisoned documents in the pool\n",
        "- **num_pool_docs** ‚Üí Total documents available for retrieval\n",
        "## Poisoned Doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- H√†m ti·ªán √≠ch ch√≠nh ---\n",
        "def compute_attack_metrics(query_embeds, pool_embs, pool_ids, poison_ids, K=10):\n",
        "    sim = torch.matmul(query_embeds, pool_embs.T)  # [n_q, n_docs]\n",
        "    n_queries = sim.shape[0]\n",
        "    n_poison = len(poison_ids)\n",
        "\n",
        "    # L·∫•y top-K index\n",
        "    topk_idx = torch.topk(sim, k=K, dim=1).indices\n",
        "\n",
        "    total_hits = 0\n",
        "    recall_hits = 0\n",
        "    success_flags = []\n",
        "    reciprocal_ranks = []\n",
        "    first_ranks = []\n",
        "\n",
        "    for i in range(n_queries):\n",
        "        scores = sim[i]\n",
        "        ranked_indices = torch.argsort(scores, descending=True)\n",
        "        ranked_ids = [pool_ids[idx] for idx in ranked_indices.tolist()]\n",
        "\n",
        "        # ch·ªâ s·ªë top-K\n",
        "        topk_docs = [pool_ids[idx] for idx in topk_idx[i].tolist()]\n",
        "        poison_in_topk = sum(d in poison_ids for d in topk_docs)\n",
        "        total_hits += poison_in_topk\n",
        "\n",
        "        # Attack Success Rate\n",
        "        success_flags.append(1 if poison_in_topk > 0 else 0)\n",
        "\n",
        "        # Recall@K (t·ªïng poison t√¨m ƒë∆∞·ª£c / t·ªïng poison c√≥)\n",
        "        recall_hits += poison_in_topk\n",
        "\n",
        "        # Rank-based metrics\n",
        "        first_poison_rank = None\n",
        "        for rank, doc in enumerate(ranked_ids, start=1):\n",
        "            if doc in poison_ids:\n",
        "                first_poison_rank = rank\n",
        "                break\n",
        "\n",
        "        if first_poison_rank:\n",
        "            reciprocal_ranks.append(1 / first_poison_rank)\n",
        "            first_ranks.append(first_poison_rank)\n",
        "        else:\n",
        "            reciprocal_ranks.append(0)\n",
        "            first_ranks.append(np.nan)\n",
        "\n",
        "    # Precision (scaled + normalized)\n",
        "    p_raw = total_hits / K\n",
        "    p_norm = total_hits / (K * n_queries)\n",
        "    p_scaled = (100 * total_hits) / (K * n_queries)\n",
        "\n",
        "    # Recall@K\n",
        "    recall_at_k = recall_hits / max(n_poison, 1)\n",
        "\n",
        "    # Attack Success Rate\n",
        "    asr = np.mean(success_flags)\n",
        "\n",
        "    # MRR & ARFP\n",
        "    mrr = np.mean(reciprocal_ranks)\n",
        "    avg_rank = np.nanmean(first_ranks)\n",
        "\n",
        "    return {\n",
        "        \"precision@k_raw\": round(p_raw, 4),\n",
        "        \"precision@k_norm\": round(p_norm, 4),\n",
        "        \"precision@k_scaled\": round(p_scaled, 4),\n",
        "        \"recall@k\": round(recall_at_k, 4),\n",
        "        \"attack_success@k\": round(asr, 4),\n",
        "        \"mrr@k\": round(mrr, 4),\n",
        "        \"avg_rank_first_poison\": round(avg_rank, 2),\n",
        "    }\n",
        "\n",
        "results = []\n",
        "\n",
        "for f in tqdm(ratio_files, desc=\"Evaluating all metrics\"):\n",
        "    ratio_name = f.stem\n",
        "    df = pd.read_csv(f)\n",
        "\n",
        "    valid_ids = [doc_id for doc_id in df[\"document_id\"] if doc_id in embedding_map]\n",
        "    pool_embs = torch.stack([embedding_map[doc_id] for doc_id in valid_ids]).to(device)\n",
        "    pool_ids = list(valid_ids)\n",
        "    poison_ids = set(df.loc[df[\"chosen\"] == 1, \"document_id\"].tolist())\n",
        "\n",
        "    for K in [1, 3, 5, 10]:\n",
        "        metrics = compute_attack_metrics(queries_emb, pool_embs, pool_ids, poison_ids, K=K)\n",
        "        results.append({\n",
        "            \"ratio\": ratio_name,\n",
        "            \"k\": K,\n",
        "            \"num_queries\": queries_emb.shape[0],\n",
        "            \"num_poisoned_docs\": len(poison_ids),\n",
        "            \"num_pool_docs\": len(pool_ids),\n",
        "            **metrics\n",
        "        })\n",
        "\n",
        "# L∆∞u ra CSV\n",
        "results_df = pd.DataFrame(results)\n",
        "out_path = HOTFLIP_DIR / \"result\" / \"attack_metrics_full.csv\"\n",
        "results_df.to_csv(out_path, index=False)\n",
        "\n",
        "print(f\"\\n‚úÖ Saved metrics to {out_path}\")\n",
        "print(results_df.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Targetted Doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ====================== C·∫§U H√åNH ======================\n",
        "DATA_DIR = Path(\"/content/SANNER_2025/data\")\n",
        "HOTFLIP_DIR = DATA_DIR / \"hotflip_result\"\n",
        "TARGETTED_POOL_DIR = HOTFLIP_DIR / \"targetted_pool\"\n",
        "EMB_DIR = HOTFLIP_DIR / \"embeddings\"\n",
        "\n",
        "# ch·ªçn thi·∫øt b·ªã\n",
        "if torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "elif torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "# ====================== H√ÄM CH√çNH ======================\n",
        "def compute_attack_metrics_target(query_embeds, pool_embs, pool_ids, target_ids, K=10):\n",
        "    \"\"\"\n",
        "    T√≠nh c√°c ch·ªâ s·ªë ƒë√°nh gi√° cho targeted attack.\n",
        "    - query_embeds: embedding c·ªßa c√°c query\n",
        "    - pool_embs: embedding c·ªßa c√°c document trong targetted pool\n",
        "    - target_ids: danh s√°ch document_id b·ªã t·∫•n c√¥ng (chosen == 1)\n",
        "    \"\"\"\n",
        "    sim = torch.matmul(query_embeds, pool_embs.T)  # [n_q, n_docs]\n",
        "    n_queries = sim.shape[0]\n",
        "    n_target = len(target_ids)\n",
        "\n",
        "    # L·∫•y top-K index\n",
        "    topk_idx = torch.topk(sim, k=K, dim=1).indices\n",
        "\n",
        "    total_hits = 0\n",
        "    recall_hits = 0\n",
        "    success_flags = []\n",
        "    reciprocal_ranks = []\n",
        "    first_ranks = []\n",
        "\n",
        "    for i in range(n_queries):\n",
        "        scores = sim[i]\n",
        "        ranked_indices = torch.argsort(scores, descending=True)\n",
        "        ranked_ids = [pool_ids[idx] for idx in ranked_indices.tolist()]\n",
        "\n",
        "        # Ch·ªâ s·ªë top-K\n",
        "        topk_docs = [pool_ids[idx] for idx in topk_idx[i].tolist()]\n",
        "        target_in_topk = sum(d in target_ids for d in topk_docs)\n",
        "        total_hits += target_in_topk\n",
        "\n",
        "        # Attack Success Rate\n",
        "        success_flags.append(1 if target_in_topk > 0 else 0)\n",
        "\n",
        "        # Recall@K (t·ªïng target t√¨m ƒë∆∞·ª£c / t·ªïng target c√≥)\n",
        "        recall_hits += target_in_topk\n",
        "\n",
        "        # Rank-based metrics\n",
        "        first_target_rank = None\n",
        "        for rank, doc in enumerate(ranked_ids, start=1):\n",
        "            if doc in target_ids:\n",
        "                first_target_rank = rank\n",
        "                break\n",
        "\n",
        "        if first_target_rank:\n",
        "            reciprocal_ranks.append(1 / first_target_rank)\n",
        "            first_ranks.append(first_target_rank)\n",
        "        else:\n",
        "            reciprocal_ranks.append(0)\n",
        "            first_ranks.append(np.nan)\n",
        "\n",
        "    # Precision (scaled + normalized)\n",
        "    p_raw = total_hits / K\n",
        "    p_norm = total_hits / (K * n_queries)\n",
        "    p_scaled = (100 * total_hits) / (K * n_queries)\n",
        "\n",
        "    # Recall@K\n",
        "    recall_at_k = recall_hits / max(n_target, 1)\n",
        "\n",
        "    # Attack Success Rate\n",
        "    asr = np.mean(success_flags)\n",
        "\n",
        "    # MRR & Avg. Rank\n",
        "    mrr = np.mean(reciprocal_ranks)\n",
        "    avg_rank = np.nanmean(first_ranks)\n",
        "\n",
        "    return {\n",
        "        \"precision@k_raw\": round(p_raw, 4),\n",
        "        \"precision@k_norm\": round(p_norm, 4),\n",
        "        \"precision@k_scaled\": round(p_scaled, 4),\n",
        "        \"recall@k\": round(recall_at_k, 4),\n",
        "        \"attack_success@k\": round(asr, 4),\n",
        "        \"mrr@k\": round(mrr, 4),\n",
        "        \"avg_rank_first_target\": round(avg_rank, 2),\n",
        "    }\n",
        "\n",
        "# ====================== T·∫¢I EMBEDDING ======================\n",
        "print(\"üîπ Loading embeddings ...\")\n",
        "\n",
        "queries_emb = torch.tensor(np.load(EMB_DIR / \"queries_emb.npy\"), dtype=torch.float32, device=device)\n",
        "meta_poisoned = pd.read_csv(EMB_DIR / \"poisoned_doc_meta.csv\")\n",
        "meta_targeted = pd.read_csv(EMB_DIR / \"targeted_doc_meta.csv\")\n",
        "meta_clean = pd.read_csv(EMB_DIR / \"clean_doc_meta.csv\")\n",
        "\n",
        "emb_poisoned = torch.tensor(np.load(EMB_DIR / \"poisoned_doc_emb.npy\"), dtype=torch.float32, device=device)\n",
        "emb_targeted = torch.tensor(np.load(EMB_DIR / \"targeted_doc_emb.npy\"), dtype=torch.float32, device=device)\n",
        "emb_clean = torch.tensor(np.load(EMB_DIR / \"clean_doc_emb.npy\"), dtype=torch.float32, device=device)\n",
        "\n",
        "embedding_map = {\n",
        "    **{doc_id: emb_poisoned[i] for i, doc_id in enumerate(meta_poisoned[\"document_id\"])},\n",
        "    **{doc_id: emb_targeted[i] for i, doc_id in enumerate(meta_targeted[\"document_id\"])},\n",
        "    **{doc_id: emb_clean[i] for i, doc_id in enumerate(meta_clean[\"document_id\"])},\n",
        "}\n",
        "\n",
        "# ====================== CH·∫†Y CHO T·ª™NG RATIO ======================\n",
        "results = []\n",
        "\n",
        "ratio_files = sorted(TARGETTED_POOL_DIR.glob(\"ratio_*percent.csv\"))\n",
        "\n",
        "for f in tqdm(ratio_files, desc=\"Evaluating all metrics for targeted attack\"):\n",
        "    ratio_name = f.stem\n",
        "    df = pd.read_csv(f)\n",
        "\n",
        "    valid_ids = [doc_id for doc_id in df[\"document_id\"] if doc_id in embedding_map]\n",
        "    pool_embs = torch.stack([embedding_map[doc_id] for doc_id in valid_ids]).to(device)\n",
        "    pool_ids = list(valid_ids)\n",
        "    target_ids = set(df.loc[df[\"chosen\"] == 1, \"document_id\"].tolist())\n",
        "\n",
        "    for K in [1, 3, 5, 10]:\n",
        "        metrics = compute_attack_metrics_target(queries_emb, pool_embs, pool_ids, target_ids, K=K)\n",
        "        results.append({\n",
        "            \"ratio\": ratio_name,\n",
        "            \"k\": K,\n",
        "            \"num_queries\": queries_emb.shape[0],\n",
        "            \"num_target_docs\": len(target_ids),\n",
        "            \"num_pool_docs\": len(pool_ids),\n",
        "            **metrics\n",
        "        })\n",
        "\n",
        "# ====================== L∆ØU K·∫æT QU·∫¢ ======================\n",
        "results_df = pd.DataFrame(results)\n",
        "out_path = HOTFLIP_DIR / \"result\" / \"attack_metrics_full_targetted.csv\"\n",
        "results_df.to_csv(out_path, index=False)\n",
        "\n",
        "print(f\"\\n‚úÖ Saved metrics to {out_path}\")\n",
        "print(results_df.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Saved merged & paired CSV to /Users/hieunguyen/Downloads/SANNER_2025/data/hotflip_result/result/result.csv\n",
            "               ratio   k  attack_success@k_poison  attack_success@k_target  \\\n",
            "0  ratio_0_1_percent   1                   0.0357                   0.0357   \n",
            "1  ratio_0_1_percent   3                   0.1001                   0.1001   \n",
            "2  ratio_0_1_percent   5                   0.1618                   0.1618   \n",
            "3  ratio_0_1_percent  10                   0.2906                   0.2906   \n",
            "4  ratio_0_5_percent   1                   0.0357                   0.0357   \n",
            "5  ratio_0_5_percent   3                   0.1001                   0.1001   \n",
            "6  ratio_0_5_percent   5                   0.1618                   0.1618   \n",
            "7  ratio_0_5_percent  10                   0.2906                   0.2906   \n",
            "8  ratio_1_0_percent   1                   0.0357                   0.0357   \n",
            "9  ratio_1_0_percent   3                   0.1001                   0.1001   \n",
            "\n",
            "   avg_poison_in_top10_poison  avg_rank_first_poison_poison  \\\n",
            "0                         NaN                          32.9   \n",
            "1                         NaN                          32.9   \n",
            "2                         NaN                          32.9   \n",
            "3                      0.3409                          32.9   \n",
            "4                         NaN                          32.9   \n",
            "5                         NaN                          32.9   \n",
            "6                         NaN                          32.9   \n",
            "7                      0.3409                          32.9   \n",
            "8                         NaN                          32.9   \n",
            "9                         NaN                          32.9   \n",
            "\n",
            "   avg_rank_first_target_target  avg_targeted_in_top10_target  \\\n",
            "0                          32.9                           NaN   \n",
            "1                          32.9                           NaN   \n",
            "2                          32.9                           NaN   \n",
            "3                          32.9                        0.3409   \n",
            "4                          32.9                           NaN   \n",
            "5                          32.9                           NaN   \n",
            "6                          32.9                           NaN   \n",
            "7                          32.9                        0.3409   \n",
            "8                          32.9                           NaN   \n",
            "9                          32.9                           NaN   \n",
            "\n",
            "   custom_precision@k_poison  custom_precision@k_target  ...  \\\n",
            "0                   3.571429                   3.571429  ...   \n",
            "1                   3.437938                   3.437938  ...   \n",
            "2                   3.453256                   3.453256  ...   \n",
            "3                   3.409270                   3.409270  ...   \n",
            "4                   3.571429                   3.571429  ...   \n",
            "5                   3.437938                   3.437938  ...   \n",
            "6                   3.453256                   3.453256  ...   \n",
            "7                   3.409270                   3.409270  ...   \n",
            "8                   3.571429                   3.571429  ...   \n",
            "9                   3.437938                   3.437938  ...   \n",
            "\n",
            "   num_queries_target  num_targeted_docs_target  precision@k_norm_poison  \\\n",
            "0               15232                       NaN                   0.0357   \n",
            "1               15232                       NaN                   0.0344   \n",
            "2               15232                       NaN                   0.0345   \n",
            "3               15232                     157.0                   0.0341   \n",
            "4               15232                       NaN                   0.0357   \n",
            "5               15232                       NaN                   0.0344   \n",
            "6               15232                       NaN                   0.0345   \n",
            "7               15232                     157.0                   0.0341   \n",
            "8               15232                       NaN                   0.0357   \n",
            "9               15232                       NaN                   0.0344   \n",
            "\n",
            "   precision@k_norm_target  precision@k_raw_poison  precision@k_raw_target  \\\n",
            "0                   0.0357                544.0000                544.0000   \n",
            "1                   0.0344                523.6667                523.6667   \n",
            "2                   0.0345                526.0000                526.0000   \n",
            "3                   0.0341                519.3000                519.3000   \n",
            "4                   0.0357                544.0000                544.0000   \n",
            "5                   0.0344                523.6667                523.6667   \n",
            "6                   0.0345                526.0000                526.0000   \n",
            "7                   0.0341                519.3000                519.3000   \n",
            "8                   0.0357                544.0000                544.0000   \n",
            "9                   0.0344                523.6667                523.6667   \n",
            "\n",
            "   precision@k_scaled_poison  precision@k_scaled_target  recall@k_poison  \\\n",
            "0                     3.5714                     3.5714           3.4650   \n",
            "1                     3.4379                     3.4379          10.0064   \n",
            "2                     3.4533                     3.4533          16.7516   \n",
            "3                     3.4093                     3.4093          33.0764   \n",
            "4                     3.5714                     3.5714           3.4650   \n",
            "5                     3.4379                     3.4379          10.0064   \n",
            "6                     3.4533                     3.4533          16.7516   \n",
            "7                     3.4093                     3.4093          33.0764   \n",
            "8                     3.5714                     3.5714           3.4650   \n",
            "9                     3.4379                     3.4379          10.0064   \n",
            "\n",
            "   recall@k_target  \n",
            "0           3.4650  \n",
            "1          10.0064  \n",
            "2          16.7516  \n",
            "3          33.0764  \n",
            "4           3.4650  \n",
            "5          10.0064  \n",
            "6          16.7516  \n",
            "7          33.0764  \n",
            "8           3.4650  \n",
            "9          10.0064  \n",
            "\n",
            "[10 rows x 26 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# --- Th∆∞ m·ª•c g·ªëc ---\n",
        "HOTFLIP_DIR = Path(\"/Users/hieunguyen/Downloads/SANNER_2025/data/hotflip_result/result\")\n",
        "RESULT_DIR = HOTFLIP_DIR \n",
        "\n",
        "# --- File poison v√† target ---\n",
        "poison_files = [\n",
        "    \"attack_metrics_full.csv\",\n",
        "    \"avg_poison_in_top10.csv\",\n",
        "    \"custom_p_at_k_poison.csv\"\n",
        "]\n",
        "\n",
        "target_files = [\n",
        "    \"attack_metrics_full_targetted.csv\",\n",
        "    \"avg_targeted_in_top10.csv\",\n",
        "    \"custom_p_at_k_targetted.csv\"\n",
        "]\n",
        "\n",
        "def merge_files(files_list, suffix):\n",
        "    merged = None\n",
        "    for fname in files_list:\n",
        "        fpath = HOTFLIP_DIR / fname\n",
        "        if not fpath.exists():\n",
        "            print(f\"‚ö†Ô∏è File not found: {fpath}\")\n",
        "            continue\n",
        "        df = pd.read_csv(fpath)\n",
        "\n",
        "        # N·∫øu file kh√¥ng c√≥ c·ªôt k, th√™m k=10\n",
        "        if \"k\" not in df.columns:\n",
        "            df[\"k\"] = 10\n",
        "\n",
        "        # ƒê·ªïi t√™n c√°c c·ªôt metric tr·ª´ ratio, k\n",
        "        rename_dict = {col: f\"{col}_{suffix}\" for col in df.columns if col not in [\"ratio\",\"k\"]}\n",
        "        df = df.rename(columns=rename_dict)\n",
        "\n",
        "        # Merge\n",
        "        if merged is None:\n",
        "            merged = df\n",
        "        else:\n",
        "            merged = pd.merge(merged, df, on=[\"ratio\",\"k\"], how=\"outer\")\n",
        "    return merged\n",
        "\n",
        "# --- Merge poison v√† target ---\n",
        "merged_poison = merge_files(poison_files, \"poison\")\n",
        "merged_target = merge_files(target_files, \"target\")\n",
        "\n",
        "# --- Merge poison + target theo ratio + k ---\n",
        "merged_all = pd.merge(merged_poison, merged_target, on=[\"ratio\",\"k\"], how=\"outer\")\n",
        "\n",
        "# --- S·∫Øp x·∫øp c·ªôt theo c·∫∑p poison/target ---\n",
        "fixed_cols = [\"ratio\",\"k\"]\n",
        "all_cols = [c for c in merged_all.columns if c not in fixed_cols]\n",
        "\n",
        "# L·∫•y t√™n metric c∆° b·∫£n, b·ªè h·∫≠u t·ªë\n",
        "metrics = sorted(set([c.rsplit(\"_\",1)[0] for c in all_cols]))\n",
        "\n",
        "ordered_cols = fixed_cols[:]\n",
        "for m in metrics:\n",
        "    poison_col = f\"{m}_poison\"\n",
        "    target_col = f\"{m}_target\"\n",
        "    if poison_col in merged_all.columns:\n",
        "        ordered_cols.append(poison_col)\n",
        "    if target_col in merged_all.columns:\n",
        "        ordered_cols.append(target_col)\n",
        "\n",
        "merged_all = merged_all[ordered_cols]\n",
        "\n",
        "# --- L∆∞u k·∫øt qu·∫£ ---\n",
        "out_path = RESULT_DIR / \"result.csv\"\n",
        "merged_all.to_csv(out_path, index=False)\n",
        "print(f\"‚úÖ Saved merged & paired CSV to {out_path}\")\n",
        "print(merged_all.head(10))\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
