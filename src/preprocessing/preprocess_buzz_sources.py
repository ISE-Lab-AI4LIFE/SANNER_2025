from pathlib import Path
import pandas as pd
import tqdm
import re
import json

# T·ª± ƒë·ªông ph√°t hi·ªán th∆∞ m·ª•c g·ªëc c·ªßa repo (SANNER_2025)
BASE_DIR = Path(__file__).resolve().parents[2]
print(f"Base directory: {BASE_DIR}")
DATA_DIR = BASE_DIR / "data"

# Th∆∞ m·ª•c d·ªØ li·ªáu ngu·ªìn v√† ƒë·∫ßu ra
RAW_FILE = DATA_DIR / "raw" / "buzz_sources_042_javascript" / "train-00000-of-00001.parquet"
OUT_DIR = DATA_DIR / "processed"

# Th∆∞ m·ª•c ƒë·∫ßu ra
queries_dir = OUT_DIR / "queries" / "buzz_sources_042_javascript"
documents_dir = OUT_DIR / "documents" / "buzz_sources_042_javascript"

queries_dir.mkdir(parents=True, exist_ok=True)
documents_dir.mkdir(parents=True, exist_ok=True)

BATCH_SIZE = 1
DOC_SEP = " [DOC_SEP] "

# ƒê·ªçc d·ªØ li·ªáu parquet
print("üì• Loading parquet data...")
df = pd.read_parquet(RAW_FILE)
print(f"‚úÖ Loaded {len(df)} records")

# Ki·ªÉm tra c·ªôt c·∫ßn thi·∫øt
if "conversations" not in df.columns:
    raise ValueError("Kh√¥ng t√¨m th·∫•y c·ªôt 'conversations' trong file parquet!")

# --- T√°ch d·ªØ li·ªáu t·ª´ tr∆∞·ªùng conversations ---
data = []
for i, row in tqdm.tqdm(df.iterrows(), total=len(df), desc="Extracting conversations"):
    try:
        conv = row["conversations"]
        # N·∫øu d·∫°ng string, parse l·∫°i
        if isinstance(conv, str):
            conv = json.loads(conv)
        # L·∫•y gi√° tr·ªã human v√† gpt
        human_text = next((item["value"] for item in conv if item["from"] == "human"), None)
        gpt_text = next((item["value"] for item in conv if item["from"] == "gpt"), None)
        if human_text and gpt_text:
            data.append({"queries": human_text, "documents": gpt_text})
    except Exception as e:
        continue

print(f"‚úÖ Extracted {len(data)} valid pairs from conversations")

df_new = pd.DataFrame(data)

# --- T·∫°o queries_df ---
queries_data = []
for idx, row in tqdm.tqdm(df_new.iterrows(), total=len(df_new), desc="Building queries"):
    query_id = f"BuzzJS_query_{idx+1}"
    queries_data.append({"id": query_id, "queries": row["queries"]})
queries_df = pd.DataFrame(queries_data)

# --- T·∫°o documents_df ---
documents_data = []
for batch_start in tqdm.tqdm(range(0, len(df_new), BATCH_SIZE), desc="Building documents"):
    batch = df_new.iloc[batch_start: batch_start + BATCH_SIZE]
    merged_docs = DOC_SEP.join(batch["documents"].astype(str).tolist())
    record_id = batch_start // BATCH_SIZE + 1
    doc_id = f"BuzzJS_document_{record_id}"
    documents_data.append({"id": doc_id, "documents": merged_docs})
documents_df = pd.DataFrame(documents_data)

# --- L∆∞u file CSV ---
queries_csv = queries_dir / "buzz_sources_042_javascript.csv"
documents_csv = documents_dir / "buzz_sources_042_javascript.csv"

queries_df.to_csv(queries_csv, index=False)
documents_df.to_csv(documents_csv, index=False)

print(f"‚úÖ Done saving:\n - {queries_csv}\n - {documents_csv}")